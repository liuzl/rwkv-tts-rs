//! åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨ - é«˜æ€§èƒ½TTSæ¨ç†å¼•æ“
//!
//! ## æ¶æ„ä¼˜åŒ–å†ç¨‹
//!
//! ### åŸæœ‰é—®é¢˜ï¼š
//! - æ¯ä¸ªå·¥ä½œçº¿ç¨‹éƒ½åˆ›å»ºç‹¬ç«‹çš„RwkvSamplerå®ä¾‹
//! - é‡å¤åŠ è½½ç›¸åŒçš„RWKVæ¨¡å‹ï¼Œé€ æˆä¸¥é‡çš„å†…å­˜æµªè´¹ï¼ˆæ¯ä¸ªå®ä¾‹å¯èƒ½å ç”¨æ•°GBå†…å­˜ï¼‰
//! - æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹çš„å…±äº«ç‰¹æ€§ï¼Œèµ„æºåˆ©ç”¨ç‡ä½
//! - çº¿ç¨‹é—´ç¼ºä¹åè°ƒï¼Œæ— æ³•å®ç°çœŸæ­£çš„æ‰¹å¤„ç†ä¼˜åŒ–
//!
//! ### ä¼˜åŒ–æ–¹æ¡ˆï¼ˆå‚è€ƒai00-coreå’Œweb-rwkvæ¶æ„ï¼‰ï¼š
//! - **å…±äº«Runtimeæ¶æ„**ï¼šåˆ›å»ºå•ä¸€çš„SharedRwkvRuntimeå®ä¾‹
//! - **å†…å­˜ä¼˜åŒ–**ï¼šæ‰€æœ‰å·¥ä½œçº¿ç¨‹å…±äº«åŒä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼Œå†…å­˜å ç”¨é™ä½90%+
//! - **å¹¶å‘å®‰å…¨**ï¼šä½¿ç”¨Arc<RwLock<>>ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„æ¨¡å‹è®¿é—®
//! - **æ‰¹å¤„ç†è°ƒåº¦**ï¼šé€šè¿‡flume channelå®ç°é«˜æ•ˆçš„ä»»åŠ¡åˆ†å‘
//! - **çŠ¶æ€éš”ç¦»**ï¼šæ¯ä¸ªæ¨ç†è¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„é‡‡æ ·å™¨çŠ¶æ€ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
//!
//! ### å¹¶å‘å¤„ç†æµç¨‹ï¼š
//! ```
//! ç”¨æˆ·è¯·æ±‚ â†’ enqueue_worker(æ”¶é›†) â†’ process_collected_batch(è½¬æ¢)
//!     â†“
//! infer_worker1 â†â”€â”
//! infer_worker2 â†â”€â”¼â”€ å…±äº«SharedRwkvRuntime â”€â†’ å¹¶è¡Œæ¨ç†
//! infer_worker3 â†â”€â”˜
//!     â†“
//! ç»“æœåˆ†å‘ â†’ ç”¨æˆ·å“åº”
//! ```
//!
//! ### æ€§èƒ½æå‡ï¼š
//! - **å†…å­˜å ç”¨**ï¼šä»NÃ—æ¨¡å‹å¤§å° é™ä½åˆ° 1Ã—æ¨¡å‹å¤§å°
//! - **å¯åŠ¨æ—¶é—´**ï¼šä»NÃ—åŠ è½½æ—¶é—´ é™ä½åˆ° 1Ã—åŠ è½½æ—¶é—´
//! - **å¹¶å‘èƒ½åŠ›**ï¼šæ”¯æŒçœŸæ­£çš„æ‰¹å¤„ç†æ¨ç†ï¼Œååé‡æ˜¾è‘—æå‡

use anyhow::Result;
use flume::{Receiver, Sender};
use rand::SeedableRng;
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{oneshot, Mutex, RwLock, Semaphore};
use tracing::{debug, error, info, warn};

use crate::ref_audio_utilities::RefAudioUtilities;
use crate::rwkv_sampler::{SamplerArgs, TtsBatchRequest};
// use rand_chacha::ChaCha8Rng; // æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä½¿ç”¨æ ‡å‡†éšæœºæ•°ç”Ÿæˆå™¨
use memmap2::Mmap;
use safetensors::SafeTensors;
use std::path::{Path, PathBuf};
use web_rwkv::{runtime::v7, tokenizer::Tokenizer};

use web_rwkv::runtime::loader::Loader;
use web_rwkv::runtime::model::{Bundle, State};

use std::sync::atomic::{AtomicU64, Ordering};

/// TTSè¯·æ±‚é¡¹ï¼ŒåŒ…å«å®Œæ•´çš„è¯·æ±‚ä¿¡æ¯å’Œå“åº”é€šé“
#[derive(Debug)]
pub struct DynamicTtsRequest {
    pub text: String,
    pub property_tokens: Vec<i32>,
    pub ref_global_tokens: Option<Vec<i32>>,
    pub ref_semantic_tokens: Option<Vec<i32>>,
    pub args: SamplerArgs,
    pub response_tx: oneshot::Sender<Result<(Vec<i32>, Vec<i32>)>>,
    pub submitted_at: Instant,
    pub batch_id: usize,
}

/// æ¨ç†æ‰¹æ¬¡
#[derive(Debug)]
pub enum InferBatch {
    /// æ‰§è¡Œæ¨ç†
    Run {
        batch_id: usize,
        requests: Vec<TtsBatchRequest>,
        sender: Sender<Vec<(Vec<i32>, Vec<i32>)>>,
    },
    /// è·å–ç»“æœ
    Result {
        batch_id: usize,
        sender: oneshot::Sender<Vec<(Vec<i32>, Vec<i32>)>>,
    },
}

/// åŠ¨æ€æ‰¹å¤„ç†é…ç½®
#[derive(Debug, Clone)]
pub struct DynamicBatchConfig {
    /// æœ€å°æ‰¹å¤„ç†å¤§å°
    pub min_batch_size: usize,
    /// æœ€å¤§æ‰¹å¤„ç†å¤§å°
    pub max_batch_size: usize,
    /// æ‰¹å¤„ç†æ”¶é›†è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub collect_timeout_ms: u64,
    /// æ¨ç†è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub inference_timeout_ms: u64,
    /// æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°
    pub max_concurrent_batches: usize,
    /// ä¿¡å·é‡è®¸å¯æ•°é‡ï¼ˆåŸºäºç¡¬ä»¶å’Œè´Ÿè½½è°ƒæ•´ï¼‰
    pub semaphore_permits: usize,
}

impl Default for DynamicBatchConfig {
    fn default() -> Self {
        Self {
            min_batch_size: 1,
            max_batch_size: 10,
            collect_timeout_ms: 50,
            inference_timeout_ms: 60000,
            max_concurrent_batches: 4, // åˆç†çš„é»˜è®¤å¹¶å‘æ•°
            semaphore_permits: 3,      // ä¿¡å·é‡è®¸å¯æ•°é‡ç•¥å°äºå¹¶å‘æ•°
        }
    }
}

/// TTSçŠ¶æ€IDï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„çŠ¶æ€å®ä¾‹
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Default)]
pub struct TtsStateId(pub u64);

/// TTSæ¨ç†é€‰é¡¹
#[derive(Debug, Clone)]
pub struct TtsInferOptions {
    /// æ¸©åº¦å‚æ•°
    pub temperature: f32,
    /// top_kå‚æ•°
    pub top_k: usize,
    /// top_på‚æ•°
    pub top_p: f32,
    /// éšæœºç§å­
    pub seed: Option<u64>,
}

impl Default for TtsInferOptions {
    fn default() -> Self {
        Self {
            temperature: 1.0,
            top_k: 50,
            top_p: 0.9,
            seed: None,
        }
    }
}

/// TTSæ¨ç†ä¸Šä¸‹æ–‡ï¼Œç±»ä¼¼ai00-coreçš„GenerateContext
#[derive(Clone)]
pub struct TtsInferContext {
    /// è¯·æ±‚ID
    pub request_id: String,
    /// çŠ¶æ€ID
    pub state_id: TtsStateId,
    /// è¾“å…¥æ–‡æœ¬
    pub text: String,
    /// æ¨ç†é€‰é¡¹
    pub options: TtsInferOptions,
    /// åˆ†è¯å™¨å¼•ç”¨
    pub tokenizer: Arc<Tokenizer>,
    /// Runtimeå¼•ç”¨
    pub runtime: Arc<web_rwkv::runtime::TokioRuntime<web_rwkv::runtime::infer::Rnn>>,
    /// æ¨¡å‹çŠ¶æ€ï¼ˆç‹¬ç«‹å‰¯æœ¬ï¼‰- é‡æ–°æ·»åŠ ä»¥ç¡®ä¿çŠ¶æ€éš”ç¦»
    pub state: Arc<Mutex<Box<dyn State + Send + Sync>>>,
    // Serialize runtime.infer calls temporarily for correctness under concurrency
    pub runtime_semaphore: Arc<Semaphore>,
}

/// å…±äº«çš„RWKV Runtimeå®ä¾‹
/// å‚è€ƒai00-coreçš„è®¾è®¡ï¼Œä½¿ç”¨å…±äº«Runtimeå’Œç‹¬ç«‹çŠ¶æ€
pub struct SharedRwkvRuntime {
    /// å…±äº«çš„Runtimeå®ä¾‹
    runtime: Arc<web_rwkv::runtime::TokioRuntime<web_rwkv::runtime::infer::Rnn>>,
    /// å…±äº«çš„æ¨¡å‹Bundleï¼ˆç”¨äºåˆ›å»ºçŠ¶æ€ï¼‰
    model_bundle: Arc<v7::Bundle<f32>>,
    /// å…±äº«çš„åˆ†è¯å™¨
    tokenizer: Arc<Tokenizer>,
    /// çŠ¶æ€IDç”Ÿæˆå™¨
    state_id_generator: AtomicU64,
    /// æ´»è·ƒçŠ¶æ€ç»Ÿè®¡
    active_states: Arc<RwLock<HashMap<TtsStateId, String>>>,
    /// æ¨¡å‹è·¯å¾„
    #[allow(dead_code)]
    model_path: String,
    /// è¯æ±‡è¡¨è·¯å¾„
    #[allow(dead_code)]
    vocab_path: String,
    // A semaphore to control concurrent inference calls
    // The number of permits should be configured based on GPU capabilities
    runtime_semaphore: Arc<Semaphore>,
}

impl SharedRwkvRuntime {
    /// åˆ›å»ºæ–°çš„å…±äº«Runtimeï¼ˆæ”¯æŒé‡åŒ–é…ç½®ï¼‰
    pub async fn new(
        model_path: String,
        vocab_path: String,
        quant_config: Option<HashMap<usize, web_rwkv::runtime::model::Quant>>,
        config: DynamicBatchConfig, // æ·»åŠ é…ç½®å‚æ•°
    ) -> Result<Self> {
        info!("ğŸ”§ åˆå§‹åŒ–å…±äº«RWKV Runtime: {}", model_path);

        // åˆ›å»ºWebRWKVä¸Šä¸‹æ–‡å’Œæ¨¡å‹
        use web_rwkv::context::{ContextBuilder, InstanceExt};
        use web_rwkv::runtime::model::{ContextAutoLimits, ModelBuilder};
        use web_rwkv::wgpu::{Instance, PowerPreference};

        // æ£€æµ‹æ¨¡å‹æ ¼å¼å¹¶åŠ è½½
        let model_file_path = if Path::new(&model_path).is_dir() {
            // å¦‚æœæ˜¯ç›®å½•ï¼Œä¼˜å…ˆå°è¯•SafeTensorsæ ¼å¼
            let safetensors_path = Path::new(&model_path).join("rwkvtts-Int8_22.safetensors");
            let prefab_path = Path::new(&model_path).join("rwkvtts-Int8_22.prefab");
            if safetensors_path.exists() {
                safetensors_path
            } else if prefab_path.exists() {
                prefab_path
            } else {
                return Err(anyhow::anyhow!(
                    "No supported model file found in directory: {}",
                    model_path
                ));
            }
        } else {
            PathBuf::from(&model_path)
        };

        let file = std::fs::File::open(&model_file_path)
            .map_err(|e| anyhow::anyhow!("Failed to open model file: {}", e))?;
        let data = unsafe { Mmap::map(&file) }
            .map_err(|e| anyhow::anyhow!("Failed to map model file: {}", e))?;

        // å°è¯•æ£€æµ‹æ ¼å¼å¹¶è·å–æ¨¡å‹ä¿¡æ¯
        let (load_type, info) = if let Ok(safetensors) = SafeTensors::deserialize(&data) {
            // SafeTensorsæ ¼å¼
            let actual_info = Loader::info(&safetensors)
                .map_err(|e| anyhow::anyhow!("Failed to get SafeTensors model info: {}", e))?;

            // æ£€æŸ¥ç‰ˆæœ¬
            if actual_info.version != web_rwkv::runtime::model::ModelVersion::V7 {
                return Err(anyhow::anyhow!(
                    "Only V7 models are supported, got: {:?}",
                    actual_info.version
                ));
            }

            info!(
                "ğŸ“Š SafeTensorsæ¨¡å‹ä¿¡æ¯: vocab={}, layers={}, embed={}, heads={}",
                actual_info.num_vocab,
                actual_info.num_layer,
                actual_info.num_emb,
                actual_info.num_head
            );

            ("safetensors", actual_info)
        } else {
            // å‡è®¾ä¸ºprefabæ ¼å¼ï¼Œä¸ºV7æ¨¡å‹åˆ›å»ºé»˜è®¤infoï¼ˆå®é™…åŠ è½½æ—¶ä¼šéªŒè¯ï¼‰
            info!("ğŸ”§ æ£€æµ‹åˆ°prefabæ ¼å¼ï¼Œä½¿ç”¨V7æ¨¡å‹é»˜è®¤é…ç½®");
            let default_info = web_rwkv::runtime::model::ModelInfo {
                version: web_rwkv::runtime::model::ModelVersion::V7,
                num_vocab: 65536,
                num_layer: 32,
                num_emb: 2048,
                num_head: 32,
                num_hidden: 2048,
                custom: web_rwkv::runtime::model::ModelCustomInfo::None,
            };
            ("prefab", default_info)
        };

        info!("ğŸ”§ æ¨¡å‹æ ¼å¼: {}", load_type);

        // åˆ›å»ºGPUå®ä¾‹å’Œé€‚é…å™¨
        let instance = Instance::default();
        let adapter = instance
            .adapter(PowerPreference::HighPerformance)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to get adapter: {}", e))?;

        // åˆ›å»ºä¸Šä¸‹æ–‡
        let context = ContextBuilder::new(adapter)
            .auto_limits(&info)
            .build()
            .await?;

        // æ ¹æ®æ ¼å¼æ„å»ºæ¨¡å‹Bundle
        let model = if load_type == "safetensors" {
            // SafeTensorsæ ¼å¼
            let safetensors = SafeTensors::deserialize(&data)
                .map_err(|e| anyhow::anyhow!("Failed to deserialize SafeTensors: {}", e))?;
            let mut builder = ModelBuilder::new(&context, safetensors);
            if let Some(ref quant) = quant_config {
                builder = builder.quant(quant.clone());
            }
            builder.build_v7().await?
        } else {
            // prefabæ ¼å¼ - ä½¿ç”¨cbor4iiå’ŒSeedç›´æ¥ååºåˆ—åŒ–
            use cbor4ii::{core::utils::SliceReader, serde::Deserializer};
            use serde::de::DeserializeSeed;
            use web_rwkv::tensor::serialization::Seed;

            let reader = SliceReader::new(&data);
            let mut deserializer = Deserializer::new(reader);
            let seed = Seed::<web_rwkv::context::Context, v7::Model>::new(&context);
            seed.deserialize(&mut deserializer)
                .map_err(|e| anyhow::anyhow!("Failed to deserialize prefab model: {}", e))?
        };
        let model_bundle = Arc::new(v7::Bundle::new(model, config.max_concurrent_batches));

        // ä½¿ç”¨é…ç½®ä¸­çš„ä¿¡å·é‡è®¸å¯æ•°é‡
        let semaphore_permits = config.semaphore_permits;
        info!("ğŸ”§ è®¾ç½®ä¿¡å·é‡è®¸å¯æ•°é‡: {} (é…ç½®å€¼)", semaphore_permits);

        // åˆ›å»ºTokioRuntimeå®ä¾‹
        let runtime = Arc::new(web_rwkv::runtime::TokioRuntime::new((*model_bundle).clone()).await);

        // åˆ›å»ºåˆ†è¯å™¨ - è¯»å–è¯æ±‡è¡¨æ–‡ä»¶å†…å®¹
        let vocab_content = std::fs::read_to_string(&vocab_path)
            .map_err(|e| anyhow::anyhow!("Failed to read vocab file {}: {}", vocab_path, e))?;
        let tokenizer = Arc::new(
            Tokenizer::new(&vocab_content)
                .map_err(|e| anyhow::anyhow!("Failed to parse vocabulary: {}", e))?,
        );

        info!("âœ… å…±äº«RWKV Runtimeåˆå§‹åŒ–å®Œæˆ");

        Ok(Self {
            runtime,
            model_bundle,
            tokenizer,
            state_id_generator: AtomicU64::new(1),
            active_states: Arc::new(RwLock::new(HashMap::new())),
            model_path,
            vocab_path,
            // ä½¿ç”¨é…ç½®ä¸­çš„ä¿¡å·é‡è®¸å¯æ•°é‡
            runtime_semaphore: Arc::new(Semaphore::new(semaphore_permits)),
        })
    }

    /// åˆ›å»ºæ–°çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªè¯·æ±‚è·å¾—ç‹¬ç«‹çš„çŠ¶æ€å‰¯æœ¬
    pub async fn create_infer_context(
        &self,
        request_id: String,
        text: String,
        options: TtsInferOptions,
    ) -> Result<TtsInferContext> {
        // ç”Ÿæˆå”¯ä¸€çš„çŠ¶æ€ID
        let state_id = TtsStateId(self.state_id_generator.fetch_add(1, Ordering::SeqCst));

        // åˆ›å»ºç‹¬ç«‹çš„çŠ¶æ€å‰¯æœ¬
        let state = Arc::new(Mutex::new(
            Box::new(self.model_bundle.state()) as Box<dyn State + Send + Sync>
        ));

        // è®°å½•æ´»è·ƒçŠ¶æ€
        {
            let mut active = self.active_states.write().await;
            active.insert(state_id, request_id.clone());
        }

        info!("ğŸ”§ åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡: {} (çŠ¶æ€ID: {:?})", request_id, state_id);

        Ok(TtsInferContext {
            request_id,
            state_id,
            text,
            options,
            tokenizer: self.tokenizer.clone(),
            runtime: self.runtime.clone(),
            state, // æ·»åŠ ç‹¬ç«‹çŠ¶æ€
            runtime_semaphore: self.runtime_semaphore.clone(),
        })
    }

    /// æ¸…ç†çŠ¶æ€
    pub async fn cleanup_state(&self, state_id: TtsStateId) {
        let mut active = self.active_states.write().await;
        active.remove(&state_id);
        info!("ğŸ§¹ æ¸…ç†çŠ¶æ€: {:?}", state_id);
    }

    /// è·å–åˆ†è¯å™¨
    pub fn tokenizer(&self) -> &Arc<Tokenizer> {
        &self.tokenizer
    }

    /// è·å–æ¨¡å‹Bundle
    pub fn model_bundle(&self) -> &Arc<v7::Bundle<f32>> {
        &self.model_bundle
    }

    /// è·å–Runtimeå®ä¾‹
    pub fn runtime(&self) -> &Arc<web_rwkv::runtime::TokioRuntime<web_rwkv::runtime::infer::Rnn>> {
        &self.runtime
    }

    /// è·å–çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯
    pub async fn stats(&self) -> crate::tts_state_manager::TtsStateStats {
        let active = self.active_states.read().await;
        crate::tts_state_manager::TtsStateStats {
            active_states: active.len(),
        }
    }
}

/// åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨
/// è´Ÿè´£ç®¡ç†å¤šä¸ªå¹¶å‘çš„TTSæ¨ç†è¯·æ±‚
pub struct DynamicBatchManager {
    /// è¯·æ±‚å‘é€é€šé“
    request_tx: flume::Sender<DynamicTtsRequest>,
    /// é…ç½®
    config: DynamicBatchConfig,
    /// å‚è€ƒéŸ³é¢‘å·¥å…·
    ref_audio_utilities: Arc<Mutex<Option<RefAudioUtilities>>>,
    /// å…±äº«è¿è¡Œæ—¶
    #[allow(dead_code)]
    shared_runtime: Arc<SharedRwkvRuntime>,
}

impl DynamicBatchManager {
    /// é‡‡æ ·logits
    fn sample_logits<R: rand::Rng + ?Sized>(
        logits: &[f32],
        args: &SamplerArgs,
        rng: &mut R,
    ) -> Result<usize> {
        // åº”ç”¨æ¸©åº¦
        let scaled_logits: Vec<f32> = logits.iter().map(|&x| x / args.temperature).collect();

        // æ‰¾åˆ°æœ€å¤§å€¼ç”¨äºæ•°å€¼ç¨³å®šæ€§
        let max_logit = scaled_logits
            .iter()
            .fold(f32::NEG_INFINITY, |a, &b| a.max(b));

        // è®¡ç®—æ¦‚ç‡
        let mut probs: Vec<f32> = scaled_logits
            .iter()
            .map(|&x| (x - max_logit).exp())
            .collect();
        let sum: f32 = probs.iter().sum();
        for p in probs.iter_mut() {
            *p /= sum;
        }

        // Top-ké‡‡æ ·
        if args.top_k > 0 && args.top_k < probs.len() {
            let mut indexed_probs: Vec<(usize, f32)> =
                probs.iter().enumerate().map(|(i, &p)| (i, p)).collect();
            indexed_probs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

            for i in args.top_k..indexed_probs.len() {
                probs[indexed_probs[i].0] = 0.0;
            }

            // é‡æ–°å½’ä¸€åŒ–
            let sum: f32 = probs.iter().sum();
            if sum > 0.0 {
                for p in probs.iter_mut() {
                    *p /= sum;
                }
            }
        }

        // Top-pé‡‡æ ·
        if args.top_p < 1.0 {
            let mut indexed_probs: Vec<(usize, f32)> =
                probs.iter().enumerate().map(|(i, &p)| (i, p)).collect();
            indexed_probs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

            let mut cumsum = 0.0;
            for (i, &(_idx, prob)) in indexed_probs.iter().enumerate() {
                cumsum += prob;
                if cumsum > args.top_p {
                    // å°†åç»­æ¦‚ç‡è®¾ä¸º0
                    for &(later_idx, _) in &indexed_probs[i + 1..] {
                        probs[later_idx] = 0.0;
                    }
                    break;
                }
            }

            // é‡æ–°å½’ä¸€åŒ–
            let sum: f32 = probs.iter().sum();
            if sum > 0.0 {
                for p in probs.iter_mut() {
                    *p /= sum;
                }
            }
        }

        // é‡‡æ ·
        let rand_val: f32 = rng.gen();
        let mut cumsum = 0.0;
        for (i, &prob) in probs.iter().enumerate() {
            cumsum += prob;
            if rand_val <= cumsum {
                return Ok(i);
            }
        }

        // å¦‚æœæ²¡æœ‰é‡‡æ ·åˆ°ï¼Œè¿”å›æœ€åä¸€ä¸ªæœ‰æ•ˆç´¢å¼•
        Ok(probs.len() - 1)
    }
    /// åˆ›å»ºæ–°çš„åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨ï¼ˆæ”¯æŒé‡åŒ–é…ç½®ï¼‰
    /// ç°åœ¨ä½¿ç”¨å…±äº«Runtimeæ¶æ„ï¼Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨
    pub async fn new(
        model_path: &str,
        vocab_path: &str,
        config: DynamicBatchConfig,
        quant_config: Option<std::collections::HashMap<usize, web_rwkv::runtime::model::Quant>>,
    ) -> Result<Self> {
        info!("ğŸš€ åˆ›å»ºåŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨ï¼Œé…ç½®: {:?}", config);
        info!("ğŸ“Š å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«Runtimeæ¶æ„ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹");

        // åˆ›å»ºå…±äº«çš„RWKV Runtimeå®ä¾‹ï¼ˆå…³é”®ä¼˜åŒ–ï¼šåªåŠ è½½ä¸€æ¬¡æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–é…ç½®ï¼‰
        let shared_runtime = Arc::new(
            SharedRwkvRuntime::new(
                model_path.to_string(),
                vocab_path.to_string(),
                quant_config,
                config.clone(),
            )
            .await?,
        );

        let (request_tx, request_rx) = flume::unbounded();

        // åˆå§‹åŒ–å‚è€ƒéŸ³é¢‘å·¥å…·
        let ref_audio_utilities = Arc::new(Mutex::new(
            RefAudioUtilities::new(
                "assets/model/BiCodecTokenize_static_qdq.onnx",
                "assets/model/wav2vec2-large-xlsr-53_static_qdq.onnx",
                3.0,                                                    // ref_segment_duration
                320,                                                    // latent_hop_length
                Some("assets/model/BiCodecDetokenize_static_qdq.onnx"), // detokenizer_path
            )
            .ok(),
        ));

        info!("âœ… åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨åˆ›å»ºæˆåŠŸï¼Œå¯åŠ¨æ ¸å¿ƒè¿è¡Œæ—¶");

        // å¯åŠ¨æ ¸å¿ƒè¿è¡Œæ—¶ï¼ˆä¼ é€’å…±äº«Runtimeï¼‰
        let runtime_config = config.clone();
        let shared_runtime_clone = shared_runtime.clone();
        tokio::spawn(async move {
            Self::run_core_runtime(shared_runtime_clone, request_rx, runtime_config).await;
        });

        Ok(Self {
            request_tx,
            config,
            ref_audio_utilities,
            shared_runtime,
        })
    }

    /// æäº¤TTSè¯·æ±‚
    pub async fn generate_tts(
        &self,
        text: String,
        property_tokens: Vec<i32>,
        ref_global_tokens: Option<Vec<i32>>,
        ref_semantic_tokens: Option<Vec<i32>>,
        args: SamplerArgs,
    ) -> Result<(Vec<i32>, Vec<i32>)> {
        info!(
            "ğŸš€ åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨æ”¶åˆ°TTSè¯·æ±‚: {}",
            text.chars().take(20).collect::<String>()
        );

        let (response_tx, response_rx) = oneshot::channel();

        let request = DynamicTtsRequest {
            text,
            property_tokens,
            ref_global_tokens,
            ref_semantic_tokens,
            args,
            response_tx,
            submitted_at: Instant::now(),
            batch_id: 0, // å°†åœ¨è°ƒåº¦å™¨ä¸­åˆ†é…
        };

        // å‘é€è¯·æ±‚åˆ°é˜Ÿåˆ—
        info!("ğŸ“¤ å‘é€è¯·æ±‚åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—");
        self.request_tx
            .send_async(request)
            .await
            .map_err(|_| anyhow::anyhow!("åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å·²å…³é—­"))?;

        let wait_start = Instant::now();
        info!(
            "â³ ç­‰å¾…æ‰¹å¤„ç†å“åº”ï¼Œè¶…æ—¶æ—¶é—´: {}ms",
            self.config.inference_timeout_ms
        );

        // ç­‰å¾…å“åº”ï¼Œæ·»åŠ è¯¦ç»†çš„è¶…æ—¶å’Œå–æ¶ˆæ—¥å¿—
        let result = tokio::time::timeout(
            Duration::from_millis(self.config.inference_timeout_ms),
            response_rx,
        )
        .await;

        let wait_duration = wait_start.elapsed();

        match result {
            Ok(Ok(response)) => {
                info!("âœ… åŠ¨æ€æ‰¹å¤„ç†è¯·æ±‚å®Œæˆï¼Œç­‰å¾…æ—¶é—´: {:?}", wait_duration);
                response
            }
            Ok(Err(_)) => {
                warn!("âŒ TTSè¯·æ±‚è¢«å–æ¶ˆï¼Œç­‰å¾…æ—¶é—´: {:?}", wait_duration);
                Err(anyhow::anyhow!(
                    "TTSè¯·æ±‚è¢«å–æ¶ˆï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨æ­£åœ¨é‡å¯æˆ–é˜Ÿåˆ—å·²æ»¡"
                ))
            }
            Err(_) => {
                error!(
                    "â° TTSè¯·æ±‚è¶…æ—¶ï¼Œç­‰å¾…æ—¶é—´: {:?}ï¼Œè¶…æ—¶é™åˆ¶: {}ms",
                    wait_duration, self.config.inference_timeout_ms
                );
                Err(anyhow::anyhow!("TTSè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–å¢åŠ è¶…æ—¶æ—¶é—´"))
            }
        }
    }

    /// è·å–å‚è€ƒéŸ³é¢‘å·¥å…·
    pub fn ref_audio_utilities(&self) -> Arc<Mutex<Option<RefAudioUtilities>>> {
        self.ref_audio_utilities.clone()
    }

    /// æ ¸å¿ƒè¿è¡Œæ—¶ - å‚è€ƒai00çš„å¤šä»»åŠ¡æ¶æ„
    /// ç°åœ¨ä½¿ç”¨å…±äº«Runtimeï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
    async fn run_core_runtime(
        shared_runtime: Arc<SharedRwkvRuntime>,
        request_rx: Receiver<DynamicTtsRequest>,
        config: DynamicBatchConfig,
    ) {
        // åˆ›å»ºæ¨ç†é€šé“
        let (infer_tx, infer_rx) = flume::unbounded::<InferBatch>();

        // å¯åŠ¨å¤šä¸ªæ¨ç†å·¥ä½œçº¿ç¨‹ï¼Œä½¿ç”¨å…±äº«Runtime
        for i in 0..config.max_concurrent_batches {
            let infer_rx_clone = infer_rx.clone();
            let shared_runtime_clone = shared_runtime.clone();
            let infer_config = config.clone();
            tokio::spawn(async move {
                Self::infer_worker(i, infer_rx_clone, shared_runtime_clone, infer_config).await;
            });
        }

        // å¯åŠ¨å•ä¸€è¯·æ±‚æ”¶é›†ä»»åŠ¡ï¼ˆé¿å…ç«äº‰ï¼‰
        let enqueue_infer_tx = infer_tx.clone();
        let enqueue_config = config.clone();
        let enqueue_request_rx = request_rx.clone();
        tokio::spawn(async move {
            Self::enqueue_worker(enqueue_request_rx, enqueue_infer_tx, enqueue_config).await;
        });

        info!("åŠ¨æ€æ‰¹å¤„ç†æ ¸å¿ƒè¿è¡Œæ—¶å¯åŠ¨å®Œæˆ");

        // ä¿æŒè¿è¡Œæ—¶æ´»è·ƒ
        loop {
            tokio::time::sleep(Duration::from_secs(1)).await;
            if request_rx.is_disconnected() {
                info!("è¯·æ±‚é€šé“æ–­å¼€ï¼Œæ ¸å¿ƒè¿è¡Œæ—¶é€€å‡º");
                break;
            }
        }
    }

    /// è¯·æ±‚æ”¶é›†å·¥ä½œçº¿ç¨‹ - å‚è€ƒai00çš„enqueueé€»è¾‘
    async fn enqueue_worker(
        request_rx: Receiver<DynamicTtsRequest>,
        infer_tx: Sender<InferBatch>,
        config: DynamicBatchConfig,
    ) {
        let mut pending_requests: VecDeque<DynamicTtsRequest> = VecDeque::new();
        let mut batch_counter = 0usize;

        info!("è¯·æ±‚æ”¶é›†å·¥ä½œçº¿ç¨‹å¯åŠ¨");

        loop {
            let collect_start = Instant::now();
            let collect_timeout = Duration::from_millis(config.collect_timeout_ms);

            // ä¼˜åŒ–çš„è¯·æ±‚æ”¶é›†ç­–ç•¥ï¼šå¿«é€Ÿæ”¶é›†å¹¶å‘è¯·æ±‚ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
            while pending_requests.len() < config.max_batch_size {
                let remaining_time = collect_timeout.saturating_sub(collect_start.elapsed());

                // å¦‚æœå·²æœ‰è¯·æ±‚ä¸”è¶…æ—¶æ—¶é—´åˆ°äº†ï¼Œç«‹å³å¤„ç†
                if !pending_requests.is_empty() && remaining_time.as_millis() == 0 {
                    break;
                }

                match tokio::time::timeout(remaining_time, request_rx.recv_async()).await {
                    Ok(Ok(mut request)) => {
                        request.batch_id = batch_counter;
                        let queue_wait_time = request.submitted_at.elapsed();
                        debug!(
                            "ğŸ“¥ æ”¶åˆ°æ–°è¯·æ±‚ï¼Œé˜Ÿåˆ—ç­‰å¾…æ—¶é—´: {:?}ï¼Œå½“å‰æ‰¹æ¬¡å¤§å°: {}",
                            queue_wait_time,
                            pending_requests.len() + 1
                        );
                        pending_requests.push_back(request);

                        // æ¿€è¿›åœ°æ”¶é›†æ‰€æœ‰ç«‹å³å¯ç”¨çš„è¯·æ±‚ï¼ˆéé˜»å¡ï¼‰
                        let mut quick_collect_count = 0;
                        let quick_collect_start = Instant::now();
                        while pending_requests.len() < config.max_batch_size
                            && quick_collect_count < 50
                        {
                            match request_rx.try_recv() {
                                Ok(mut req) => {
                                    req.batch_id = batch_counter;
                                    pending_requests.push_back(req);
                                    quick_collect_count += 1;
                                }
                                Err(_) => break,
                            }
                        }

                        if quick_collect_count > 0 {
                            debug!(
                                "å¿«é€Ÿæ”¶é›†åˆ° {} ä¸ªé¢å¤–è¯·æ±‚ï¼Œè€—æ—¶ {:?}",
                                quick_collect_count,
                                quick_collect_start.elapsed()
                            );
                        }

                        // å¦‚æœæ”¶é›†åˆ°å¤šä¸ªè¯·æ±‚ï¼Œç«‹å³å¤„ç†
                        if pending_requests.len() > 1 {
                            break;
                        }

                        // å¦‚æœåªæœ‰ä¸€ä¸ªè¯·æ±‚ï¼Œåªç­‰å¾…å¾ˆçŸ­æ—¶é—´ï¼ˆ10msï¼‰æ”¶é›†æ›´å¤šè¯·æ±‚
                        if pending_requests.len() == 1 && collect_start.elapsed().as_millis() >= 10
                        {
                            break;
                        }
                    }
                    Ok(Err(_)) => {
                        info!("è¯·æ±‚é€šé“å…³é—­ï¼Œå·¥ä½œçº¿ç¨‹é€€å‡º");
                        return;
                    }
                    Err(_) => {
                        // è¶…æ—¶ï¼Œå¦‚æœæœ‰è¯·æ±‚åˆ™ç«‹å³å¤„ç†
                        if !pending_requests.is_empty() {
                            break;
                        }
                    }
                }
            }

            // å¤„ç†æ”¶é›†åˆ°çš„è¯·æ±‚
            if !pending_requests.is_empty() {
                let batch_size = pending_requests.len();
                let batch_id = batch_counter;
                let collect_duration = collect_start.elapsed();
                batch_counter += 1;

                // è®¡ç®—é˜Ÿåˆ—ä¸­è¯·æ±‚çš„å¹³å‡ç­‰å¾…æ—¶é—´
                let avg_queue_wait = if !pending_requests.is_empty() {
                    let total_wait: Duration = pending_requests
                        .iter()
                        .map(|req| req.submitted_at.elapsed())
                        .sum();
                    total_wait / pending_requests.len() as u32
                } else {
                    Duration::ZERO
                };

                info!(
                    "ğŸ“¦ æ”¶é›†åˆ°æ‰¹æ¬¡ {}: {} ä¸ªè¯·æ±‚ï¼Œæ”¶é›†è€—æ—¶ {:?}ï¼Œå¹³å‡é˜Ÿåˆ—ç­‰å¾…æ—¶é—´: {:?}",
                    batch_id, batch_size, collect_duration, avg_queue_wait
                );

                Self::process_collected_batch(
                    pending_requests.drain(..).collect(),
                    &infer_tx,
                    batch_id,
                )
                .await;
            }
        }
    }

    /// å¤„ç†æ”¶é›†åˆ°çš„æ‰¹æ¬¡
    async fn process_collected_batch(
        requests: Vec<DynamicTtsRequest>,
        infer_tx: &Sender<InferBatch>,
        batch_id: usize,
    ) {
        let batch_size = requests.len();
        let process_start = Instant::now();
        let (result_tx, result_rx) = flume::unbounded();

        info!("ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {}: {} ä¸ªè¯·æ±‚", batch_id, batch_size);

        // è½¬æ¢ä¸ºæ‰¹å¤„ç†è¯·æ±‚
        let batch_requests: Vec<TtsBatchRequest> = requests
            .iter()
            .map(|req| TtsBatchRequest {
                text: req.text.clone(),
                property_tokens: req.property_tokens.clone(),
                ref_global_tokens: req.ref_global_tokens.clone(),
                ref_semantic_tokens: req.ref_semantic_tokens.clone(),
                args: req.args.clone(),
            })
            .collect();

        // å‘é€åˆ°æ¨ç†é˜Ÿåˆ—
        let infer_batch = InferBatch::Run {
            batch_id,
            requests: batch_requests,
            sender: result_tx,
        };

        debug!("ğŸ“¤ å‘é€æ‰¹æ¬¡ {} åˆ°æ¨ç†é˜Ÿåˆ—", batch_id);
        if let Err(e) = infer_tx.send_async(infer_batch).await {
            error!("âŒ å‘é€æ¨ç†æ‰¹æ¬¡ {} å¤±è´¥: {}", batch_id, e);
            // å‘é€é”™è¯¯ç»™æ‰€æœ‰è¯·æ±‚
            for request in requests {
                let _ = request
                    .response_tx
                    .send(Err(anyhow::anyhow!("æ¨ç†é˜Ÿåˆ—å‘é€å¤±è´¥")));
            }
            return;
        }

        // ç­‰å¾…æ¨ç†ç»“æœ
        debug!("â³ ç­‰å¾…æ‰¹æ¬¡ {} æ¨ç†ç»“æœ", batch_id);
        match result_rx.recv_async().await {
            Ok(results) => {
                let process_duration = process_start.elapsed();
                // æ£€æŸ¥ç»“æœæ•°é‡æ˜¯å¦åŒ¹é…
                if results.len() == batch_size {
                    // åˆ†å‘ç»“æœ
                    for (request, result) in requests.into_iter().zip(results.into_iter()) {
                        let _ = request.response_tx.send(Ok(result));
                    }
                    info!(
                        "âœ… æ‰¹æ¬¡ {} å¤„ç†å®Œæˆ: {} ä¸ªè¯·æ±‚ï¼Œæ€»è€—æ—¶: {:?}",
                        batch_id, batch_size, process_duration
                    );
                } else {
                    // ç»“æœæ•°é‡ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯æ¨ç†å¤±è´¥
                    error!(
                        "âŒ æ‰¹æ¬¡ {} ç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ› {}, å®é™… {}",
                        batch_id,
                        batch_size,
                        results.len()
                    );
                    // å‘é€é”™è¯¯ç»™æ‰€æœ‰è¯·æ±‚
                    for request in requests {
                        let _ = request
                            .response_tx
                            .send(Err(anyhow::anyhow!("æ¨ç†å¤±è´¥ï¼Œç»“æœæ•°é‡ä¸åŒ¹é…")));
                    }
                }
            }
            Err(e) => {
                let process_duration = process_start.elapsed();
                error!(
                    "âŒ æ¥æ”¶æ‰¹æ¬¡ {} æ¨ç†ç»“æœå¤±è´¥: {}ï¼Œè€—æ—¶: {:?}",
                    batch_id, e, process_duration
                );
                // å‘é€é”™è¯¯ç»™æ‰€æœ‰è¯·æ±‚
                for request in requests {
                    let _ = request
                        .response_tx
                        .send(Err(anyhow::anyhow!("æ¨ç†ç»“æœæ¥æ”¶å¤±è´¥")));
                }
            }
        }
    }

    /// æ¨ç†å·¥ä½œçº¿ç¨‹ - é‡æ„ç‰ˆï¼šä½¿ç”¨ç‹¬ç«‹çŠ¶æ€ç®¡ç†ç¡®ä¿çŠ¶æ€éš”ç¦»
    /// å…³é”®æ”¹è¿›ï¼šæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
    async fn infer_worker(
        worker_id: usize,
        infer_rx: Receiver<InferBatch>,
        shared_runtime: Arc<SharedRwkvRuntime>,
        _config: DynamicBatchConfig,
    ) {
        info!("ğŸ”§ æ¨ç†å·¥ä½œçº¿ç¨‹ {} å¯åŠ¨ï¼Œä½¿ç”¨ç‹¬ç«‹çŠ¶æ€ç®¡ç†æ¶æ„", worker_id);
        info!(
            "ğŸ”’ çŠ¶æ€éš”ç¦»ï¼šå·¥ä½œçº¿ç¨‹ {} å°†ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹æ¨ç†ä¸Šä¸‹æ–‡",
            worker_id
        );

        while let Ok(batch) = infer_rx.recv_async().await {
            match batch {
                InferBatch::Run {
                    batch_id,
                    requests,
                    sender,
                } => {
                    let batch_size = requests.len();
                    let infer_start = Instant::now();

                    info!(
                        "å·¥ä½œçº¿ç¨‹ {} å¼€å§‹æ¨ç†æ‰¹æ¬¡ {}: {} ä¸ªè¯·æ±‚ (ç‹¬ç«‹çŠ¶æ€æ¨¡å¼)",
                        worker_id, batch_id, batch_size
                    );

                    // ğŸ”§ å…³é”®æ”¹è¿›ï¼šä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
                    // ç¡®ä¿å®Œå…¨çš„çŠ¶æ€éš”ç¦»ï¼Œé¿å…å¹¶å‘è¯·æ±‚é—´çš„çŠ¶æ€æ±¡æŸ“
                    let result = Self::process_batch_with_independent_contexts(
                        shared_runtime.clone(),
                        requests,
                        batch_id as u64,
                    )
                    .await;

                    let infer_time = infer_start.elapsed();

                    match result {
                        Ok(results) => {
                            info!(
                                "å·¥ä½œçº¿ç¨‹ {} æ‰¹æ¬¡ {} æ¨ç†å®Œæˆ: {:.2}ms, å¹³å‡æ¯è¯·æ±‚: {:.2}ms",
                                worker_id,
                                batch_id,
                                infer_time.as_secs_f64() * 1000.0,
                                infer_time.as_secs_f64() * 1000.0 / batch_size as f64
                            );

                            if let Err(e) = sender.send_async(results).await {
                                error!("å‘é€æ¨ç†ç»“æœå¤±è´¥: {}", e);
                            }
                        }
                        Err(e) => {
                            error!("æ‰¹æ¬¡ {} æ¨ç†å¤±è´¥: {}", batch_id, e);
                            // å‘é€ä¸è¯·æ±‚æ•°é‡åŒ¹é…çš„é”™è¯¯ç»“æœ
                            let error_results: Vec<(Vec<i32>, Vec<i32>)> =
                                (0..batch_size).map(|_| (vec![], vec![])).collect();
                            let _ = sender.send_async(error_results).await;
                        }
                    }
                }
                InferBatch::Result { batch_id, sender } => {
                    // å¤„ç†ç»“æœè¯·æ±‚ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    warn!("æ”¶åˆ°ç»“æœè¯·æ±‚ {}ï¼Œå½“å‰å®ç°ä¸æ”¯æŒ", batch_id);
                    let _ = sender.send(vec![]);
                }
            }
        }

        info!("æ¨ç†å·¥ä½œçº¿ç¨‹ {} é€€å‡º", worker_id);
    }

    /// ä½¿ç”¨ç‹¬ç«‹ä¸Šä¸‹æ–‡å¤„ç†æ‰¹æ¬¡
    /// ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿çŠ¶æ€å®Œå…¨éš”ç¦»
    async fn process_batch_with_independent_contexts(
        shared_runtime: Arc<SharedRwkvRuntime>,
        requests: Vec<TtsBatchRequest>,
        batch_id: u64,
    ) -> Result<Vec<(Vec<i32>, Vec<i32>)>> {
        let batch_size = requests.len();
        let mut results = Vec::with_capacity(batch_size);

        info!(
            "ğŸ”§ ä¸ºæ‰¹æ¬¡ {} åˆ›å»º {} ä¸ªç‹¬ç«‹æ¨ç†ä¸Šä¸‹æ–‡",
            batch_id, batch_size
        );

        // ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡å¹¶é¡ºåºå¤„ç†ï¼ˆé¿å…GPUèµ„æºäº‰ç”¨ï¼‰
        // æ³¨æ„ï¼šè¿™é‡Œæ”¹ä¸ºé¡ºåºå¤„ç†è€Œä¸æ˜¯å¹¶è¡Œå¤„ç†ï¼Œå› ä¸ºGPUèµ„æºæ˜¯æœ‰é™çš„
        for (idx, request) in requests.into_iter().enumerate() {
            let shared_runtime_clone = shared_runtime.clone();
            let request_id = format!("batch_{}_req_{}", batch_id, idx);

            // åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
            let options = TtsInferOptions {
                temperature: request.args.temperature,
                top_k: request.args.top_k,
                top_p: request.args.top_p,
                seed: request.args.seed,
            };

            let infer_context = shared_runtime_clone
                .create_infer_context(request_id.clone(), request.text.clone(), options)
                .await?;

            // ä¿å­˜çŠ¶æ€IDç”¨äºæ¸…ç†
            let state_id = infer_context.state_id;

            // æ‰§è¡Œç‹¬ç«‹æ¨ç†
            let result = Self::execute_independent_inference(infer_context, request).await;

            // æ¸…ç†çŠ¶æ€
            shared_runtime_clone.cleanup_state(state_id).await;

            match result {
                Ok(res) => {
                    results.push(res);
                    info!("âœ… è¯·æ±‚ {} å¤„ç†å®Œæˆ", request_id);
                }
                Err(e) => {
                    error!("âŒ è¯·æ±‚ {} å¤„ç†å¤±è´¥: {}", request_id, e);
                    results.push((vec![], vec![]));
                }
            }
        }

        info!(
            "âœ… æ‰¹æ¬¡ {} ç‹¬ç«‹æ¨ç†å®Œæˆï¼Œå¤„ç†äº† {} ä¸ªè¯·æ±‚",
            batch_id,
            results.len()
        );

        Ok(results)
    }

    /// æ‰§è¡Œç‹¬ç«‹æ¨ç†
    async fn execute_independent_inference(
        infer_context: TtsInferContext,
        request: TtsBatchRequest,
    ) -> Result<(Vec<i32>, Vec<i32>)> {
        use web_rwkv::runtime::infer::{RnnInput, RnnInputBatch, RnnOption};

        let request_id = &infer_context.request_id;
        info!(
            "ğŸš€ [{}] å¼€å§‹ç‹¬ç«‹æ¨ç† - æ–‡æœ¬: '{}'",
            request_id, request.text
        );

        // ä¸ºæœ¬æ¬¡è¯·æ±‚åˆ›å»ºç‹¬ç«‹RNGï¼ˆå¯å¤ç°ä¸”äº’ä¸å¹²æ‰°ï¼‰
        let mut rng: rand::rngs::StdRng = if let Some(seed) = request.args.seed {
            rand::rngs::StdRng::seed_from_u64(seed)
        } else {
            rand::rngs::StdRng::from_rng(rand::thread_rng()).expect("failed to seed StdRng")
        };

        // Acquire runtime semaphore for the entire inference to ensure isolation
        let _runtime_permit = infer_context
            .runtime_semaphore
            .acquire()
            .await
            .map_err(|e| anyhow::anyhow!("æ— æ³•è·å–è¿è¡Œæ—¶ä¿¡å·é‡: {}", e))?;

        info!("ğŸ”’ [{}] å·²è·å–ä¿¡å·é‡è®¸å¯ï¼Œå¼€å§‹æ¨ç†", request_id);

        // è·å–tokenizerå’Œruntime
        let tokenizer = &infer_context.tokenizer;
        let runtime = &infer_context.runtime;
        let state = &infer_context.state;

        // ç¼–ç æ–‡æœ¬
        let text_tokens_u32: Vec<u32> = tokenizer
            .encode(request.text.as_bytes())
            .map_err(|e| anyhow::anyhow!(e.to_string()))?;
        let text_tokens: Vec<i32> = text_tokens_u32.into_iter().map(|t| t as i32).collect();

        debug!(
            "ğŸ” [{}] æ–‡æœ¬ç¼–ç ç»“æœ: {:?} (é•¿åº¦: {})",
            request_id,
            text_tokens,
            text_tokens.len()
        );

        // æ„å»ºè¾“å…¥åºåˆ—ï¼šå±æ€§tokens + TTS_TAG_2 + æ–‡æœ¬tokens + TTS_TAG_0
        let mut input_tokens: Vec<i32> = Vec::new();
        input_tokens.extend_from_slice(&request.property_tokens);
        input_tokens.push(crate::rwkv_sampler::TTS_TAG_2);
        input_tokens.extend_from_slice(&text_tokens);
        input_tokens.push(crate::rwkv_sampler::TTS_TAG_0);

        debug!(
            "ğŸ” [{}] å®Œæ•´è¾“å…¥åºåˆ—: {:?} (é•¿åº¦: {})",
            request_id,
            input_tokens,
            input_tokens.len()
        );

        // === Prefill é˜¶æ®µ ===
        let input_tokens_u32: Vec<u32> = input_tokens.iter().map(|&t| t as u32).collect();
        let token_chunk_size = 64usize;

        info!("ğŸ”§ [{}] Prefillé˜¶æ®µ - åˆå§‹åŒ–ç‹¬ç«‹çŠ¶æ€", request_id);

        // åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
        let batch = RnnInputBatch::new(input_tokens_u32.clone(), RnnOption::Last);
        let mut inference = RnnInput::new(vec![batch], token_chunk_size);

        // ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€ï¼Œç¡®ä¿çŠ¶æ€éš”ç¦»
        {
            let initial_state = state.lock().await.init();
            state.lock().await.load(initial_state, 0)?;
            info!("ğŸ”§ [{}] å·²ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€", request_id);
        }

        // æ¶ˆåŒ–è¾“å…¥ç›´åˆ°äº§ç”Ÿè¾“å‡º
        let last_logits: Vec<f32> = loop {
            let (remaining_input, output) = runtime.infer(inference.clone()).await?;
            inference = remaining_input;
            if !output.is_empty() && output[0].0.size() > 0 {
                break output[0].0.clone().to_vec();
            }
        };

        // === Global é˜¶æ®µ ===
        let mut global_tokens: Vec<i32> = Vec::new();
        let mut semantic_tokens: Vec<i32> = Vec::new();

        // è®¾ç½®é‡‡æ ·å‚æ•°
        let mut args_global = request.args.clone();
        let mut args_sem = request.args.clone();
        if args_global.top_k == 0 {
            args_global.top_k = 20;
        }
        if args_sem.top_k == 0 {
            args_sem.top_k = 80;
        }

        // ç”Ÿæˆ32ä¸ªglobal tokens
        let global_tokens_size: usize = 32;
        info!(
            "ğŸ” [{}] å¼€å§‹ç”Ÿæˆ {} ä¸ªglobal tokens",
            request_id, global_tokens_size
        );

        for i in 0..global_tokens_size {
            let logits: Vec<f32> = if i == 0 {
                last_logits.clone()
            } else {
                // ç»§ç»­æ¨ç†è·å–logits - ä½¿ç”¨ç°æœ‰inferenceä¸Šä¸‹æ–‡
                loop {
                    let (next_inference, output) = runtime.infer(inference.clone()).await?;
                    inference = next_inference;
                    if output[0].0.size() > 0 {
                        break output[0].0.clone().to_vec();
                    }
                }
            };

            // ä»…åœ¨[0..4096)èŒƒå›´å†…é‡‡æ ·
            let vocab_global = if logits.len() < 4096 {
                logits.len()
            } else {
                4096
            };
            let next_id = Self::sample_logits(&logits[..vocab_global], &args_global, &mut rng)?;

            global_tokens.push(next_id as i32);

            // åé¦ˆåˆ°æ¨¡å‹ï¼š+8196ï¼ˆGLOBAL_TOKEN_OFFSETï¼‰
            let feed_id = (next_id as i32 + crate::rwkv_sampler::GLOBAL_TOKEN_OFFSET) as u32;
            inference.batches[0].push(feed_id);
        }

        // === åˆ‡æ¢åˆ° Semantic é˜¶æ®µ ===
        inference.batches[0].push(crate::rwkv_sampler::TTS_TAG_1 as u32);
        // è®©æ ‡ç­¾ç”Ÿæ•ˆï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºï¼Œå¹¶ä¿ç•™logitsä¾›é¦–æ­¥ä½¿ç”¨
        let last_sem_logits: Vec<f32> = loop {
            let (next_inference, output) = runtime.infer(inference).await?;
            inference = next_inference;
            if output[0].0.size() > 0 {
                break output[0].0.clone().to_vec();
            }
        };

        // è¯­ä¹‰é˜¶æ®µï¼šé™åˆ¶æœ€å¤§ç”Ÿæˆæ­¥æ•°ä¸º2048
        let semantic_limit: usize = usize::min(request.args.max_tokens, 2048);
        info!(
            "ğŸ” [{}] å¼€å§‹ç”Ÿæˆsemantic tokensï¼Œæœ€å¤§é™åˆ¶: {}",
            request_id, semantic_limit
        );

        for i in 0..semantic_limit {
            let logits: Vec<f32> = if i == 0 {
                last_sem_logits.clone()
            } else {
                loop {
                    let (next_inference, output) = runtime.infer(inference.clone()).await?;
                    inference = next_inference;
                    if output[0].0.size() > 0 {
                        break output[0].0.clone().to_vec();
                    }
                }
            };

            // è¯­ä¹‰é˜¶æ®µä»…é‡‡æ · [0..8192]ï¼ˆåŒ…å«EOSï¼‰ï¼Œå±è”½TTS_TAG_*ä¸å…¶å®ƒåŸŸ
            let mut logits_masked = logits.clone();
            for (i, v) in logits_masked.iter_mut().enumerate() {
                if i > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
                    *v = f32::NEG_INFINITY;
                }
            }
            for tag in [
                crate::rwkv_sampler::TTS_TAG_0,
                crate::rwkv_sampler::TTS_TAG_1,
                crate::rwkv_sampler::TTS_TAG_2,
            ] {
                let idx = tag as usize;
                if idx < logits_masked.len() {
                    logits_masked[idx] = f32::NEG_INFINITY;
                }
            }

            // ä¸C++ä¸€è‡´ï¼šè¯­ä¹‰é˜¶æ®µé¦–æ­¥ç¦æ­¢EOS
            if i == 0 {
                let eos_idx = crate::rwkv_sampler::TTS_EOS_TOKEN as usize;
                if eos_idx < logits_masked.len() {
                    logits_masked[eos_idx] = f32::NEG_INFINITY;
                }
            }

            let next_id = Self::sample_logits(&logits_masked, &args_sem, &mut rng)?;
            if next_id == crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
                info!("ğŸ” [{}] é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ", request_id);
                break;
            }

            semantic_tokens.push(next_id as i32);

            // åé¦ˆåˆ°æ¨¡å‹ï¼šè¯­ä¹‰é˜¶æ®µç›´æ¥ä½¿ç”¨åŸå§‹tokenï¼ˆä¸åŠ åç§»ï¼‰
            inference.batches[0].push(next_id as u32);
        }

        info!(
            "âœ… [{}] ç”Ÿæˆå®Œæˆ: global tokens: {} ä¸ª, semantic tokens: {} ä¸ª",
            request_id,
            global_tokens.len(),
            semantic_tokens.len()
        );

        Ok((global_tokens, semantic_tokens))
    }

    /// è·å–é…ç½®
    pub fn config(&self) -> &DynamicBatchConfig {
        &self.config
    }
}

/// å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å•ä¾‹
static GLOBAL_DYNAMIC_BATCH_MANAGER: std::sync::OnceLock<Arc<DynamicBatchManager>> =
    std::sync::OnceLock::new();

/// åˆå§‹åŒ–å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨ï¼ˆæ”¯æŒé‡åŒ–é…ç½®ï¼‰
pub async fn init_global_dynamic_batch_manager(
    model_path: &str,
    vocab_path: &str,
    config: DynamicBatchConfig,
    quant_config: Option<std::collections::HashMap<usize, web_rwkv::runtime::model::Quant>>,
) -> Result<()> {
    let manager = DynamicBatchManager::new(model_path, vocab_path, config, quant_config).await?;

    GLOBAL_DYNAMIC_BATCH_MANAGER
        .set(Arc::new(manager))
        .map_err(|_| anyhow::anyhow!("å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–"))?;

    Ok(())
}

/// è·å–å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å®ä¾‹
pub fn get_global_dynamic_batch_manager() -> Result<Arc<DynamicBatchManager>> {
    GLOBAL_DYNAMIC_BATCH_MANAGER
        .get()
        .cloned()
        .ok_or_else(|| anyhow::anyhow!("å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨æœªåˆå§‹åŒ–"))
}
