use anyhow::Result;
use flume::{Receiver, Sender};

use rand::SeedableRng;
use std::collections::VecDeque;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::oneshot;
use tracing::{debug, error, info, warn};

// å¯¼å…¥æ‹†åˆ†çš„æ¨¡å—
use crate::batch_types::*;
// use crate::feature_extractor::*; // æš‚æ—¶æ³¨é‡Šæ‰æœªä½¿ç”¨çš„å¯¼å…¥
// use crate::sampler_manager::*; // æš‚æ—¶æ³¨é‡Šæ‰æœªä½¿ç”¨çš„å¯¼å…¥
use crate::shared_runtime::*;

// å¼•å…¥æ–°çš„æ¨ç†æ¨¡å—
use crate::normal_mode_inference::execute_normal_inference;
use crate::zero_shot_inference::execute_zero_shot_inference;

/// åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨
/// è´Ÿè´£æ”¶é›†è¯·æ±‚ã€ç»„ç»‡æ‰¹æ¬¡ã€åè°ƒæ¨ç†å·¥ä½œçº¿ç¨‹
pub struct DynamicBatchManager {
    /// é…ç½®
    config: DynamicBatchConfig,
    /// è¯·æ±‚å‘é€é€šé“
    request_tx: Sender<DynamicTtsRequest>,
    /// å…±äº«è¿è¡Œæ—¶
    _shared_runtime: Arc<SharedRwkvRuntime>,
}

impl DynamicBatchManager {
    /// åˆ›å»ºæ–°çš„åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨
    pub async fn new(
        model_path: &str,
        vocab_path: &str,
        config: DynamicBatchConfig,
        quant_config: Option<std::collections::HashMap<usize, web_rwkv::runtime::model::Quant>>,
    ) -> Result<Self> {
        info!("ğŸš€ åˆå§‹åŒ–åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨");
        info!("ğŸ“Š é…ç½®: {:?}", config);

        // åˆ›å»ºå…±äº«è¿è¡Œæ—¶
        let shared_runtime = Arc::new(
            SharedRwkvRuntime::new(
                model_path.to_string(),
                vocab_path.to_string(),
                quant_config,
                config.clone(),
            )
            .await?,
        );

        // åˆ›å»ºè¯·æ±‚é€šé“
        let (request_tx, request_rx) = flume::unbounded();
        let (infer_tx, infer_rx) = flume::unbounded();

        // å¯åŠ¨æ ¸å¿ƒè¿è¡Œæ—¶
        let shared_runtime_clone = shared_runtime.clone();
        let config_clone = config.clone();
        tokio::spawn(async move {
            Self::run_core_runtime(shared_runtime_clone, request_rx, infer_tx, config_clone).await;
        });

        // å¯åŠ¨æ¨ç†å·¥ä½œçº¿ç¨‹
        for worker_id in 0..config.max_concurrent_batches {
            let infer_rx_clone = infer_rx.clone();
            let shared_runtime_clone = shared_runtime.clone();
            let config_clone = config.clone();
            tokio::spawn(async move {
                Self::infer_worker(
                    worker_id,
                    infer_rx_clone,
                    shared_runtime_clone,
                    config_clone,
                )
                .await;
            });
        }

        info!("âœ… åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ");

        Ok(Self {
            config,
            request_tx,
            _shared_runtime: shared_runtime,
        })
    }

    /// ç”ŸæˆTTS
    pub async fn generate_tts(
        &self,
        text: String,
        property_tokens: Vec<i32>,
        ref_global_tokens: Option<Vec<i32>>,
        ref_semantic_tokens: Option<Vec<i32>>,
        args: crate::rwkv_sampler::SamplerArgs,
    ) -> Result<(Vec<i32>, Vec<i32>)> {
        let (response_tx, response_rx) = oneshot::channel();

        let request = DynamicTtsRequest {
            text,
            property_tokens,
            ref_global_tokens,
            ref_semantic_tokens,
            args,
            response_tx,
            submitted_at: Instant::now(),
            batch_id: 0, // å°†åœ¨æ”¶é›†é˜¶æ®µè®¾ç½®
        };

        self.request_tx
            .send_async(request)
            .await
            .map_err(|e| anyhow::anyhow!("å‘é€è¯·æ±‚å¤±è´¥: {}", e))?;

        response_rx
            .await
            .map_err(|e| anyhow::anyhow!("æ¥æ”¶å“åº”å¤±è´¥: {}", e))?
    }

    /// æ ¸å¿ƒè¿è¡Œæ—¶ - è´Ÿè´£æ”¶é›†è¯·æ±‚å¹¶åˆ†å‘åˆ°æ¨ç†å·¥ä½œçº¿ç¨‹
    async fn run_core_runtime(
        _shared_runtime: Arc<SharedRwkvRuntime>,
        request_rx: Receiver<DynamicTtsRequest>,
        infer_tx: Sender<InferBatch>,
        config: DynamicBatchConfig,
    ) {
        info!("ğŸ”§ æ ¸å¿ƒè¿è¡Œæ—¶å¯åŠ¨");

        // å¯åŠ¨è¯·æ±‚æ”¶é›†å·¥ä½œçº¿ç¨‹
        tokio::spawn(Self::enqueue_worker(request_rx, infer_tx, config));

        // ä¿æŒè¿è¡Œæ—¶æ´»è·ƒ
        loop {
            tokio::time::sleep(Duration::from_secs(1)).await;
        }
    }

    /// è¯·æ±‚æ”¶é›†å·¥ä½œçº¿ç¨‹
    async fn enqueue_worker(
        request_rx: Receiver<DynamicTtsRequest>,
        infer_tx: Sender<InferBatch>,
        config: DynamicBatchConfig,
    ) {
        info!("ğŸ“¥ è¯·æ±‚æ”¶é›†å·¥ä½œçº¿ç¨‹å¯åŠ¨");
        let mut pending_requests = VecDeque::new();
        let mut batch_counter = 1usize;

        loop {
            let collect_start = Instant::now();

            // æ”¶é›†è¯·æ±‚çš„é€»è¾‘
            loop {
                let timeout = Duration::from_millis(config.collect_timeout_ms);
                match tokio::time::timeout(timeout, request_rx.recv_async()).await {
                    Ok(Ok(mut request)) => {
                        request.batch_id = batch_counter;
                        pending_requests.push_back(request);

                        // æ¿€è¿›åœ°æ”¶é›†æ‰€æœ‰ç«‹å³å¯ç”¨çš„è¯·æ±‚ï¼ˆéé˜»å¡ï¼‰
                        let mut quick_collect_count = 0;
                        let quick_collect_start = Instant::now();
                        while pending_requests.len() < config.max_batch_size
                            && quick_collect_count < 50
                        {
                            match request_rx.try_recv() {
                                Ok(mut req) => {
                                    req.batch_id = batch_counter;
                                    pending_requests.push_back(req);
                                    quick_collect_count += 1;
                                }
                                Err(_) => break,
                            }
                        }

                        if quick_collect_count > 0 {
                            debug!(
                                "å¿«é€Ÿæ”¶é›†åˆ° {} ä¸ªé¢å¤–è¯·æ±‚ï¼Œè€—æ—¶ {:?}",
                                quick_collect_count,
                                quick_collect_start.elapsed()
                            );
                        }

                        // å¦‚æœæ”¶é›†åˆ°å¤šä¸ªè¯·æ±‚ï¼Œç«‹å³å¤„ç†
                        if pending_requests.len() > 1 {
                            break;
                        }

                        // å¦‚æœåªæœ‰ä¸€ä¸ªè¯·æ±‚ï¼Œåªç­‰å¾…å¾ˆçŸ­æ—¶é—´ï¼ˆ10msï¼‰æ”¶é›†æ›´å¤šè¯·æ±‚
                        if pending_requests.len() == 1 && collect_start.elapsed().as_millis() >= 10
                        {
                            break;
                        }
                    }
                    Ok(Err(_)) => {
                        info!("è¯·æ±‚é€šé“å…³é—­ï¼Œå·¥ä½œçº¿ç¨‹é€€å‡º");
                        return;
                    }
                    Err(_) => {
                        // è¶…æ—¶ï¼Œå¦‚æœæœ‰è¯·æ±‚åˆ™ç«‹å³å¤„ç†
                        if !pending_requests.is_empty() {
                            break;
                        }
                    }
                }
            }

            // å¤„ç†æ”¶é›†åˆ°çš„è¯·æ±‚
            if !pending_requests.is_empty() {
                let batch_size = pending_requests.len();
                let batch_id = batch_counter;
                let collect_duration = collect_start.elapsed();
                batch_counter += 1;

                // è®¡ç®—é˜Ÿåˆ—ä¸­è¯·æ±‚çš„å¹³å‡ç­‰å¾…æ—¶é—´
                let avg_queue_wait = if !pending_requests.is_empty() {
                    let total_wait: Duration = pending_requests
                        .iter()
                        .map(|req| req.submitted_at.elapsed())
                        .sum();
                    total_wait / pending_requests.len() as u32
                } else {
                    Duration::ZERO
                };

                info!(
                    "ğŸ“¦ æ”¶é›†åˆ°æ‰¹æ¬¡ {}: {} ä¸ªè¯·æ±‚ï¼Œæ”¶é›†è€—æ—¶ {:?}ï¼Œå¹³å‡é˜Ÿåˆ—ç­‰å¾…æ—¶é—´: {:?}",
                    batch_id, batch_size, collect_duration, avg_queue_wait
                );

                Self::process_collected_batch(
                    pending_requests.drain(..).collect(),
                    &infer_tx,
                    batch_id,
                )
                .await;
            }
        }
    }

    /// å¤„ç†æ”¶é›†åˆ°çš„æ‰¹æ¬¡
    async fn process_collected_batch(
        requests: Vec<DynamicTtsRequest>,
        infer_tx: &Sender<InferBatch>,
        batch_id: usize,
    ) {
        let batch_size = requests.len();
        let process_start = Instant::now();
        let (result_tx, result_rx) = flume::unbounded();

        info!("ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {}: {} ä¸ªè¯·æ±‚", batch_id, batch_size);

        // è½¬æ¢ä¸ºæ‰¹å¤„ç†è¯·æ±‚
        let batch_requests: Vec<crate::rwkv_sampler::TtsBatchRequest> = requests
            .iter()
            .map(|req| crate::rwkv_sampler::TtsBatchRequest {
                text: req.text.clone(),
                property_tokens: req.property_tokens.clone(),
                ref_global_tokens: req.ref_global_tokens.clone(),
                ref_semantic_tokens: req.ref_semantic_tokens.clone(),
                args: req.args.clone(),
            })
            .collect();

        // å‘é€åˆ°æ¨ç†é˜Ÿåˆ—
        let infer_batch = InferBatch::Run {
            batch_id,
            requests: batch_requests,
            sender: result_tx,
        };

        debug!("ğŸ“¤ å‘é€æ‰¹æ¬¡ {} åˆ°æ¨ç†é˜Ÿåˆ—", batch_id);
        if let Err(e) = infer_tx.send_async(infer_batch).await {
            error!("âŒ å‘é€æ¨ç†æ‰¹æ¬¡ {} å¤±è´¥: {}", batch_id, e);
            // å‘é€é”™è¯¯ç»™æ‰€æœ‰è¯·æ±‚
            for request in requests {
                let _ = request
                    .response_tx
                    .send(Err(anyhow::anyhow!("æ¨ç†é˜Ÿåˆ—å‘é€å¤±è´¥")));
            }
            return;
        }

        // ç­‰å¾…æ¨ç†ç»“æœ
        debug!("â³ ç­‰å¾…æ‰¹æ¬¡ {} æ¨ç†ç»“æœ", batch_id);
        match result_rx.recv_async().await {
            Ok(results) => {
                let process_duration = process_start.elapsed();
                // æ£€æŸ¥ç»“æœæ•°é‡æ˜¯å¦åŒ¹é…
                if results.len() == batch_size {
                    // åˆ†å‘ç»“æœ
                    for (request, result) in requests.into_iter().zip(results.into_iter()) {
                        let _ = request.response_tx.send(Ok(result));
                    }
                    info!(
                        "âœ… æ‰¹æ¬¡ {} å¤„ç†å®Œæˆ: {} ä¸ªè¯·æ±‚ï¼Œæ€»è€—æ—¶: {:?}",
                        batch_id, batch_size, process_duration
                    );
                } else {
                    // ç»“æœæ•°é‡ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯æ¨ç†å¤±è´¥
                    error!(
                        "âŒ æ‰¹æ¬¡ {} ç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ› {}, å®é™… {}",
                        batch_id,
                        batch_size,
                        results.len()
                    );
                    // å‘é€é”™è¯¯ç»™æ‰€æœ‰è¯·æ±‚
                    for request in requests {
                        let _ = request
                            .response_tx
                            .send(Err(anyhow::anyhow!("æ¨ç†å¤±è´¥ï¼Œç»“æœæ•°é‡ä¸åŒ¹é…")));
                    }
                }
            }
            Err(e) => {
                let process_duration = process_start.elapsed();
                error!(
                    "âŒ æ¥æ”¶æ‰¹æ¬¡ {} æ¨ç†ç»“æœå¤±è´¥: {}ï¼Œè€—æ—¶: {:?}",
                    batch_id, e, process_duration
                );
                // å‘é€é”™è¯¯ç»™æ‰€æœ‰è¯·æ±‚
                for request in requests {
                    let _ = request
                        .response_tx
                        .send(Err(anyhow::anyhow!("æ¨ç†ç»“æœæ¥æ”¶å¤±è´¥")));
                }
            }
        }
    }

    /// æ¨ç†å·¥ä½œçº¿ç¨‹ - é‡æ„ç‰ˆï¼šä½¿ç”¨ç‹¬ç«‹çŠ¶æ€ç®¡ç†ç¡®ä¿çŠ¶æ€éš”ç¦»
    /// å…³é”®æ”¹è¿›ï¼šæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
    async fn infer_worker(
        worker_id: usize,
        infer_rx: Receiver<InferBatch>,
        shared_runtime: Arc<SharedRwkvRuntime>,
        _config: DynamicBatchConfig,
    ) {
        info!("ğŸ”§ æ¨ç†å·¥ä½œçº¿ç¨‹ {} å¯åŠ¨ï¼Œä½¿ç”¨ç‹¬ç«‹çŠ¶æ€ç®¡ç†æ¶æ„", worker_id);
        info!(
            "ğŸ”’ çŠ¶æ€éš”ç¦»ï¼šå·¥ä½œçº¿ç¨‹ {} å°†ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹æ¨ç†ä¸Šä¸‹æ–‡",
            worker_id
        );

        while let Ok(batch) = infer_rx.recv_async().await {
            match batch {
                InferBatch::Run {
                    batch_id,
                    requests,
                    sender,
                } => {
                    let batch_size = requests.len();
                    let infer_start = Instant::now();

                    info!(
                        "å·¥ä½œçº¿ç¨‹ {} å¼€å§‹æ¨ç†æ‰¹æ¬¡ {}: {} ä¸ªè¯·æ±‚ (ç‹¬ç«‹çŠ¶æ€æ¨¡å¼)",
                        worker_id, batch_id, batch_size
                    );

                    // ğŸ”§ å…³é”®æ”¹è¿›ï¼šä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
                    // ç¡®ä¿å®Œå…¨çš„çŠ¶æ€éš”ç¦»ï¼Œé¿å…å¹¶å‘è¯·æ±‚é—´çš„çŠ¶æ€æ±¡æŸ“
                    let result = Self::process_batch_with_independent_contexts(
                        shared_runtime.clone(),
                        requests,
                        batch_id as u64,
                    )
                    .await;

                    let infer_time = infer_start.elapsed();

                    match result {
                        Ok(results) => {
                            info!(
                                "å·¥ä½œçº¿ç¨‹ {} æ‰¹æ¬¡ {} æ¨ç†å®Œæˆ: {:.2}ms, å¹³å‡æ¯è¯·æ±‚: {:.2}ms",
                                worker_id,
                                batch_id,
                                infer_time.as_secs_f64() * 1000.0,
                                infer_time.as_secs_f64() * 1000.0 / batch_size as f64
                            );

                            if let Err(e) = sender.send_async(results).await {
                                error!("å‘é€æ¨ç†ç»“æœå¤±è´¥: {}", e);
                            }
                        }
                        Err(e) => {
                            error!("æ‰¹æ¬¡ {} æ¨ç†å¤±è´¥: {}", batch_id, e);
                            // å‘é€ä¸è¯·æ±‚æ•°é‡åŒ¹é…çš„é”™è¯¯ç»“æœ
                            let error_results: Vec<(Vec<i32>, Vec<i32>)> =
                                (0..batch_size).map(|_| (vec![], vec![])).collect();
                            let _ = sender.send_async(error_results).await;
                        }
                    }
                }
                InferBatch::Result { batch_id, sender } => {
                    // å¤„ç†ç»“æœè¯·æ±‚ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    warn!("æ”¶åˆ°ç»“æœè¯·æ±‚ {}ï¼Œå½“å‰å®ç°ä¸æ”¯æŒ", batch_id);
                    let _ = sender.send(vec![]);
                }
            }
        }

        info!("æ¨ç†å·¥ä½œçº¿ç¨‹ {} é€€å‡º", worker_id);
    }

    /// ä½¿ç”¨ç‹¬ç«‹ä¸Šä¸‹æ–‡å¤„ç†æ‰¹æ¬¡
    /// ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿çŠ¶æ€å®Œå…¨éš”ç¦»
    async fn process_batch_with_independent_contexts(
        shared_runtime: Arc<SharedRwkvRuntime>,
        requests: Vec<crate::rwkv_sampler::TtsBatchRequest>,
        batch_id: u64,
    ) -> Result<Vec<(Vec<i32>, Vec<i32>)>> {
        let batch_size = requests.len();
        let mut results = Vec::with_capacity(batch_size);

        info!(
            "ğŸ”§ ä¸ºæ‰¹æ¬¡ {} åˆ›å»º {} ä¸ªç‹¬ç«‹æ¨ç†ä¸Šä¸‹æ–‡",
            batch_id, batch_size
        );

        // ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡å¹¶é¡ºåºå¤„ç†ï¼ˆé¿å…GPUèµ„æºäº‰ç”¨ï¼‰
        // æ³¨æ„ï¼šè¿™é‡Œæ”¹ä¸ºé¡ºåºå¤„ç†è€Œä¸æ˜¯å¹¶è¡Œå¤„ç†ï¼Œå› ä¸ºGPUèµ„æºæ˜¯æœ‰é™çš„
        for request in requests.into_iter() {
            let shared_runtime_clone = shared_runtime.clone();
            // ç»Ÿä¸€ä½¿ç”¨å…¨å±€è¯·æ±‚IDå‘½åï¼šreq_<number>
            let request_id = shared_runtime_clone.generate_request_id();

            // æ£€æµ‹æ˜¯å¦ä¸ºå£°éŸ³å…‹éš†åœºæ™¯
            let is_voice_cloning =
                request.ref_global_tokens.is_some() || request.ref_semantic_tokens.is_some();

            // åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
            let options = TtsInferOptions {
                temperature: request.args.temperature,
                top_k: request.args.top_k,
                top_p: request.args.top_p,
                seed: if is_voice_cloning {
                    // å£°éŸ³å…‹éš†æ—¶å¿½ç•¥ç”¨æˆ·æä¾›çš„seedå‚æ•°ï¼Œç¡®ä¿ç¡®å®šæ€§
                    info!(
                        "ğŸ¯ [{}] å£°éŸ³å…‹éš†åœºæ™¯ï¼šå¿½ç•¥ç”¨æˆ·seedå‚æ•°ï¼Œä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·",
                        request_id
                    );
                    None
                } else {
                    request.args.seed
                },
                voice_fidelity: request.args.voice_fidelity,
                layered_randomness: request.args.layered_randomness.clone(),
                sampling: None,
                token_chunk_size: request.args.token_chunk_size,
            };

            let infer_context = shared_runtime_clone
                .create_infer_context(request_id.clone(), request.text.clone(), options)
                .await?;

            // ä¿å­˜çŠ¶æ€IDç”¨äºæ¸…ç†
            let state_id = infer_context.state_id;

            // æ‰§è¡Œç‹¬ç«‹æ¨ç†
            let result = Self::execute_independent_inference(infer_context, request).await;

            // æ¸…ç†çŠ¶æ€
            shared_runtime_clone.cleanup_state(state_id).await;

            match result {
                Ok(res) => {
                    results.push(res);
                    info!("âœ… è¯·æ±‚ {} å¤„ç†å®Œæˆ", request_id);
                }
                Err(e) => {
                    error!("âŒ è¯·æ±‚ {} å¤„ç†å¤±è´¥: {}", request_id, e);
                    results.push((vec![], vec![]));
                }
            }
        }

        info!(
            "âœ… æ‰¹æ¬¡ {} ç‹¬ç«‹æ¨ç†å®Œæˆï¼Œå¤„ç†äº† {} ä¸ªè¯·æ±‚",
            batch_id,
            results.len()
        );

        Ok(results)
    }

    /// æ‰§è¡Œç‹¬ç«‹æ¨ç†
    async fn execute_independent_inference(
        infer_context: TtsInferContext,
        request: crate::rwkv_sampler::TtsBatchRequest,
    ) -> Result<(Vec<i32>, Vec<i32>)> {
        let request_id = &infer_context.request_id;
        info!(
            "ğŸš€ [{}] å¼€å§‹ç‹¬ç«‹æ¨ç† - æ–‡æœ¬: '{}'",
            request_id, request.text
        );

        // æ£€æµ‹æ˜¯å¦ä¸ºå£°éŸ³å…‹éš†åœºæ™¯
        let is_voice_cloning =
            request.ref_global_tokens.is_some() || request.ref_semantic_tokens.is_some();

        // ä¸ºæœ¬æ¬¡è¯·æ±‚åˆ›å»ºç‹¬ç«‹RNGï¼ˆå¯å¤ç°ä¸”äº’ä¸å¹²æ‰°ï¼‰
        let rng: rand::rngs::StdRng = if is_voice_cloning {
            // å£°éŸ³å…‹éš†æ—¶ä¸ä½¿ç”¨éšæœºæ•°ï¼Œä½¿ç”¨å›ºå®šç§å­ç¡®ä¿ç¡®å®šæ€§
            info!(
                "ğŸ¯ [{}] å£°éŸ³å…‹éš†æ¨¡å¼ï¼šä½¿ç”¨å›ºå®šç§å­ç¡®ä¿ç»“æœä¸€è‡´æ€§",
                request_id
            );
            rand::rngs::StdRng::seed_from_u64(0) // ä½¿ç”¨å›ºå®šç§å­
        } else if let Some(seed) = request.args.seed {
            rand::rngs::StdRng::seed_from_u64(seed)
        } else {
            rand::rngs::StdRng::from_rng(rand::thread_rng()).expect("failed to seed StdRng")
        };

        // Acquire runtime semaphore for the entire inference to ensure isolation
        let _runtime_permit = infer_context
            .runtime_semaphore
            .acquire()
            .await
            .map_err(|e| anyhow::anyhow!("æ— æ³•è·å–è¿è¡Œæ—¶ä¿¡å·é‡: {}", e))?;

        info!("ğŸ”’ [{}] å·²è·å–ä¿¡å·é‡è®¸å¯ï¼Œå¼€å§‹æ¨ç†", request_id);

        // è·å–tokenizer
        let tokenizer = &infer_context.tokenizer;

        // ç¼–ç æ–‡æœ¬
        let text_tokens_u32: Vec<u32> = tokenizer
            .encode(request.text.as_bytes())
            .map_err(|e| anyhow::anyhow!(e.to_string()))?;
        let text_tokens_raw: Vec<i32> = text_tokens_u32.into_iter().map(|t| t as i32).collect();

        // æ ¹æ®C++ä»£ç é€»è¾‘ï¼Œæ–‡æœ¬tokensç›´æ¥ä½¿ç”¨åŸå§‹IDï¼Œä¸éœ€è¦ä»»ä½•åç§»
        let text_tokens: Vec<i32> = text_tokens_raw.clone();

        debug!(
            "ğŸ” [{}] æ–‡æœ¬ç¼–ç ç»“æœ: åŸå§‹={:?}, æœ€ç»ˆ={:?} (é•¿åº¦: {})",
            request_id,
            text_tokens_raw,
            text_tokens,
            text_tokens.len()
        );

        // é‡Šæ”¾åŸå§‹tokenså˜é‡
        drop(text_tokens_raw);

        // æ£€æµ‹æ˜¯å¦ä¸ºZero-shotæ¨¡å¼ï¼ˆæœ‰é¢„æå–çš„éŸ³è‰²ç‰¹å¾ï¼‰
        let is_zero_shot =
            request.ref_global_tokens.is_some() && request.ref_semantic_tokens.is_some();

        if is_zero_shot {
            info!("ğŸ¯ [{}] æ£€æµ‹åˆ°Zero-shotæ¨¡å¼ï¼Œè°ƒç”¨ä¸“ç”¨æ¨ç†å‡½æ•°", request_id);
            return execute_zero_shot_inference(&infer_context, request, text_tokens, Some(rng))
                .await;
        }

        // æ™®é€šæ¨¡å¼æ¨ç†
        info!("ğŸ¯ [{}] æ™®é€šæ¨¡å¼æ¨ç†ï¼Œè°ƒç”¨ä¸“ç”¨æ¨ç†å‡½æ•°", request_id);
        return execute_normal_inference(&infer_context, request, text_tokens).await;
    }

    /// è·å–é…ç½®
    pub fn config(&self) -> &DynamicBatchConfig {
        &self.config
    }
}

/// å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å•ä¾‹
static GLOBAL_DYNAMIC_BATCH_MANAGER: std::sync::OnceLock<Arc<DynamicBatchManager>> =
    std::sync::OnceLock::new();

/// åˆå§‹åŒ–å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨ï¼ˆæ”¯æŒé‡åŒ–é…ç½®ï¼‰
pub async fn init_global_dynamic_batch_manager(
    model_path: &str,
    vocab_path: &str,
    config: DynamicBatchConfig,
    quant_config: Option<std::collections::HashMap<usize, web_rwkv::runtime::model::Quant>>,
) -> Result<()> {
    let manager = DynamicBatchManager::new(model_path, vocab_path, config, quant_config).await?;

    GLOBAL_DYNAMIC_BATCH_MANAGER
        .set(Arc::new(manager))
        .map_err(|_| anyhow::anyhow!("å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–"))?;

    Ok(())
}

/// è·å–å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å®ä¾‹
pub fn get_global_dynamic_batch_manager() -> Result<Arc<DynamicBatchManager>> {
    GLOBAL_DYNAMIC_BATCH_MANAGER
        .get()
        .cloned()
        .ok_or_else(|| anyhow::anyhow!("å…¨å±€åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨æœªåˆå§‹åŒ–"))
}
