use anyhow::Result;
use rand::rngs::StdRng;
use rand::SeedableRng;
use tracing::warn;
use web_rwkv::runtime::infer::{RnnInput, RnnInputBatch, RnnOption};

use crate::shared_runtime::TtsInferContext;

/// æ‰§è¡ŒZero-shotæ¨ç†
pub async fn execute_zero_shot_inference(
    infer_context: TtsInferContext,
    text_tokens: Vec<i32>,
    property_tokens: Vec<i32>,
    rng: rand::rngs::StdRng,
    request: &crate::rwkv_sampler::TtsBatchRequest,
) -> Result<(Vec<i32>, Vec<i32>)> {
    let request_id = &infer_context.request_id;
    // å¼€å§‹Zero-shotæ¨ç†

    // Acquire runtime semaphore for the entire inference to ensure isolation
    let _runtime_permit = infer_context
        .runtime_semaphore
        .acquire()
        .await
        .map_err(|e| anyhow::anyhow!("æ— æ³•è·å–è¿è¡Œæ—¶ä¿¡å·é‡: {}", e))?;

    // å·²è·å–ä¿¡å·é‡è®¸å¯ï¼Œå¼€å§‹æ¨ç†

    // è·å–runtime
    let runtime = &infer_context.runtime;
    let state = &infer_context.state;
    let token_chunk_size = infer_context.options.token_chunk_size;

    // === éªŒè¯å’Œè¯»å–é¢„æå–çš„éŸ³è‰²ç‰¹å¾ ===
    let ref_global = request
        .ref_global_tokens
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("Zero-shotæ¨¡å¼éœ€è¦é¢„æå–çš„global tokens"))?;
    let ref_semantic = request
        .ref_semantic_tokens
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("Zero-shotæ¨¡å¼éœ€è¦é¢„æå–çš„semantic tokens"))?;

    // æ–‡æœ¬tokensä¿¡æ¯

    // ä¿®æ­£tokensèŒƒå›´ï¼Œç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
    let corrected_global: Vec<i32> = ref_global.iter().map(|&t| t.clamp(0, 4095)).collect();
    let _corrected_semantic: Vec<i32> = ref_semantic.iter().map(|&t| t.clamp(0, 8192)).collect();

    if corrected_global != *ref_global {
        warn!("ğŸ”§ [{}] å·²ä¿®æ­£global tokensèŒƒå›´åˆ°[0..4096)", request_id);
    }
    if _corrected_semantic != *ref_semantic {
        warn!("ğŸ”§ [{}] å·²ä¿®æ­£semantic tokensèŒƒå›´åˆ°[0..8192]", request_id);
    }

    // æ„å»ºè¾“å…¥åºåˆ—ï¼šå±æ€§tokens + TTS_TAG_2 + æ–‡æœ¬tokens + TTS_TAG_0
    let mut input_tokens: Vec<i32> = Vec::new();
    input_tokens.extend_from_slice(&property_tokens);
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_2);
    input_tokens.extend_from_slice(&text_tokens);
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_0);
    // åŠ å…¥é¢„è¯»å–çš„global tokensï¼ˆæ·»åŠ åç§»ï¼‰
    for &token in &corrected_global {
        input_tokens.push(token + crate::rwkv_sampler::GLOBAL_TOKEN_OFFSET);
    }
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_1);
    // åŠ å…¥é¢„è¯»å–çš„semantic tokens
    input_tokens.extend_from_slice(&_corrected_semantic);

    // === Prefill é˜¶æ®µï¼ˆå¤åˆ¶æ™®é€šæ¨¡å¼ï¼‰===
    let input_tokens_u32: Vec<u32> = input_tokens.iter().map(|&t| t as u32).collect();

    // Prefillé˜¶æ®µ - åˆå§‹åŒ–ç‹¬ç«‹çŠ¶æ€

    // åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
    let batch = RnnInputBatch::new(input_tokens_u32.clone(), RnnOption::Last);
    let mut inference = RnnInput::new(vec![batch], token_chunk_size);

    // ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€ï¼Œç¡®ä¿çŠ¶æ€éš”ç¦»ï¼ˆä¼˜åŒ–ï¼šåˆå¹¶äºŒæ¬¡é”æ“ä½œï¼‰
    {
        let state_guard = state.lock().await;
        let initial_state = state_guard.init();
        state_guard.load(initial_state, 0)?;
        drop(state_guard); // æ˜¾å¼é‡Šæ”¾é”
                           // å·²ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€
    }

    // æ¶ˆåŒ–è¾“å…¥ç›´åˆ°äº§ç”Ÿè¾“å‡º
    let _last_logits: Vec<f32> = loop {
        let (remaining_input, output) = runtime.infer(inference.clone()).await?;
        inference = remaining_input;
        if !output.is_empty() && output[0].0.size() > 0 {
            break output[0].0.clone().to_vec();
        }
    };

    // === Global é˜¶æ®µï¼šè·³è¿‡ç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨é¢„æå–çš„tokens ===
    let global_tokens: Vec<i32> = corrected_global.clone();
    let mut semantic_tokens: Vec<i32> = Vec::new();

    // å¼€å§‹ç”ŸæˆTTS tokens

    // å°†é¢„æå–çš„global tokensåé¦ˆåˆ°æ¨¡å‹ï¼ˆä¸åŠ åç§»é‡ï¼Œä¸æ™®é€šæ¨¡å¼ä¸€è‡´ï¼‰
    for &token in &global_tokens {
        inference.batches[0].push(token as u32);
    }

    // å·²å°†é¢„æå–çš„global tokensåé¦ˆåˆ°æ¨¡å‹

    // === åˆ‡æ¢åˆ° Semantic é˜¶æ®µï¼ˆå¤åˆ¶æ™®é€šæ¨¡å¼ç»“æ„ï¼‰===
    inference.batches[0].push(crate::rwkv_sampler::TTS_TAG_1 as u32);
    // åˆ‡æ¢åˆ°Semanticé˜¶æ®µï¼Œæ¨å…¥TTS_TAG_1

    // è®©æ ‡ç­¾ç”Ÿæ•ˆï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºï¼Œå¹¶ä¿ç•™logitsä¾›é¦–æ­¥ä½¿ç”¨
    let last_sem_logits: Vec<f32> = loop {
        let (next_inference, output) = runtime.infer(inference).await?;
        inference = next_inference;
        if output[0].0.size() > 0 {
            break output[0].0.clone().to_vec();
        }
    };

    // === Semantic tokens ç”Ÿæˆé˜¶æ®µï¼ˆå¤åˆ¶æ™®é€šæ¨¡å¼å‚æ•°å’Œé€»è¾‘ï¼‰===
    let semantic_limit: usize = usize::min(2048, 2048);

    // Zero-shotæ¨¡å¼ï¼šè·³è¿‡Globalé˜¶æ®µï¼Œç›´æ¥ä½¿ç”¨é¢„æå–çš„global_tokens
    // è®¾ç½®Semanticé˜¶æ®µé‡‡æ ·å‚æ•°
    let args_semantic = crate::rwkv_sampler::SamplerArgs {
        temperature: 1.0, // Semanticé˜¶æ®µä½¿ç”¨å›ºå®šå‚æ•°
        top_p: 0.95,
        top_k: 80,
        seed: infer_context.options.seed,
        max_tokens: 2048,
        voice_fidelity: infer_context.options.voice_fidelity,
        layered_randomness: infer_context.options.layered_randomness.clone(),
        token_chunk_size: infer_context.options.token_chunk_size,
    };

    // å‚æ•°å¯¹æ¯”æ‰“å°ï¼šPython vs Rust (Zero-shotæ¨¡å¼)
    log::info!(
        "ğŸ” [{}] Zero-shotæ¨¡å¼é‡‡æ ·å‚æ•°å¯¹æ¯” (Python vs Rust):",
        request_id
    );
    log::info!("   ğŸ“Š Semanticé˜¶æ®µ:");
    log::info!("      Python: temperature=1.0, top_p=0.95, top_k=80");
    log::info!(
        "      Rust:   temperature={:.1}, top_p={:.2}, top_k={}",
        args_semantic.temperature,
        args_semantic.top_p,
        args_semantic.top_k
    );

    // éªŒè¯å‚æ•°ä¸€è‡´æ€§
    let semantic_match = (args_semantic.temperature - 1.0).abs() < 0.001
        && (args_semantic.top_p - 0.95).abs() < 0.001
        && args_semantic.top_k == 80;

    if semantic_match {
        log::info!(
            "âœ… [{}] Zero-shot Semanticå‚æ•°å®Œå…¨åŒ¹é…Pythonç‰ˆæœ¬ï¼",
            request_id
        );
    } else {
        log::warn!(
            "âš ï¸ [{}] Zero-shot Semanticå‚æ•°ä¸Pythonç‰ˆæœ¬ä¸åŒ¹é…ï¼",
            request_id
        );
    }

    // å¼€å§‹ç”Ÿæˆsemantic tokens
    println!(
        "ğŸ¯ [{}] Zero-shotæ¨¡å¼å¼€å§‹ç”ŸæˆSemantic tokensï¼Œæœ€å¤§æ•°é‡: {}",
        request_id, semantic_limit
    );

    // ç®€åŒ–é‡‡æ ·ï¼Œç§»é™¤ä¼˜åŒ–ç»„ä»¶

    // åˆ›å»ºç‹¬ç«‹çš„RNGç”¨äºsemanticé˜¶æ®µ
    let semantic_rng = if args_semantic.layered_randomness.use_independent_seeds {
        if let Some(seed) = args_semantic.seed {
            // ç”¨æˆ·æä¾›äº†seedï¼Œä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
            StdRng::seed_from_u64(
                seed.wrapping_add(args_semantic.layered_randomness.semantic_seed_offset),
            )
        } else {
            // ç”¨æˆ·æ²¡æœ‰æä¾›seedï¼Œä½¿ç”¨éšæœºé‡‡æ ·
            StdRng::from_rng(rand::thread_rng()).expect("failed to seed StdRng")
        }
    } else {
        rng
    };

    let mut semantic_rng_opt = Some(semantic_rng);
    for i in 0..semantic_limit {
        let logits: Vec<f32> = if i == 0 {
            last_sem_logits.clone()
        } else {
            loop {
                let (next_inference, output) = runtime.infer(inference.clone()).await?;
                inference = next_inference;
                if output[0].0.size() > 0 {
                    break output[0].0.clone().to_vec();
                }
            }
        };

        // è¯­ä¹‰é˜¶æ®µä»…é‡‡æ · [0..8192]ï¼ˆåŒ…å«EOSï¼‰ï¼Œå±è”½TTS_TAG_*ä¸å…¶å®ƒåŸŸ
        let mut logits_masked = logits.clone();
        // ä¿®å¤ï¼šä¸å±è”½EOS tokenï¼Œåªå±è”½å¤§äºEOS tokençš„éƒ¨åˆ†
        for (j, v) in logits_masked.iter_mut().enumerate() {
            if j > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
                *v = f32::NEG_INFINITY;
            }
        }
        // å±è”½TTS_TAG tokensï¼Œä½†ä¿ç•™EOS token
        for tag in [
            crate::rwkv_sampler::TTS_TAG_0,
            crate::rwkv_sampler::TTS_TAG_1,
            crate::rwkv_sampler::TTS_TAG_2,
        ] {
            let idx = tag as usize;
            if idx < logits_masked.len() {
                logits_masked[idx] = f32::NEG_INFINITY;
            }
        }

        // ä½¿ç”¨ç®€å•é‡‡æ ·å™¨é‡‡æ ·
        let next_id = crate::rwkv_sampler::sample_logits(
            &logits_masked,
            &args_semantic,
            None, // forbid_token
            &mut semantic_rng_opt,
        );

        // æ£€æŸ¥æ˜¯å¦é‡åˆ°EOS tokenï¼ˆå¿…é¡»åœ¨èŒƒå›´æ£€æŸ¥ä¹‹å‰ï¼‰
        if next_id == crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
            break;
        }

        // é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿tokenåœ¨semanticèŒƒå›´å†… [0..8192)ï¼ˆä¿®å¤ï¼šåº”è¯¥æ˜¯>8192è€Œä¸æ˜¯>=8192ï¼‰
        if next_id > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
            warn!(
                "ğŸš¨ [{}] Token {} è¶…å‡ºsemanticèŒƒå›´[0..8192]ï¼Œåœæ­¢ç”Ÿæˆä»¥ç¡®ä¿ç¨³å®šæ€§",
                request_id, next_id
            );
            break;
        }

        semantic_tokens.push(next_id as i32);

        // åé¦ˆåˆ°æ¨¡å‹ï¼šè¯­ä¹‰é˜¶æ®µç›´æ¥ä½¿ç”¨åŸå§‹tokenï¼ˆä¸åŠ åç§»ï¼‰
        inference.batches[0].push(next_id as u32);

        // æ‰“å°å½“å‰ç”Ÿæˆè¿›åº¦
        if (i + 1) % 16 == 0 || i == semantic_limit - 1 {
            println!(
                "ğŸ“Š [{}] Zero-shot Semanticé˜¶æ®µ: å·²ç”Ÿæˆ {}/{} tokens",
                request_id,
                i + 1,
                semantic_limit
            );
        }
    }
    // TTS tokensç”Ÿæˆå®Œæˆ
    println!(
        "âœ… [{}] Zero-shot TTSç”Ÿæˆå®Œæˆ - Global tokens: {}, Semantic tokens: {}",
        request_id,
        global_tokens.len(),
        semantic_tokens.len()
    );
    Ok((global_tokens, semantic_tokens))
}
