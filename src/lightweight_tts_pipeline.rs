//! è½»é‡çº§TTSæµæ°´çº¿
//! å¤ç”¨å…¨å±€èµ„æºï¼Œä¸å†æ¯æ¬¡åˆ›å»ºæ–°çš„æ¨¡å‹å®ä¾‹

use crate::{
    dynamic_batch_manager::get_global_dynamic_batch_manager,
    onnx_session_pool::get_global_onnx_manager,
    properties_util,
    rwkv_sampler::{SamplerArgs, TtsBatchRequest},
    voice_feature_manager::VoiceFeatureManager,
};
use anyhow::Result;
use ndarray::{Array1, Array2};
use ort::{session::SessionInputValue, value::Value};
use std::path::Path;
use tracing;

/// è½»é‡çº§TTSæµæ°´çº¿å‚æ•°
#[derive(Debug, Clone)]
pub struct LightweightTtsPipelineArgs {
    pub text: String,
    pub prompt_text: String,
    pub ref_audio_path: String,
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: usize,
    pub max_tokens: usize,
    pub age: String,
    pub gender: String,
    pub emotion: String,
    pub pitch: String,
    pub speed: String,
    pub zero_shot: bool,
    pub validate: bool,
    pub seed: Option<u64>,
    // æ–°å¢ï¼švoice_idç”¨äºä»ç¼“å­˜è·å–tokens
    pub voice_id: Option<String>,
    // æ–°å¢ï¼šç›´æ¥ä¼ å…¥çš„éŸ³è‰²ç‰¹å¾tokens
    pub voice_global_tokens: Option<Vec<i32>>,
    pub voice_semantic_tokens: Option<Vec<i32>>,
}

impl Default for LightweightTtsPipelineArgs {
    fn default() -> Self {
        Self {
            text: String::new(),
            prompt_text: String::new(),
            ref_audio_path: String::new(),
            temperature: 1.0,
            top_p: 0.90,
            top_k: 0,
            max_tokens: 8000,
            age: "youth-adult".to_string(),
            gender: "female".to_string(),
            emotion: "NEUTRAL".to_string(),
            pitch: "medium".to_string(),
            speed: "medium".to_string(),
            zero_shot: false,
            validate: false,
            seed: None,
            voice_id: None,
            voice_global_tokens: None,
            voice_semantic_tokens: None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_pipeline_args_default() {
        let args = LightweightTtsPipelineArgs::default();

        // æµ‹è¯•æ–°æ·»åŠ çš„å­—æ®µ
        assert_eq!(args.prompt_text, "");
        assert_eq!(args.ref_audio_path, "");

        // æµ‹è¯•å…¶ä»–åŸºæœ¬å­—æ®µ
        assert_eq!(args.text, "");
        assert_eq!(args.temperature, 1.0);
        assert_eq!(args.top_p, 0.90);
        assert_eq!(args.top_k, 0);
        assert_eq!(args.max_tokens, 8000);
        assert_eq!(args.age, "youth-adult");
        assert_eq!(args.gender, "female");
        assert_eq!(args.emotion, "NEUTRAL");
        assert_eq!(args.pitch, "medium");
        assert_eq!(args.speed, "medium");
        assert!(!args.zero_shot);
        assert!(!args.validate);
        assert_eq!(args.seed, None);
        assert_eq!(args.voice_id, None);
        assert_eq!(args.voice_global_tokens, None);
        assert_eq!(args.voice_semantic_tokens, None);
    }

    #[test]
    fn test_pipeline_args_custom() {
        let args = LightweightTtsPipelineArgs {
            prompt_text: "è¿™æ˜¯æç¤ºæ–‡æœ¬".to_string(),
            ref_audio_path: "/path/to/audio.wav".to_string(),
            text: "è¿™æ˜¯è¦åˆæˆçš„æ–‡æœ¬".to_string(),
            zero_shot: true,
            ..Default::default()
        };

        // éªŒè¯å­—æ®µè®¾ç½®æ­£ç¡®
        assert_eq!(args.prompt_text, "è¿™æ˜¯æç¤ºæ–‡æœ¬");
        assert_eq!(args.ref_audio_path, "/path/to/audio.wav");
        assert_eq!(args.text, "è¿™æ˜¯è¦åˆæˆçš„æ–‡æœ¬");
        assert!(args.zero_shot);
    }

    #[test]
    fn test_pipeline_args_clone() {
        let args = LightweightTtsPipelineArgs {
            prompt_text: "æµ‹è¯•å…‹éš†".to_string(),
            ref_audio_path: "/test/path.wav".to_string(),
            ..Default::default()
        };

        let cloned_args = args.clone();

        assert_eq!(args.prompt_text, cloned_args.prompt_text);
        assert_eq!(args.ref_audio_path, cloned_args.ref_audio_path);
    }

    #[test]
    fn test_process_text_zero_shot() {
        let pipeline = LightweightTtsPipeline::new();
        let result = pipeline.process_text_zero_shot("ç”¨æˆ·æ–‡æœ¬", "æç¤ºæ–‡æœ¬");
        assert_eq!(result, "æç¤ºæ–‡æœ¬ç”¨æˆ·æ–‡æœ¬");
    }
}

/// è½»é‡çº§TTSæµæ°´çº¿ï¼Œå¤ç”¨å…¨å±€èµ„æº
#[derive(Debug)]
pub struct LightweightTtsPipeline {}

impl LightweightTtsPipeline {
    /// åˆ›å»ºæ–°çš„è½»é‡çº§TTSæµæ°´çº¿
    pub fn new() -> Self {
        Self {}
    }

    /// å¤„ç†æ–‡æœ¬
    fn process_text(&self, text: &str) -> String {
        text.to_string()
    }

    /// å¤„ç†æ–‡æœ¬ï¼ˆZero-shotæ¨¡å¼ï¼‰
    /// æ³¨æ„ï¼šZero-shotæ¨¡å¼ä¸‹ç»“åˆå‚è€ƒéŸ³é¢‘çš„æç¤ºæ–‡æœ¬å’Œç”¨æˆ·è¾“å…¥æ–‡æœ¬
    /// è¿”å›æ ¼å¼ä¸º"prompt_text + user_text"çš„ç»„åˆï¼Œä»¥æ”¹å–„è¯­éŸ³åˆæˆæ•ˆæœ
    pub fn process_text_zero_shot(&self, text: &str, prompt_text: &str) -> String {
        let combined_text = format!("{}{}", prompt_text, text);
        #[cfg(debug_assertions)]
        {
            // Zero-shotæ¨¡å¼ï¼šä½¿ç”¨ç»„åˆæ–‡æœ¬å¤„ç†
        }
        combined_text
    }

    /// ç”ŸæˆTTSå±æ€§tokens
    fn generate_property_tokens(&self, args: &LightweightTtsPipelineArgs) -> Vec<i32> {
        // å¦‚æœæä¾›äº†é¢„æå–çš„éŸ³è‰²ç‰¹å¾tokensæˆ–å¤„äºzero_shotæ¨¡å¼ï¼Œåˆ™ä¸ä½¿ç”¨ä¼ ç»Ÿå±æ€§å‚æ•°
        if (args.voice_global_tokens.is_some() && args.voice_semantic_tokens.is_some())
            || args.zero_shot
        {
            tracing::info!("ğŸ­ ä½¿ç”¨é¢„æå–éŸ³è‰²ç‰¹å¾æˆ–zero_shotæ¨¡å¼ï¼Œè·³è¿‡ä¼ ç»Ÿå±æ€§å‚æ•°");
            vec![] // ä½¿ç”¨é¢„æå–éŸ³è‰²ç‰¹å¾æˆ–zero_shotæ¨¡å¼æ—¶ï¼Œä¼ ç»Ÿå±æ€§å‚æ•°ä¸èµ·ä½œç”¨
        } else {
            // æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ‰“å°ä¼ å…¥çš„å‚æ•°
            tracing::info!(
                "ğŸµ ç”Ÿæˆå±æ€§tokens - age: {}, gender: {}, emotion: {}, pitch: {}, speed: {}",
                args.age,
                args.gender,
                args.emotion,
                args.pitch,
                args.speed
            );

            // ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„pitchå’Œspeedå­—ç¬¦ä¸²ï¼Œæ— éœ€åˆ†ç±»è½¬æ¢
            // æ³¨æ„ï¼šå‡½æ•°å®šä¹‰çš„å‚æ•°é¡ºåºæ˜¯(age, gender, emotion, pitch, speed) - ä¸Python/C++ç‰ˆæœ¬ä¸€è‡´
            let tokens = properties_util::convert_standard_properties_to_tokens(
                &args.age,     // age - ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²å¹´é¾„
                &args.gender,  // gender
                &args.emotion, // emotion
                &args.pitch,   // pitch - ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²
                &args.speed,   // speed - ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²
            );

            tracing::info!("ğŸ¯ ç”Ÿæˆçš„å±æ€§tokens: {:?}", tokens);
            tokens
        }
    }

    /// å¤„ç†å‚è€ƒéŸ³é¢‘ï¼ˆZero-shotæ¨¡å¼ï¼‰
    async fn process_reference_audio(&self, ref_audio_path: &str) -> Result<(Vec<i32>, Vec<i32>)> {
        if ref_audio_path.is_empty() || !Path::new(ref_audio_path).exists() {
            return Err(anyhow::anyhow!("å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {}", ref_audio_path));
        }

        let onnx_manager = get_global_onnx_manager()?;

        // åŠ è½½éŸ³é¢‘æ–‡ä»¶
        let audio_data = self.load_audio_file(ref_audio_path).await?;

        // ä½¿ç”¨BiCodec Tokenizeä¼šè¯
        let bicodec_session = onnx_manager.acquire_bicodec_tokenize_session().await?;
        let (global_tokens, semantic_tokens) = self
            .tokenize_audio_with_session(&audio_data, bicodec_session)
            .await?;

        Ok((global_tokens, semantic_tokens))
    }

    /// åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒWAVå’ŒMP3æ ¼å¼ï¼‰
    async fn load_audio_file(&self, audio_path: &str) -> Result<Vec<f32>> {
        use std::path::Path;

        if !Path::new(audio_path).exists() {
            return Err(anyhow::anyhow!("éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {}", audio_path));
        }

        let audio_path = audio_path.to_string();
        let result = tokio::task::spawn_blocking(move || -> Result<Vec<f32>> {
            let path = Path::new(&audio_path);
            let extension = path
                .extension()
                .and_then(|ext| ext.to_str())
                .unwrap_or("")
                .to_lowercase();

            let (mut audio, sample_rate, channels) = match extension.as_str() {
                "wav" => {
                    // ä½¿ç”¨houndå¤„ç†WAVæ–‡ä»¶
                    use hound::WavReader;
                    let mut reader = WavReader::open(&audio_path)?;
                    let spec = reader.spec();

                    let samples: Result<Vec<f32>, _> = reader
                        .samples::<i16>()
                        .map(|s| s.map(|sample| sample as f32 / i16::MAX as f32))
                        .collect();
                    let audio = samples?;

                    (audio, spec.sample_rate, spec.channels as usize)
                }
                "mp3" => {
                    // ä½¿ç”¨symphoniaå¤„ç†MP3æ–‡ä»¶
                    use std::fs::File;
                    use symphonia::core::audio::{AudioBufferRef, Signal};
                    use symphonia::core::codecs::{DecoderOptions, CODEC_TYPE_MP3};
                    use symphonia::core::formats::FormatOptions;
                    use symphonia::core::io::MediaSourceStream;
                    use symphonia::core::meta::MetadataOptions;
                    use symphonia::core::probe::Hint;

                    let file = File::open(&audio_path)?;
                    let mss = MediaSourceStream::new(Box::new(file), Default::default());

                    let mut hint = Hint::new();
                    hint.with_extension("mp3");

                    let meta_opts: MetadataOptions = Default::default();
                    let fmt_opts: FormatOptions = Default::default();

                    let probed = symphonia::default::get_probe()
                        .format(&hint, mss, &fmt_opts, &meta_opts)?;

                    let mut format = probed.format;
                    let track = format
                        .tracks()
                        .iter()
                        .find(|t| t.codec_params.codec == CODEC_TYPE_MP3)
                        .ok_or_else(|| anyhow::anyhow!("æœªæ‰¾åˆ°MP3éŸ³è½¨"))?;

                    let track_id = track.id;
                    let mut decoder = symphonia::default::get_codecs()
                        .make(&track.codec_params, &DecoderOptions { verify: false })?;

                    let mut audio_data = Vec::new();
                    let mut sample_rate = 44100u32;
                    let mut channels = 2usize;

                    while let Ok(packet) = format.next_packet() {
                        if packet.track_id() != track_id {
                            continue;
                        }

                        match decoder.decode(&packet)? {
                            AudioBufferRef::F32(buf) => {
                                sample_rate = buf.spec().rate;
                                channels = buf.spec().channels.count();
                                for &sample in buf.chan(0) {
                                    audio_data.push(sample);
                                }
                                if channels > 1 {
                                    for &sample in buf.chan(1) {
                                        audio_data.push(sample);
                                    }
                                }
                            }
                            AudioBufferRef::S16(buf) => {
                                sample_rate = buf.spec().rate;
                                channels = buf.spec().channels.count();
                                for &sample in buf.chan(0) {
                                    audio_data.push(sample as f32 / i16::MAX as f32);
                                }
                                if channels > 1 {
                                    for &sample in buf.chan(1) {
                                        audio_data.push(sample as f32 / i16::MAX as f32);
                                    }
                                }
                            }
                            _ => return Err(anyhow::anyhow!("ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼")),
                        }
                    }

                    (audio_data, sample_rate, channels)
                }
                _ => {
                    return Err(anyhow::anyhow!("ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: {}", extension));
                }
            };

            // è½¬æ¢ä¸ºå•å£°é“
            if channels > 1 {
                let len = audio.len() / channels;
                let mut mono_audio = Vec::with_capacity(len);
                for i in 0..len {
                    mono_audio.push(audio[i * channels]);
                }
                audio = mono_audio;
            }

            // é‡é‡‡æ ·åˆ°16kHz
            if sample_rate != 16000 {
                let original_len = audio.len();
                let target_len = (original_len as f32 * 16000.0 / sample_rate as f32) as usize;
                let mut resampled = Vec::with_capacity(target_len);
                for i in 0..target_len {
                    let idx = i * original_len / target_len;
                    resampled.push(audio[idx]);
                }
                audio = resampled;
            }

            Ok(audio)
        })
        .await??;

        Ok(result)
    }

    /// ä½¿ç”¨ONNXä¼šè¯è¿›è¡ŒéŸ³é¢‘tokenize
    pub async fn tokenize_audio_with_session(
        &self,
        audio_data: &[f32],
        mut session_guard: crate::onnx_session_pool::SessionGuard,
    ) -> Result<(Vec<i32>, Vec<i32>)> {
        // é¢„å…ˆè·å–wav2vec2ä¼šè¯ï¼ˆå¼‚æ­¥ï¼‰ï¼Œéšååœ¨é˜»å¡çº¿ç¨‹ä¸­ä½¿ç”¨
        let onnx_manager = get_global_onnx_manager()?;
        let mut wav2vec2_guard = onnx_manager.acquire_wav2vec2_session().await?;

        let audio_data = audio_data.to_vec();
        let result = tokio::task::spawn_blocking(move || -> Result<(Vec<i32>, Vec<i32>)> {
            // è½¬æ¢éŸ³é¢‘æ•°æ®ä¸ºndarray
            let wav = Array1::from(audio_data);

            // æå–å‚è€ƒç‰‡æ®µï¼ˆé•¿åº¦ä¸Refå®ç°ä¸€è‡´ï¼‰
            let ref_wav = Self::get_ref_clip(&wav);

            // ä¿®å¤ï¼šä½¿ç”¨åŸå§‹wavæ•°æ®è¿›è¡Œwav2vec2ç‰¹å¾æå–ï¼Œä¸C++å’ŒPythonå®ç°ä¿æŒä¸€è‡´
            // C++å®ç°ä¸­ç›´æ¥ä½¿ç”¨åŸå§‹audioè¿›è¡Œextract_wav2vec2_features(audio)
            // Pythonå®ç°ä¸­ä½¿ç”¨åŸå§‹wavè¿›è¡Œextract_wav2vec2_features(wav)
            // æ³¨æ„ï¼šzero_mean_unit_variance_normalizeå·²ç»åœ¨extract_wav2vec2_featureså†…éƒ¨è¿›è¡Œäº†

            // åº”ç”¨é›¶å‡å€¼å•ä½æ–¹å·®å½’ä¸€åŒ–é¢„å¤„ç† - ä¸C++å’ŒPythonå®ç°ä¿æŒä¸€è‡´
            let normalized_wav =
                crate::ref_audio_utilities::RefAudioUtilities::zero_mean_unit_variance_normalize(
                    wav.to_vec(),
                );
            let feat_input = Array1::from(normalized_wav).insert_axis(ndarray::Axis(0));
            let feat_dyn = feat_input.into_dyn();
            let feat_shape: Vec<i64> = feat_dyn.shape().iter().map(|&d| d as i64).collect();
            let feat_vec = feat_dyn.into_raw_vec();
            let feat_tensor = Value::from_array((feat_shape, feat_vec))?;

            let wav2vec2_outputs = wav2vec2_guard
                .session_mut()
                .run(ort::inputs![SessionInputValue::from(feat_tensor)])?;
            let (feat_out_shape, feat_data) = wav2vec2_outputs[0].try_extract_tensor::<f32>()?;

            if !(feat_out_shape.len() == 3 && feat_out_shape[0] == 1) {
                return Err(anyhow::anyhow!(
                    "Unexpected wav2vec2 output shape: {:?}",
                    feat_out_shape
                ));
            }
            let time_steps = feat_out_shape[1] as usize;
            let feature_dim = feat_out_shape[2] as usize;
            if feature_dim != 1024 {
                return Err(anyhow::anyhow!(
                    "Expected feature dimension 1024, got {}",
                    feature_dim
                ));
            }
            let feat = Array2::from_shape_vec((time_steps, feature_dim), feat_data.to_vec())?;

            // æå–melé¢‘è°±å›¾ï¼ˆç²¾ç¡®å¯¹é½Refå®ç°ï¼‰
            // ä¿®å¤ï¼šä½¿ç”¨ä¸C++å®Œå…¨ä¸€è‡´çš„å‚æ•°
            let ref_mel =
                crate::tts_pipeline_fixes::TtsPipelineFixes::extract_mel_spectrogram_consistent(
                    &ref_wav,
                )?;

            // å‡†å¤‡ONNXè¾“å…¥
            // ç¡®ä¿æ•°æ®æ˜¯è¡Œä¼˜å…ˆå¸ƒå±€ï¼ˆC-orderï¼‰ï¼Œä¸C++å®ç°ä¿æŒä¸€è‡´
            let ref_mel_c_order = if ref_mel.is_standard_layout() {
                ref_mel.clone()
            } else {
                ref_mel.as_standard_layout().to_owned()
            };

            let feat_c_order = if feat.is_standard_layout() {
                feat.clone()
            } else {
                feat.as_standard_layout().to_owned()
            };

            let ref_mel_input = ref_mel_c_order.insert_axis(ndarray::Axis(0));
            let feat_input2 = feat_c_order.insert_axis(ndarray::Axis(0));

            let ref_mel_dyn = ref_mel_input.into_dyn();
            let feat_dyn2 = feat_input2.into_dyn();

            let ref_mel_shape: Vec<i64> = ref_mel_dyn.shape().iter().map(|&d| d as i64).collect();
            let ref_mel_vec: Vec<f32> = ref_mel_dyn.into_raw_vec();
            let ref_mel_tensor = Value::from_array((ref_mel_shape, ref_mel_vec))?;

            let feat_shape2: Vec<i64> = feat_dyn2.shape().iter().map(|&d| d as i64).collect();
            let feat_vec2: Vec<f32> = feat_dyn2.into_raw_vec();
            let feat_tensor2 = Value::from_array((feat_shape2, feat_vec2))?;

            // è¿è¡ŒBiCodec Tokenize ONNXæ¨ç†
            let outputs = session_guard.session_mut().run(ort::inputs![
                "ref_wav_mel" => SessionInputValue::from(ref_mel_tensor),
                "feat" => SessionInputValue::from(feat_tensor2)
            ])?;

            // ä¿®å¤è¾“å‡ºè§£æé¡ºåºï¼šä¸Pythonå’ŒC++å®ç°ä¿æŒä¸€è‡´
            // Python: semantic_tokens = outputs[0], global_tokens = outputs[1]
            // C++: semantic_tokens = output_tensors["semantic_tokens"], global_tokens = output_tensors["global_tokens"]

            // å…ˆå°è¯•æŒ‰åç§°è§£æè¾“å‡º
            let mut semantic_tokens: Vec<i32> = vec![];
            let mut global_tokens: Vec<i32> = vec![];

            // æ£€æŸ¥è¾“å‡ºåç§°æ¥ç¡®å®šæ­£ç¡®çš„è§£æé¡ºåº
            for (_name, output) in outputs.iter() {
                // è·å–è¾“å‡ºçš„å½¢çŠ¶ä¿¡æ¯
                let shape = output.shape();

                // semantic_tokens é€šå¸¸æ˜¯å½¢çŠ¶ä¸º [1, length] çš„å¼ é‡
                // global_tokens é€šå¸¸æ˜¯å½¢çŠ¶ä¸º [1, 1, 32] çš„å¼ é‡
                if shape.len() == 2 && shape[0] == 1 {
                    // è¿™å¾ˆå¯èƒ½æ˜¯ semantic_tokens
                    semantic_tokens = match output.try_extract_tensor::<i64>() {
                        Ok((_s_sem, semantic_tokens_slice)) => {
                            semantic_tokens_slice.iter().map(|&x| x as i32).collect()
                        }
                        Err(_) => {
                            let (_s_sem, semantic_tokens_slice) =
                                output.try_extract_tensor::<i32>()?;
                            semantic_tokens_slice.to_vec()
                        }
                    };
                } else if shape.len() == 3 && shape[0] == 1 && shape[1] == 1 {
                    // è¿™å¾ˆå¯èƒ½æ˜¯ global_tokens
                    global_tokens = match output.try_extract_tensor::<i64>() {
                        Ok((_s_glb, global_tokens_slice)) => {
                            global_tokens_slice.iter().map(|&x| x as i32).collect()
                        }
                        Err(_) => {
                            let (_s_glb, global_tokens_slice) =
                                output.try_extract_tensor::<i32>()?;
                            global_tokens_slice.to_vec()
                        }
                    };
                }
            }

            // å¦‚æœæŒ‰å½¢çŠ¶æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç´¢å¼•æ–¹å¼ä½œä¸ºå¤‡é€‰
            if semantic_tokens.is_empty() && global_tokens.is_empty() && outputs.len() >= 2 {
                semantic_tokens = match outputs[0].try_extract_tensor::<i64>() {
                    Ok((_s_sem, semantic_tokens_slice)) => {
                        semantic_tokens_slice.iter().map(|&x| x as i32).collect()
                    }
                    Err(_) => {
                        let (_s_sem, semantic_tokens_slice) =
                            outputs[0].try_extract_tensor::<i32>()?;
                        semantic_tokens_slice.to_vec()
                    }
                };

                global_tokens = match outputs[1].try_extract_tensor::<i64>() {
                    Ok((_s_glb, global_tokens_slice)) => {
                        global_tokens_slice.iter().map(|&x| x as i32).collect()
                    }
                    Err(_) => {
                        let (_s_glb, global_tokens_slice) =
                            outputs[1].try_extract_tensor::<i32>()?;
                        global_tokens_slice.to_vec()
                    }
                };
            }

            Ok((global_tokens, semantic_tokens))
        })
        .await??;

        Ok(result)
    }

    /// åˆ›å»ºæ¢…å°”æ»¤æ³¢å™¨ç»„
    #[allow(dead_code)]
    fn create_mel_filterbank_slaney_with_fmax(
        n_mels: usize,
        n_fft: usize,
        sample_rate: f32,
        fmin: f32,
        fmax: f32,
    ) -> Array2<f32> {
        let n_freqs = n_fft / 2 + 1;
        let mut filterbank = Array2::zeros((n_mels, n_freqs));

        let hz_to_mel = |hz: f32| 2595.0 * (1.0 + hz / 700.0).log10();
        let mel_to_hz = |mel: f32| 700.0 * (10.0_f32.powf(mel / 2595.0) - 1.0);

        let mel_min = hz_to_mel(fmin);
        let mel_max = hz_to_mel(fmax);

        let mel_points: Vec<f32> = (0..=n_mels + 1)
            .map(|i| mel_min + i as f32 * (mel_max - mel_min) / (n_mels + 1) as f32)
            .collect();

        let hz_points: Vec<f32> = mel_points.iter().map(|&mel| mel_to_hz(mel)).collect();
        let bin_points: Vec<f32> = hz_points
            .iter()
            .map(|&hz| hz * n_fft as f32 / sample_rate)
            .collect();

        for m in 1..=n_mels {
            let left = bin_points[m - 1];
            let center = bin_points[m];
            let right = bin_points[m + 1];

            for k in 0..n_freqs {
                let k_f = k as f32;
                if k_f >= left && k_f <= right {
                    if k_f <= center {
                        if center > left {
                            filterbank[[m - 1, k]] = (k_f - left) / (center - left);
                        }
                    } else if right > center {
                        filterbank[[m - 1, k]] = (right - k_f) / (right - center);
                    }
                }
            }

            // Slaneyå½’ä¸€åŒ–ï¼šé¢ç§¯å½’ä¸€åŒ–ä¸º 2/(fhi-flo)
            let fhi = hz_points[m + 1];
            let flo = hz_points[m - 1];
            let norm_factor = 2.0 / (fhi - flo);
            for k in 0..n_freqs {
                filterbank[[m - 1, k]] *= norm_factor;
            }
        }

        filterbank
    }

    /// è®¡ç®—åŠŸç‡è°±
    #[allow(dead_code)]
    fn compute_power_spectrum(frame: &[f32]) -> Vec<f32> {
        let n_fft = frame.len();
        let n_freqs = n_fft / 2 + 1;
        let mut power_spectrum = vec![0.0f32; n_freqs];

        for (k, power) in power_spectrum.iter_mut().enumerate().take(n_freqs) {
            let mut real = 0.0f32;
            let mut imag = 0.0f32;

            for (n, &sample) in frame.iter().enumerate().take(n_fft) {
                let angle = -2.0 * std::f32::consts::PI * k as f32 * n as f32 / n_fft as f32;
                real += sample * angle.cos();
                imag += sample * angle.sin();
            }

            *power = real * real + imag * imag;
        }

        power_spectrum
    }

    /// è§£ç éŸ³é¢‘
    async fn decode_audio(
        &self,
        global_tokens: &[i32],
        semantic_tokens: &[i32],
    ) -> Result<Vec<f32>> {
        let onnx_manager = get_global_onnx_manager()?;

        // è·å–BiCodec Detokenizeä¼šè¯
        let detokenize_session = onnx_manager.acquire_bicodec_detokenize_session().await?;

        // æ‰§è¡Œè§£ç 
        let audio = self
            .detokenize_audio_with_session(global_tokens, semantic_tokens, detokenize_session)
            .await?;

        Ok(audio)
    }

    /// æ‰¹é‡è§£ç éŸ³é¢‘ï¼ˆCPUä¼˜åŒ–ï¼šå‡å°‘ä¼šè¯è·å–å¼€é”€ï¼‰
    async fn decode_audio_batch(
        &self,
        batch_requests: &[(Vec<i32>, Vec<i32>)],
    ) -> Result<Vec<Vec<f32>>> {
        let onnx_manager = get_global_onnx_manager()?;
        let batch_size = batch_requests.len();
        let mut results = Vec::with_capacity(batch_size);

        // æ‰¹é‡è·å–ä¼šè¯ï¼Œå‡å°‘é”ç«äº‰
        let session_guards = onnx_manager
            .acquire_bicodec_detokenize_sessions_batch(batch_size)
            .await?;

        // å¹¶è¡Œæ‰§è¡Œè§£ç ï¼ˆä½¿ç”¨CPUå¤šæ ¸å¿ƒï¼‰
        let mut tasks = Vec::with_capacity(batch_size);
        for ((global_tokens, semantic_tokens), session_guard) in
            batch_requests.iter().zip(session_guards.into_iter())
        {
            let global_tokens_clone = global_tokens.clone();
            let semantic_tokens_clone = semantic_tokens.clone();
            let mut session_guard_clone = session_guard;

            let task = tokio::task::spawn_blocking(move || {
                // åœ¨é˜»å¡çº¿ç¨‹ä¸­æ‰§è¡ŒCPUå¯†é›†å‹æ“ä½œ
                let global_shape: Vec<i64> =
                    [1i64, 1i64, global_tokens_clone.len() as i64].to_vec();
                let global_vec_i64: Vec<i64> =
                    global_tokens_clone.iter().map(|&x| x as i64).collect();
                let global_tensor = match Value::from_array((global_shape, global_vec_i64)) {
                    Ok(tensor) => tensor,
                    Err(e) => {
                        eprintln!("åˆ›å»ºå…¨å±€tensorå¤±è´¥: {}", e);
                        return vec![];
                    }
                };

                let semantic_shape: Vec<i64> = [1i64, semantic_tokens_clone.len() as i64].to_vec();
                let semantic_vec_i64: Vec<i64> =
                    semantic_tokens_clone.iter().map(|&x| x as i64).collect();
                let semantic_tensor = match Value::from_array((semantic_shape, semantic_vec_i64)) {
                    Ok(tensor) => tensor,
                    Err(e) => {
                        eprintln!("åˆ›å»ºè¯­ä¹‰tensorå¤±è´¥: {}", e);
                        return vec![];
                    }
                };

                let outputs = match session_guard_clone.session_mut().run(ort::inputs![
                    "semantic_tokens" => SessionInputValue::from(semantic_tensor),
                    "global_tokens" => SessionInputValue::from(global_tensor)
                ]) {
                    Ok(outputs) => outputs,
                    Err(e) => {
                        eprintln!("ONNXæ¨ç†å¤±è´¥: {}", e);
                        return vec![];
                    }
                };

                match outputs[0].try_extract_tensor::<f32>() {
                    Ok((_shape, audio_slice)) => audio_slice.to_vec(),
                    Err(e) => {
                        eprintln!("æå–éŸ³é¢‘tensorå¤±è´¥: {}", e);
                        vec![]
                    }
                }
            });
            tasks.push(task);
        }

        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for task in tasks {
            let audio_result = task
                .await
                .map_err(|e| anyhow::anyhow!("æ‰¹å¤„ç†è§£ç ä»»åŠ¡å¤±è´¥: {}", e))?;
            results.push(audio_result);
        }

        Ok(results)
    }

    /// ä½¿ç”¨ONNXä¼šè¯è¿›è¡ŒéŸ³é¢‘è§£ç 
    async fn detokenize_audio_with_session(
        &self,
        global_tokens: &[i32],
        semantic_tokens: &[i32],
        mut session_guard: crate::onnx_session_pool::SessionGuard,
    ) -> Result<Vec<f32>> {
        // ä¼˜åŒ–ï¼šç§»é™¤spawn_blockingï¼Œç›´æ¥åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ
        // ç›´æ¥è½¬æ¢ä¸ºi64ï¼Œå‡å°‘ä¸­é—´æ­¥éª¤å’Œå†…å­˜åˆ†é…
        let global_shape: Vec<i64> = [1i64, 1i64, global_tokens.len() as i64].to_vec();
        let global_vec_i64: Vec<i64> = global_tokens.iter().map(|&x| x as i64).collect();
        let global_tensor = Value::from_array((global_shape, global_vec_i64))?;

        let semantic_shape: Vec<i64> = [1i64, semantic_tokens.len() as i64].to_vec();
        let semantic_vec_i64: Vec<i64> = semantic_tokens.iter().map(|&x| x as i64).collect();
        let semantic_tensor = Value::from_array((semantic_shape, semantic_vec_i64))?;

        // ç›´æ¥è¿è¡ŒONNXæ¨ç†ï¼Œé¿å…spawn_blockingçš„å¼€é”€
        let outputs = session_guard.session_mut().run(ort::inputs![
            "semantic_tokens" => SessionInputValue::from(semantic_tensor),
            "global_tokens" => SessionInputValue::from(global_tensor)
        ])?;

        let (_shape, audio_slice) = outputs[0].try_extract_tensor::<f32>()?;
        Ok(audio_slice.to_vec())
    }

    /// ç”Ÿæˆè¯­éŸ³ï¼ˆä½¿ç”¨æ‰¹å¤„ç†è°ƒåº¦å™¨ï¼‰
    pub async fn generate_speech(&self, args: &LightweightTtsPipelineArgs) -> Result<Vec<f32>> {
        let total_start = std::time::Instant::now();

        // 1. å¤„ç†æ–‡æœ¬
        let text_start = std::time::Instant::now();
        let processed_text = if args.zero_shot {
            self.process_text_zero_shot(&args.text, &args.prompt_text)
        } else {
            self.process_text(&args.text)
        };
        let text_processing_time = text_start.elapsed();

        // 2. å¤„ç†å±æ€§tokensæˆ–å‚è€ƒéŸ³é¢‘
        let ref_start = std::time::Instant::now();
        let (property_tokens, ref_global_tokens, ref_semantic_tokens) =
            // ä¼˜å…ˆä½¿ç”¨voice_idä»ç¼“å­˜è·å–tokens
            if let Some(voice_id) = &args.voice_id {
                // åˆ›å»ºVoiceFeatureManagerå®ä¾‹ï¼ˆå‡è®¾ä½¿ç”¨é»˜è®¤RAFç›®å½•ï¼‰
                let voice_manager = VoiceFeatureManager::new("./raf")?;
                match voice_manager.get_voice_tokens(voice_id).await {
                    Ok((global_tokens, semantic_tokens)) => {
                        (vec![], Some(global_tokens), Some(semantic_tokens))
                    }
                    Err(_) => {
                        // å›é€€åˆ°ç›´æ¥ä¼ å…¥çš„tokensæˆ–å…¶ä»–æ–¹å¼
                        if let (Some(global_tokens), Some(semantic_tokens)) = (&args.voice_global_tokens, &args.voice_semantic_tokens) {
                            (vec![], Some(global_tokens.clone()), Some(semantic_tokens.clone()))
                        } else if args.zero_shot {
                            let (global, semantic) = self.process_reference_audio(&args.ref_audio_path).await?;
                            (vec![], Some(global), Some(semantic))
                        } else {
                            let tokens = self.generate_property_tokens(args);
                            (tokens, None, None)
                        }
                    }
                }
            }
            // ä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¼ å…¥çš„éŸ³è‰²ç‰¹å¾tokens
            else if let (Some(global_tokens), Some(semantic_tokens)) = (&args.voice_global_tokens, &args.voice_semantic_tokens) {
                (vec![], Some(global_tokens.clone()), Some(semantic_tokens.clone()))
            } else if args.zero_shot {
                // åœ¨zero-shotæ¨¡å¼ä¸‹ï¼Œä¼˜åŒ–ä¸ºä¸€æ¬¡æ€§è·å–æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯
                // ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„éŸ³è‰²ç‰¹å¾tokensï¼ˆå¦‚æœæä¾›äº†çš„è¯ï¼‰
                if let (Some(global_tokens), Some(semantic_tokens)) = (&args.voice_global_tokens, &args.voice_semantic_tokens) {
                    (vec![], Some(global_tokens.clone()), Some(semantic_tokens.clone()))
                } else {
                    // å¤„ç†å‚è€ƒéŸ³é¢‘æ–‡ä»¶
                    let (global, semantic) = self.process_reference_audio(&args.ref_audio_path).await?;
                    (vec![], Some(global), Some(semantic))
                }
            } else {
                let tokens = self.generate_property_tokens(args);
                println!("generate_property_tokens: {:?}", tokens);
                (tokens, None, None)
            };
        let reference_processing_time = ref_start.elapsed();

        // 3. åˆ›å»ºé‡‡æ ·å‚æ•°
        let sampler_args = SamplerArgs {
            temperature: args.temperature,
            top_p: args.top_p,
            top_k: args.top_k,
            max_tokens: args.max_tokens,
            seed: args.seed,
            voice_fidelity: 0.8, // é»˜è®¤éŸ³è‰²ä¿çœŸåº¦
            layered_randomness: crate::rwkv_sampler::LayeredRandomnessConfig::default(),
            token_chunk_size: 512, // ä½¿ç”¨é»˜è®¤å€¼
        };

        // 4. åˆ›å»ºæ‰¹å¤„ç†è¯·æ±‚
        let request = TtsBatchRequest {
            text: processed_text,
            property_tokens,
            ref_global_tokens,
            ref_semantic_tokens,
            args: sampler_args,
            voice_id: args.voice_id.clone(),
        };

        // 5. æäº¤åˆ°åŠ¨æ€æ‰¹å¤„ç†ç®¡ç†å™¨å¹¶ç­‰å¾…RWKVæ¨ç†
        let inference_start = std::time::Instant::now();
        let manager = get_global_dynamic_batch_manager()?;
        let (global_tokens, semantic_tokens) = manager
            .generate_tts(
                request.text,
                request.property_tokens,
                request.ref_global_tokens,
                request.ref_semantic_tokens,
                request.voice_id,
                request.args,
            )
            .await?;
        let inference_time = inference_start.elapsed();

        // 6. è§£ç éŸ³é¢‘
        if global_tokens.is_empty() && semantic_tokens.is_empty() {
            return Ok(vec![0.0; 16000]);
        }

        let decode_start = std::time::Instant::now();
        let audio = self.decode_audio(&global_tokens, &semantic_tokens).await?;
        let audio_decoding_time = decode_start.elapsed();

        let total_time = total_start.elapsed();
        let audio_duration = audio.len() as f64 / 16000.0; // å‡è®¾16kHzé‡‡æ ·ç‡
        let _rtf = total_time.as_secs_f64() / audio_duration;

        // è¾“å‡ºè¯¦ç»†çš„è€—æ—¶ç»Ÿè®¡
        println!("â±ï¸  TTSç”Ÿæˆè¯¦ç»†è€—æ—¶ç»Ÿè®¡:");
        println!("  æ–‡æœ¬å¤„ç†è€—æ—¶: {:.2}ms", text_processing_time.as_millis());
        println!(
            "  å‚è€ƒéŸ³é¢‘å¤„ç†è€—æ—¶: {:.2}ms",
            reference_processing_time.as_millis()
        );
        println!("  RWKVæ¨ç†è€—æ—¶: {:.2}ms", inference_time.as_millis());
        println!("  éŸ³é¢‘è§£ç è€—æ—¶: {:.2}ms", audio_decoding_time.as_millis());
        println!("  æ€»è€—æ—¶: {:.2}ms", total_time.as_millis());

        Ok(audio)
    }

    /// æ‰¹é‡ç”Ÿæˆè¯­éŸ³ï¼ˆCPUä¼˜åŒ–ï¼šæ”¯æŒæ‰¹å¤„ç†æ¨ç†å’ŒéŸ³é¢‘è§£ç ï¼‰
    pub async fn generate_speech_batch(
        &self,
        batch_args: Vec<LightweightTtsPipelineArgs>,
    ) -> Result<Vec<Vec<f32>>> {
        let total_start = std::time::Instant::now();
        let batch_size = batch_args.len();

        // 1. å¤„ç†æ‰€æœ‰è¯·æ±‚çš„æ–‡æœ¬å’Œå‚è€ƒéŸ³é¢‘
        let mut batch_requests = Vec::with_capacity(batch_size);
        let mut processed_texts = Vec::with_capacity(batch_size);
        let mut ref_processing_results = Vec::with_capacity(batch_size);

        for args in &batch_args {
            // å¤„ç†æ–‡æœ¬
            let processed_text = if args.zero_shot {
                self.process_text_zero_shot(&args.text, &args.prompt_text)
            } else {
                self.process_text(&args.text)
            };
            processed_texts.push(processed_text);

            // å¤„ç†å‚è€ƒéŸ³é¢‘æˆ–å±æ€§tokens
            let ref_result = if let Some(voice_id) = &args.voice_id {
                let voice_manager = VoiceFeatureManager::new("./raf")?;
                match voice_manager.get_voice_tokens(voice_id).await {
                    Ok((global_tokens, semantic_tokens)) => {
                        (vec![], Some(global_tokens), Some(semantic_tokens))
                    }
                    Err(_) => {
                        if let (Some(global_tokens), Some(semantic_tokens)) =
                            (&args.voice_global_tokens, &args.voice_semantic_tokens)
                        {
                            (
                                vec![],
                                Some(global_tokens.clone()),
                                Some(semantic_tokens.clone()),
                            )
                        } else if args.zero_shot {
                            let (global, semantic) =
                                self.process_reference_audio(&args.ref_audio_path).await?;
                            (vec![], Some(global), Some(semantic))
                        } else {
                            let tokens = self.generate_property_tokens(args);
                            (tokens, None, None)
                        }
                    }
                }
            } else if let (Some(global_tokens), Some(semantic_tokens)) =
                (&args.voice_global_tokens, &args.voice_semantic_tokens)
            {
                (
                    vec![],
                    Some(global_tokens.clone()),
                    Some(semantic_tokens.clone()),
                )
            } else if args.zero_shot {
                if let (Some(global_tokens), Some(semantic_tokens)) =
                    (&args.voice_global_tokens, &args.voice_semantic_tokens)
                {
                    (
                        vec![],
                        Some(global_tokens.clone()),
                        Some(semantic_tokens.clone()),
                    )
                } else {
                    let (global, semantic) =
                        self.process_reference_audio(&args.ref_audio_path).await?;
                    (vec![], Some(global), Some(semantic))
                }
            } else {
                let tokens = self.generate_property_tokens(args);
                (tokens, None, None)
            };
            ref_processing_results.push(ref_result);
        }

        // 2. åˆ›å»ºæ‰¹å¤„ç†è¯·æ±‚
        for (i, args) in batch_args.iter().enumerate() {
            let (property_tokens, ref_global_tokens, ref_semantic_tokens) =
                &ref_processing_results[i];

            let sampler_args = SamplerArgs {
                temperature: args.temperature,
                top_p: args.top_p,
                top_k: args.top_k,
                max_tokens: args.max_tokens,
                seed: args.seed,
                voice_fidelity: 0.8,
                layered_randomness: crate::rwkv_sampler::LayeredRandomnessConfig::default(),
                token_chunk_size: 512,
            };

            let request = TtsBatchRequest {
                text: processed_texts[i].clone(),
                property_tokens: property_tokens.clone(),
                ref_global_tokens: ref_global_tokens.clone(),
                ref_semantic_tokens: ref_semantic_tokens.clone(),
                args: sampler_args,
                voice_id: args.voice_id.clone(),
            };
            batch_requests.push(request);
        }

        // 3. æ‰¹é‡æ‰§è¡ŒRWKVæ¨ç†
        let manager = get_global_dynamic_batch_manager()?;
        let inference_results = manager.generate_tts_batch(batch_requests).await?;

        // 4. æ‰¹é‡è§£ç éŸ³é¢‘
        let audio_results = self.decode_audio_batch(&inference_results).await?;

        let total_time = total_start.elapsed();
        println!(
            "â±ï¸  æ‰¹é‡TTSç”Ÿæˆå®Œæˆ: {}ä¸ªè¯·æ±‚, æ€»è€—æ—¶: {:.2}ms",
            batch_size,
            total_time.as_millis()
        );

        Ok(audio_results)
    }

    /// ä¿å­˜éŸ³é¢‘åˆ°æ–‡ä»¶ï¼ˆæ”¯æŒWAVå’ŒMP3æ ¼å¼ï¼‰
    pub fn save_audio(
        &self,
        audio_samples: &[f32],
        output_path: &str,
        sample_rate: u32,
    ) -> Result<()> {
        use std::path::Path;

        #[cfg(debug_assertions)]
        {
            // ä¿å­˜éŸ³é¢‘åˆ°æŒ‡å®šè·¯å¾„
        }

        let path = Path::new(output_path);
        let extension = path
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("wav")
            .to_lowercase();

        match extension.as_str() {
            "mp3" => self.save_audio_mp3(audio_samples, output_path, sample_rate),
            "wav" => self.save_audio_wav(audio_samples, output_path, sample_rate),
            _ => self.save_audio_wav(audio_samples, output_path, sample_rate),
        }
    }

    /// ä¿å­˜éŸ³é¢‘åˆ°WAVæ–‡ä»¶
    fn save_audio_wav(
        &self,
        audio_samples: &[f32],
        output_path: &str,
        sample_rate: u32,
    ) -> Result<()> {
        let spec = hound::WavSpec {
            channels: 1,
            sample_rate,
            bits_per_sample: 32,
            sample_format: hound::SampleFormat::Float,
        };

        let mut writer = hound::WavWriter::create(output_path, spec)?;
        for &sample in audio_samples {
            writer.write_sample(sample)?;
        }
        writer.finalize()?;

        #[cfg(debug_assertions)]
        {
            // WAVéŸ³é¢‘ä¿å­˜å®Œæˆ
        }
        Ok(())
    }

    /// ä¿å­˜éŸ³é¢‘åˆ°MP3æ–‡ä»¶
    fn save_audio_mp3(
        &self,
        audio_samples: &[f32],
        output_path: &str,
        sample_rate: u32,
    ) -> Result<()> {
        use mp3lame_encoder::{Builder, FlushNoGap};
        use std::fs::File;
        use std::io::Write;

        // è½¬æ¢f32æ ·æœ¬åˆ°i16
        let i16_samples: Vec<i16> = audio_samples
            .iter()
            .map(|&sample| {
                let clamped = sample.clamp(-1.0, 1.0);
                (clamped * i16::MAX as f32) as i16
            })
            .collect();

        // é…ç½®MP3ç¼–ç å™¨
        let mut builder = Builder::new().ok_or_else(|| anyhow::anyhow!("åˆ›å»ºMP3ç¼–ç å™¨å¤±è´¥"))?;

        builder
            .set_num_channels(1)
            .map_err(|e| anyhow::anyhow!("è®¾ç½®å£°é“æ•°å¤±è´¥: {}", e))?;

        builder
            .set_sample_rate(sample_rate)
            .map_err(|e| anyhow::anyhow!("è®¾ç½®é‡‡æ ·ç‡å¤±è´¥: {}", e))?;

        builder
            .set_brate(mp3lame_encoder::Bitrate::Kbps128)
            .map_err(|e| anyhow::anyhow!("è®¾ç½®æ¯”ç‰¹ç‡å¤±è´¥: {}", e))?;

        builder
            .set_quality(mp3lame_encoder::Quality::Best)
            .map_err(|e| anyhow::anyhow!("è®¾ç½®è´¨é‡å¤±è´¥: {}", e))?;

        let mut encoder = builder
            .build()
            .map_err(|e| anyhow::anyhow!("æ„å»ºMP3ç¼–ç å™¨å¤±è´¥: {}", e))?;

        // åˆ›å»ºè¾“å‡ºæ–‡ä»¶
        let mut output_file =
            File::create(output_path).map_err(|e| anyhow::anyhow!("åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤±è´¥: {}", e))?;

        // ç¼–ç éŸ³é¢‘æ•°æ®
        use mp3lame_encoder::InterleavedPcm;
        use std::mem::MaybeUninit;

        let mut mp3_buffer: Vec<MaybeUninit<u8>> =
            vec![MaybeUninit::uninit(); i16_samples.len() * 2];
        let pcm_input = InterleavedPcm(&i16_samples);
        let encoded_size = encoder
            .encode(pcm_input, &mut mp3_buffer)
            .map_err(|e| anyhow::anyhow!("MP3ç¼–ç å¤±è´¥: {}", e))?;

        // å®‰å…¨åœ°è½¬æ¢MaybeUninit<u8>åˆ°u8
        let encoded_data: Vec<u8> = mp3_buffer[..encoded_size]
            .iter()
            .map(|x| unsafe { x.assume_init() })
            .collect();

        output_file
            .write_all(&encoded_data)
            .map_err(|e| anyhow::anyhow!("å†™å…¥MP3æ•°æ®å¤±è´¥: {}", e))?;

        // åˆ·æ–°ç¼–ç å™¨å¹¶å†™å…¥å‰©ä½™æ•°æ®
        let mut flush_buffer: Vec<MaybeUninit<u8>> = vec![MaybeUninit::uninit(); 7200];
        let flush_size = encoder
            .flush::<FlushNoGap>(&mut flush_buffer)
            .map_err(|e| anyhow::anyhow!("åˆ·æ–°MP3ç¼–ç å™¨å¤±è´¥: {}", e))?;

        if flush_size > 0 {
            let flush_data: Vec<u8> = flush_buffer[..flush_size]
                .iter()
                .map(|x| unsafe { x.assume_init() })
                .collect();

            output_file
                .write_all(&flush_data)
                .map_err(|e| anyhow::anyhow!("å†™å…¥MP3åˆ·æ–°æ•°æ®å¤±è´¥: {}", e))?;
        }

        #[cfg(debug_assertions)]
        {
            // MP3éŸ³é¢‘ä¿å­˜å®Œæˆ
        }
        Ok(())
    }
}

impl Default for LightweightTtsPipeline {
    fn default() -> Self {
        Self::new()
    }
}

impl LightweightTtsPipeline {
    fn get_ref_clip(wav: &Array1<f32>) -> Array1<f32> {
        // ä¸Refå®ç°ä¿æŒä¸€è‡´ï¼šé•¿åº¦ = (ref_segment_duration * sample_rate) // latent_hop_length * latent_hop_length
        let sample_rate: u32 = 16000;
        let ref_segment_duration: f32 = 6.0;
        let latent_hop_length: u32 = 320;

        let ref_segment_length = ((ref_segment_duration * sample_rate as f32) as u32
            / latent_hop_length
            * latent_hop_length) as usize;

        let wav_length = wav.len();
        if ref_segment_length > wav_length {
            // å¦‚æœéŸ³é¢‘ä¸è¶³æŒ‡å®šé•¿åº¦ï¼Œé‡å¤éŸ³é¢‘ç›´åˆ°è¾¾åˆ°è¦æ±‚
            let repeat_times = ref_segment_length / wav_length + 1;
            let mut repeated = Vec::with_capacity(wav_length * repeat_times);
            for _ in 0..repeat_times {
                repeated.extend(wav.iter());
            }
            Array1::from(repeated)
                .slice(ndarray::s![..ref_segment_length])
                .to_owned()
        } else {
            // æˆªå–æŒ‡å®šé•¿åº¦
            wav.slice(ndarray::s![..ref_segment_length]).to_owned()
        }
    }

    #[allow(dead_code)]
    fn extract_mel_spectrogram_simple(wav: &Array1<f32>) -> Result<Array2<f32>> {
        // å‚æ•°ä¸Refå®ç°ä¸€è‡´
        let n_mels: usize = 128;
        let n_fft: usize = 1024;
        let hop_length: usize = 320;
        let win_length: usize = 1024;
        let sample_rate: f32 = 16000.0;

        // center=true çš„å¡«å……
        let pad_width = n_fft / 2;
        let mut padded_wav = vec![0.0f32; wav.len() + 2 * pad_width];
        for (i, &sample) in wav.iter().enumerate() {
            padded_wav[pad_width + i] = sample;
        }

        let wav_len = padded_wav.len();
        let n_frames = if wav_len <= n_fft {
            1
        } else {
            (wav_len - n_fft) / hop_length + 1
        };

        // Hannçª—
        let window: Vec<f32> = if win_length == n_fft {
            (0..n_fft)
                .map(|i| {
                    let angle = 2.0 * std::f32::consts::PI * i as f32 / (n_fft - 1) as f32;
                    0.5 * (1.0 - angle.cos())
                })
                .collect()
        } else {
            let mut window = vec![0.0f32; n_fft];
            let start_pad = (n_fft - win_length) / 2;
            for i in 0..win_length {
                let angle = 2.0 * std::f32::consts::PI * i as f32 / (win_length - 1) as f32;
                window[start_pad + i] = 0.5 * (1.0 - angle.cos());
            }
            window
        };

        // ä¿®å¤ï¼šä½¿ç”¨ä¸C++å’ŒPythonå®ç°ä¸€è‡´çš„å‚æ•°
        // C++: melSpectrogram(ref_wav_samples, 16000, 1024, 320, 128, 10, 8000, 1.0f, true, false)
        // Python: extract_mel_spectrogram(wav, n_mels=128, n_fft=1024, hop_length=320, win_length=1024)
        // æ³¨æ„ï¼šC++ä¸­fmin=10, fmax=8000, power=1.0, center=true, norm=false(slaney)
        let mel_filters =
            Self::create_mel_filterbank_slaney_with_fmax(n_mels, n_fft, sample_rate, 10.0, 8000.0);

        let mut mel_spectrogram = Array2::zeros((n_mels, n_frames));

        for frame_idx in 0..n_frames {
            let start = frame_idx * hop_length;
            let end = (start + n_fft).min(wav_len);

            // æå–å¸§å¹¶åº”ç”¨çª—å‡½æ•°
            let mut frame = vec![0.0f32; n_fft];
            for i in 0..(end - start) {
                frame[i] = padded_wav[start + i] * window[i];
            }
            // é›¶å¡«å……å‰©ä½™éƒ¨åˆ†
            for item in frame.iter_mut().take(n_fft).skip(end - start) {
                *item = 0.0;
            }

            // è®¡ç®—åŠŸç‡è°±ï¼ˆç®€åŒ–ç‰ˆï¼‰
            let power_spectrum = Self::compute_power_spectrum(&frame);

            // åº”ç”¨æ¢…å°”æ»¤æ³¢å™¨
            for mel_idx in 0..n_mels {
                let mut mel_energy = 0.0f32;
                for freq_idx in 0..power_spectrum.len() {
                    mel_energy += power_spectrum[freq_idx] * mel_filters[[mel_idx, freq_idx]];
                }
                mel_spectrogram[[mel_idx, frame_idx]] = mel_energy;
            }
        }

        Ok(mel_spectrogram)
    }
}

impl LightweightTtsPipeline {
    // å°†å¯èƒ½å¸¦æœ‰ç»Ÿä¸€åç§»ï¼ˆå¦‚+8196ï¼‰çš„codebook tokenå®‰å…¨å½’ä¸€åˆ°æ¨¡å‹å£°ç å™¨æ‰€éœ€çš„åŸå§‹ç´¢å¼•ç©ºé—´
    #[allow(dead_code)]
    fn normalize_codebook_offset(tokens: &[i32], offset: i32) -> Vec<i32> {
        if tokens.is_empty() {
            return vec![];
        }
        let min_v = tokens.iter().copied().min().unwrap_or(0);
        if min_v >= offset {
            tokens.iter().map(|&t| t - offset).collect()
        } else {
            tokens.to_vec()
        }
    }
}
