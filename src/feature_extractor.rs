//! ç‰¹å¾æå–æ¨¡å—
//!
//! æœ¬æ¨¡å—è´Ÿè´£å¤„ç†TTSæ¨ç†ä¸­çš„ç‰¹å¾é¢„æå–åŠŸèƒ½ï¼Œ
//! åŒ…æ‹¬æ–‡æœ¬é¢„å¤„ç†ã€tokenåŒ–å’Œç‰¹å¾ç¼“å­˜ç­‰ã€‚

use anyhow::Result;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::Mutex;
use tracing::{debug, info, warn};
// use web_rwkv::runtime::model::State; // æš‚æ—¶æ³¨é‡Šæ‰æœªä½¿ç”¨çš„å¯¼å…¥
use web_rwkv::tokenizer::Tokenizer;

use crate::batch_types::TtsInferOptions;
use crate::shared_runtime::TtsInferContext;

/// ç‰¹å¾æå–å™¨ï¼Œè´Ÿè´£é¢„å¤„ç†å’Œç‰¹å¾ç¼“å­˜
pub struct FeatureExtractor {
    /// ç‰¹å¾ç¼“å­˜
    feature_cache: Arc<Mutex<HashMap<String, Vec<u16>>>>,
    /// æœ€å¤§ç¼“å­˜å¤§å°
    max_cache_size: usize,
}

impl FeatureExtractor {
    /// åˆ›å»ºæ–°çš„ç‰¹å¾æå–å™¨
    pub fn new(max_cache_size: usize) -> Self {
        Self {
            feature_cache: Arc::new(Mutex::new(HashMap::new())),
            max_cache_size,
        }
    }

    /// é¢„å¤„ç†æ–‡æœ¬å¹¶æå–ç‰¹å¾
    pub async fn extract_features(&self, context: &TtsInferContext) -> Result<Vec<u16>> {
        let text = &context.text;
        let tokenizer = &context.tokenizer;

        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached_tokens) = self.get_cached_features(text).await {
            debug!("ğŸ¯ ä½¿ç”¨ç¼“å­˜ç‰¹å¾: {} tokens", cached_tokens.len());
            return Ok(cached_tokens);
        }

        // é¢„å¤„ç†æ–‡æœ¬
        let processed_text = self.preprocess_text(text, &context.options)?;

        // TokenåŒ–
        let tokens = self.tokenize_text(&processed_text, tokenizer)?;

        // ç¼“å­˜ç»“æœ
        self.cache_features(text.clone(), tokens.clone()).await;

        info!("âœ… æå–ç‰¹å¾å®Œæˆ: {} -> {} tokens", text.len(), tokens.len());
        Ok(tokens)
    }

    /// é¢„å¤„ç†æ–‡æœ¬
    fn preprocess_text(&self, text: &str, _options: &TtsInferOptions) -> Result<String> {
        let mut processed = text.to_string();

        // åŸºæœ¬æ¸…ç†
        processed = processed.trim().to_string();

        // é¢„å¤„ç†æ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        processed = processed.trim().to_string();
        processed = processed.replace("\n", " ").replace("\t", " ");
        // åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        while processed.contains("  ") {
            processed = processed.replace("  ", " ");
        }

        debug!("ğŸ“ æ–‡æœ¬é¢„å¤„ç†: '{}' -> '{}'", text, processed);
        Ok(processed)
    }

    /// TokenåŒ–æ–‡æœ¬
    fn tokenize_text(&self, text: &str, tokenizer: &Tokenizer) -> Result<Vec<u16>> {
        let tokens = tokenizer
            .encode(text.as_bytes())
            .map_err(|e| anyhow::anyhow!("TokenåŒ–å¤±è´¥: {}", e))?
            .iter()
            .map(|&id| id as u16)
            .collect::<Vec<u16>>();

        debug!(
            "ğŸ”¤ TokenåŒ–å®Œæˆ: {} chars -> {} tokens",
            text.len(),
            tokens.len()
        );
        Ok(tokens)
    }

    /// ä»ç¼“å­˜è·å–ç‰¹å¾
    async fn get_cached_features(&self, text: &str) -> Option<Vec<u16>> {
        let cache = self.feature_cache.lock().await;
        cache.get(text).cloned()
    }

    /// ç¼“å­˜ç‰¹å¾
    async fn cache_features(&self, text: String, tokens: Vec<u16>) {
        let mut cache = self.feature_cache.lock().await;

        // æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if cache.len() >= self.max_cache_size {
            // ç®€å•çš„LRUç­–ç•¥ï¼šæ¸…ç†ä¸€åŠç¼“å­˜
            let keys_to_remove: Vec<_> = cache.keys().take(cache.len() / 2).cloned().collect();
            for key in keys_to_remove {
                cache.remove(&key);
            }
            warn!("ğŸ§¹ ç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†äº† {} ä¸ªæ¡ç›®", cache.len() / 2);
        }

        cache.insert(text, tokens);
        debug!("ğŸ’¾ ç¼“å­˜ç‰¹å¾ï¼Œå½“å‰ç¼“å­˜å¤§å°: {}", cache.len());
    }

    /// æ¸…ç†ç¼“å­˜
    pub async fn clear_cache(&self) {
        let mut cache = self.feature_cache.lock().await;
        cache.clear();
        info!("ğŸ§¹ æ¸…ç†ç‰¹å¾ç¼“å­˜");
    }

    /// è·å–ç¼“å­˜ç»Ÿè®¡
    pub async fn cache_stats(&self) -> FeatureCacheStats {
        let cache = self.feature_cache.lock().await;
        FeatureCacheStats {
            size: cache.len(),
            max_size: self.max_cache_size,
        }
    }
}

/// ç‰¹å¾ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct FeatureCacheStats {
    pub size: usize,
    pub max_size: usize,
}

/// é¢„æå–ç‰¹å¾å¤„ç†å™¨
/// è´Ÿè´£æ‰¹é‡é¢„å¤„ç†å’Œç‰¹å¾æå–
pub struct PreExtractProcessor {
    feature_extractor: FeatureExtractor,
}

impl PreExtractProcessor {
    /// åˆ›å»ºæ–°çš„é¢„æå–å¤„ç†å™¨
    pub fn new(max_cache_size: usize) -> Self {
        Self {
            feature_extractor: FeatureExtractor::new(max_cache_size),
        }
    }

    /// æ‰¹é‡é¢„æå–ç‰¹å¾
    pub async fn batch_pre_extract(&self, contexts: &[TtsInferContext]) -> Result<Vec<Vec<u16>>> {
        let mut results = Vec::with_capacity(contexts.len());

        for context in contexts {
            let tokens = self.feature_extractor.extract_features(context).await?;
            results.push(tokens);
        }

        info!("âœ… æ‰¹é‡é¢„æå–å®Œæˆ: {} ä¸ªè¯·æ±‚", contexts.len());
        Ok(results)
    }

    /// è·å–ç‰¹å¾æå–å™¨å¼•ç”¨
    pub fn feature_extractor(&self) -> &FeatureExtractor {
        &self.feature_extractor
    }
}

/// ç‰¹å¾æå–ç›¸å…³çš„è¾…åŠ©å‡½æ•°
pub mod utils {
    use super::*;

    /// éªŒè¯tokenåºåˆ—çš„æœ‰æ•ˆæ€§
    pub fn validate_tokens(tokens: &[u16], vocab_size: usize) -> Result<()> {
        for &token in tokens {
            if token as usize >= vocab_size {
                return Err(anyhow::anyhow!(
                    "æ— æ•ˆtoken: {} >= vocab_size({})",
                    token,
                    vocab_size
                ));
            }
        }
        Ok(())
    }

    /// è®¡ç®—tokenåºåˆ—çš„ç»Ÿè®¡ä¿¡æ¯
    pub fn token_stats(tokens: &[u16]) -> TokenStats {
        if tokens.is_empty() {
            return TokenStats {
                count: 0,
                unique_count: 0,
                min_token: 0,
                max_token: 0,
            };
        }

        let mut unique_tokens = std::collections::HashSet::new();
        let mut min_token = tokens[0];
        let mut max_token = tokens[0];

        for &token in tokens {
            unique_tokens.insert(token);
            min_token = min_token.min(token);
            max_token = max_token.max(token);
        }

        TokenStats {
            count: tokens.len(),
            unique_count: unique_tokens.len(),
            min_token,
            max_token,
        }
    }

    /// Tokenç»Ÿè®¡ä¿¡æ¯
    #[derive(Debug, Clone)]
    pub struct TokenStats {
        pub count: usize,
        pub unique_count: usize,
        pub min_token: u16,
        pub max_token: u16,
    }
}
