//! RWKVæ¨¡å‹æ¨ç†é‡‡æ ·å™¨
//! å®ç°åŸºäºweb-rwkvåº“çš„RWKVæ¨¡å‹æ¨ç†å’Œé‡‡æ ·åŠŸèƒ½

use crate::voice_feature_manager::VoiceFeatureManager;
use anyhow::Result;
use memmap2::Mmap;
use rand::{rngs::StdRng, Rng, SeedableRng};
use safetensors::SafeTensors;
use serde::de::DeserializeSeed;
use sha2::{Digest, Sha256};
use std::collections::HashMap;
use std::path::Path;
use std::sync::atomic::{AtomicUsize, Ordering};
use web_rwkv::{
    context::{Context, ContextBuilder, InstanceExt},
    runtime::{
        infer::{Rnn, RnnInput, RnnInputBatch, RnnOption},
        loader::Loader,
        model::{ContextAutoLimits, ModelBuilder, ModelInfo, ModelVersion, Quant},
        v7, Runtime, TokioRuntime,
    },
    tensor::serialization::Seed,
    tokenizer::Tokenizer,
    wgpu::{self, Instance},
};

// Import optimization components
use crate::inference_state_manager::{InferenceStateConfig, InferenceStateManager};
use crate::streaming_inference::{BatchConfig, StreamingInference};
use std::sync::Arc;
use std::time::Duration;

/// å…¬å¼€çš„é‡‡æ ·å‡½æ•°ï¼Œæ”¯æŒä¼ å…¥RNGå‚æ•°
/// åŒ¹é…PythonåŸç‰ˆsampler_simple_batchçš„è¡Œä¸ºï¼šæ¸©åº¦åº”ç”¨åœ¨logitsä¸Šï¼Œæ”¯æŒå™ªå£°
pub fn sample_logits(
    logits: &[f32],
    args: &SamplerArgs,
    forbid_token: Option<usize>,
    rng: &mut Option<StdRng>,
) -> usize {
    // å®ç°ä¸Pythonç‰ˆæœ¬ä¸€è‡´çš„sample_logitsé€»è¾‘
    sample_logits_with_top_p_k(
        logits,
        args.temperature,
        args.top_p,
        args.top_k,
        forbid_token,
        rng,
    )
}

/// å®ç°ä¸Pythonç‰ˆæœ¬ä¸€è‡´çš„sample_logitså‡½æ•°
/// æ”¯æŒtemperatureã€top_pã€top_ké‡‡æ ·
/// æŒ‰ç…§Pythonç‰ˆæœ¬çš„é¡ºåºï¼šsoftmax -> top_k -> top_p -> temperature -> multinomial
pub fn sample_logits_with_top_p_k(
    logits: &[f32],
    temperature: f32,
    top_p: f32,
    top_k: usize,
    forbid_token: Option<usize>,
    rng: &mut Option<StdRng>,
) -> usize {
    let vocab_size = logits.len();
    if vocab_size == 0 {
        return 0;
    }

    // åˆ›å»ºå¯ä¿®æ”¹çš„logitså‰¯æœ¬
    let mut modified_logits = logits.to_vec();

    // å¤„ç†ç¦æ­¢token
    if let Some(ft) = forbid_token {
        if ft < vocab_size {
            modified_logits[ft] = f32::NEG_INFINITY;
        }
    }

    // æ­¥éª¤1: è®¡ç®—softmaxæ¦‚ç‡ï¼ˆåŒ¹é…Pythonç‰ˆæœ¬ï¼šprobs = F.softmax(logits.float(), dim=-1)ï¼‰
    let max_logit = modified_logits
        .iter()
        .fold(f32::NEG_INFINITY, |a, &b| a.max(b));
    let mut probs: Vec<f32> = modified_logits
        .iter()
        .map(|&logit| (logit - max_logit).exp())
        .collect();

    let sum: f32 = probs.iter().sum();
    if sum > 0.0 {
        for prob in probs.iter_mut() {
            *prob /= sum;
        }
    }

    // æ­¥éª¤2: Top-kæˆªæ–­ï¼ˆåŒ¹é…Pythonç‰ˆæœ¬ï¼šprobs[sorted_ids[top_k:]] = 0ï¼‰
    if top_k > 0 && top_k < vocab_size {
        // åˆ›å»ºç´¢å¼•-æ¦‚ç‡å¯¹å¹¶æ’åº
        let mut indexed_probs: Vec<(usize, f32)> =
            probs.iter().enumerate().map(|(i, &p)| (i, p)).collect();
        indexed_probs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

        // ä¿ç•™top-kï¼Œå…¶ä½™è®¾ä¸º0
        for (idx, _) in indexed_probs.iter().skip(top_k) {
            probs[*idx] = 0.0;
        }
    }

    // æ­¥éª¤3: Top-pæˆªæ–­ï¼ˆåŒ¹é…Pythonç‰ˆæœ¬çš„å¤æ‚é€»è¾‘ï¼‰
    if top_p < 1.0 {
        // åˆ›å»ºç´¢å¼•-æ¦‚ç‡å¯¹å¹¶æ’åº
        let mut indexed_probs: Vec<(usize, f32)> =
            probs.iter().enumerate().map(|(i, &p)| (i, p)).collect();
        indexed_probs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

        // è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        let mut cumulative_prob = 0.0;
        let mut cutoff_index = None;
        let mut cutoff_prob = 0.0;

        for (i, (_, prob)) in indexed_probs.iter().enumerate() {
            cumulative_prob += prob;
            if cumulative_prob >= top_p {
                cutoff_index = Some(i);
                cutoff_prob = *prob;
                break;
            }
        }

        if let Some(_cutoff_idx) = cutoff_index {
            // å°†å°äºcutoffçš„æ¦‚ç‡è®¾ä¸º0
            for prob in probs.iter_mut() {
                if *prob < cutoff_prob {
                    *prob = 0.0;
                }
            }

            // å¤„ç†ç­‰äºcutoffçš„æ¦‚ç‡ï¼ˆåŒ¹é…Pythonç‰ˆæœ¬çš„ç²¾ç¡®é€»è¾‘ï¼‰
            if top_p > 0.0 {
                let current_sum: f32 = probs.iter().sum();
                if current_sum < top_p {
                    let remaining = top_p - current_sum;
                    let cutoff_count = probs.iter().filter(|&&p| p == cutoff_prob).count();
                    if cutoff_count > 0 {
                        let adjustment = remaining / cutoff_count as f32;
                        for prob in probs.iter_mut() {
                            if *prob == cutoff_prob {
                                *prob = cutoff_prob + adjustment;
                            }
                        }
                    }
                }
            }
        }
    }

    // æ­¥éª¤4: åº”ç”¨æ¸©åº¦ï¼ˆåŒ¹é…Pythonç‰ˆæœ¬ï¼šprobs = probs ** (1.0 / temperature)ï¼‰
    if temperature != 1.0 && temperature > 0.0 {
        let temp_inv = 1.0 / temperature;
        for prob in probs.iter_mut() {
            if *prob > 0.0 {
                *prob = prob.powf(temp_inv);
            }
        }

        // é‡æ–°å½’ä¸€åŒ–
        let sum: f32 = probs.iter().sum();
        if sum > 0.0 {
            for prob in probs.iter_mut() {
                *prob /= sum;
            }
        }
    }

    // æ­¥éª¤5: å¤šé¡¹å¼é‡‡æ ·ï¼ˆåŒ¹é…Pythonç‰ˆæœ¬ï¼štorch.multinomial(probs, num_samples=1).item()ï¼‰
    if let Some(rng_ref) = rng {
        // ä½¿ç”¨ä¼ å…¥çš„RNG
        let rand_val: f32 = rng_ref.gen();
        let mut cumulative = 0.0;
        for (i, &prob) in probs.iter().enumerate() {
            cumulative += prob;
            if rand_val <= cumulative {
                return i;
            }
        }
        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€ä¸ªéé›¶æ¦‚ç‡çš„ç´¢å¼•
        for (i, &prob) in probs.iter().enumerate().rev() {
            if prob > 0.0 {
                return i;
            }
        }
    } else {
        // æ²¡æœ‰RNGæ—¶ä½¿ç”¨ç¡®å®šæ€§ç§å­
        let mut temp_rng = StdRng::seed_from_u64(42);
        let rand_val: f32 = temp_rng.gen();
        let mut cumulative = 0.0;
        for (i, &prob) in probs.iter().enumerate() {
            cumulative += prob;
            if rand_val <= cumulative {
                return i;
            }
        }
        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€ä¸ªéé›¶æ¦‚ç‡çš„ç´¢å¼•
        for (i, &prob) in probs.iter().enumerate().rev() {
            if prob > 0.0 {
                return i;
            }
        }
    }

    // æœ€åçš„å›é€€
    0
}

// å·²åˆ é™¤æœ‰é—®é¢˜çš„sample_logits_implå‡½æ•°ï¼Œç»Ÿä¸€ä½¿ç”¨FastSamplerç‰ˆæœ¬çš„sample_logits

/// åŠ è½½ç±»å‹æšä¸¾
enum LoadType {
    SafeTensors(Vec<u8>), // å­˜å‚¨åŸå§‹æ•°æ®è€Œä¸æ˜¯å¼•ç”¨
    Prefab(Vec<u8>),
}

/// æ‰¹å¤„ç†TTSè¯·æ±‚ç»“æ„
#[derive(Debug, Clone)]
pub struct TtsBatchRequest {
    pub text: String,
    pub property_tokens: Vec<i32>,
    pub ref_global_tokens: Option<Vec<i32>>,
    pub ref_semantic_tokens: Option<Vec<i32>>,
    pub args: SamplerArgs,
    /// éŸ³è‰²IDï¼Œç”¨äºä»ç¼“å­˜ä¸­å¿«é€Ÿè·å–tokens
    pub voice_id: Option<String>,
}

/// é‡‡æ ·å‚æ•°
#[derive(Debug, Clone)]
pub struct SamplerArgs {
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: usize,
    pub max_tokens: usize,
    // å¯é€‰éšæœºç§å­ï¼šæä¾›åˆ™å¯ç”¨ç¡®å®šæ€§é‡‡æ ·
    pub seed: Option<u64>,
    // éŸ³è‰²ä¿çœŸåº¦æ§åˆ¶ï¼š0.0-1.0ï¼Œè¶Šé«˜è¶Šä¿æŒå‚è€ƒéŸ³è‰²
    pub voice_fidelity: f32,
    // åˆ†å±‚éšæœºæ€§æ§åˆ¶
    pub layered_randomness: LayeredRandomnessConfig,
    // Token chunk sizeé…ç½®
    pub token_chunk_size: usize,
}

/// åˆ†å±‚éšæœºæ€§é…ç½®
#[derive(Debug, Clone)]
pub struct LayeredRandomnessConfig {
    /// Globalé˜¶æ®µçš„éšæœºæ€§å¼ºåº¦ (0.0-1.0)
    pub global_randomness: f32,
    /// Semanticé˜¶æ®µçš„éšæœºæ€§å¼ºåº¦ (0.0-1.0)
    pub semantic_randomness: f32,
    /// æ˜¯å¦ä½¿ç”¨ç‹¬ç«‹çš„ç§å­ç­–ç•¥
    pub use_independent_seeds: bool,
    /// Globalé˜¶æ®µç§å­åç§»
    pub global_seed_offset: u64,
    /// Semanticé˜¶æ®µç§å­åç§»
    pub semantic_seed_offset: u64,
}

impl Default for LayeredRandomnessConfig {
    fn default() -> Self {
        Self {
            global_randomness: 0.1,   // å¤§å¹…é™ä½globalé˜¶æ®µéšæœºæ€§ï¼Œä¿æŠ¤éŸ³è‰²ç‰¹å¾
            semantic_randomness: 0.4, // é€‚åº¦é™ä½semanticé˜¶æ®µéšæœºæ€§
            use_independent_seeds: true,
            global_seed_offset: 1000,
            semantic_seed_offset: 2000,
        }
    }
}

impl Default for SamplerArgs {
    fn default() -> Self {
        Self {
            temperature: 1.0,
            top_p: 0.85, // ä¿®å¤ï¼šä¸Pythonç‰ˆæœ¬sample_logitså‡½æ•°é»˜è®¤å€¼ä¸€è‡´
            top_k: 0,
            max_tokens: 2048, // ä¿®å¤ï¼šæé«˜é»˜è®¤å€¼ä»¥æ”¯æŒæ›´é•¿çš„éŸ³é¢‘ç”Ÿæˆ
            seed: None,
            voice_fidelity: 0.8, // é»˜è®¤é«˜éŸ³è‰²ä¿çœŸåº¦
            layered_randomness: LayeredRandomnessConfig::default(),
            token_chunk_size: 512, // é»˜è®¤token chunk size
        }
    }
}

/// Prefabæ–‡ä»¶ç»“æ„ä½“
/// TTSç›¸å…³å¸¸é‡
pub const TTS_EOS_TOKEN: i32 = 8192;
pub const TTS_TAG_0: i32 = 8193;
pub const TTS_TAG_1: i32 = 8194;
pub const TTS_TAG_2: i32 = 8195;
// æ³¨æ„ï¼šä»¥ä¸‹åç§»é‡å¸¸é‡å·²åºŸå¼ƒï¼Œæ ¹æ®C++ä»£ç ï¼Œtokensåº”ç›´æ¥ä½¿ç”¨åŸå§‹ID
pub const GLOBAL_TOKEN_OFFSET: i32 = 8196; // Global tokensåœ¨prefillæ—¶éœ€è¦åç§»
                                           // pub const SEMANTIC_TOKEN_OFFSET: i32 = 4096; // å·²åºŸå¼ƒï¼šä¸å†ç»™tokensæ·»åŠ åç§»

/// RWKVé‡‡æ ·å™¨ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’ŒTTS tokens
pub struct RwkvSampler {
    runtime: Box<dyn Runtime<Rnn> + Send + Sync>, // ä½¿ç”¨TokioRuntimeå°è£…Bundle
    tokenizer: Tokenizer,
    // å¸¦ç§å­çš„RNGï¼ˆå¯é€‰ï¼Œå¯ç”¨åˆ™å®ç°ç¡®å®šæ€§é‡‡æ ·ï¼‰
    rng: Option<StdRng>,
    batch_counter: AtomicUsize,
    // Token chunk sizeé…ç½®
    token_chunk_size: usize,
    // Optimization components
    #[allow(dead_code)]
    streaming_inference: Option<Arc<StreamingInference>>,
    inference_state_manager: Arc<InferenceStateManager>,
}
impl RwkvSampler {
    /// åˆ›å»ºé»˜è®¤é‡åŒ–é…ç½®
    /// é»˜è®¤ä¸ä½¿ç”¨é‡åŒ–ä»¥æé«˜æ¨ç†ç²¾åº¦
    pub fn default_quant_config() -> HashMap<usize, Quant> {
        HashMap::new() // è¿”å›ç©ºé…ç½®ï¼Œä¸ä½¿ç”¨é‡åŒ–
    }

    /// åˆ›å»ºæ–°çš„RWKVé‡‡æ ·å™¨
    ///
    /// # Arguments
    /// * `model_path` - RWKVæ¨¡å‹ç›®å½•æˆ–æ¨¡å‹æ–‡ä»¶(.safetensors)è·¯å¾„
    /// * `vocab_path` - è¯è¡¨æ–‡ä»¶è·¯å¾„
    /// * `quant_config` - é‡åŒ–é…ç½®ï¼ŒNoneè¡¨ç¤ºä¸ä½¿ç”¨é‡åŒ–
    ///
    /// # Returns
    /// * `Result<RwkvSampler>` - RWKVé‡‡æ ·å™¨å®ä¾‹æˆ–é”™è¯¯
    pub async fn new(
        model_path: &str,
        vocab_path: &str,
        quant_config: Option<HashMap<usize, Quant>>,
        token_chunk_size: usize,
    ) -> Result<Self> {
        // æ£€æŸ¥æ¨¡å‹ç›®å½•/æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        let model_path_ref = Path::new(model_path);
        if !model_path_ref.exists() {
            return Err(anyhow::anyhow!("æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {}", model_path));
        }

        // æ£€æŸ¥è¯è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if !Path::new(vocab_path).exists() {
            return Err(anyhow::anyhow!("è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {}", vocab_path));
        }

        // è§£ææ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼š
        // - è‹¥ä¼ å…¥ç›®å½•ï¼Œåˆ™ä¼˜å…ˆæŸ¥æ‰¾ "webrwkv.safetensors"ï¼Œå…¶æ¬¡ "rwkvtts-Int8_22.safetensors"
        // - è‹¥ä¼ å…¥æ–‡ä»¶ï¼Œåˆ™ç›´æ¥ä½¿ç”¨è¯¥æ–‡ä»¶
        let model_file_path = if model_path_ref.is_dir() {
            let prefab_path = model_path_ref.join("webrwkv.safetensors");
            let safetensors_path = model_path_ref.join("rwkvtts-Int8_22.safetensors");
            if prefab_path.exists() {
                prefab_path
            } else if safetensors_path.exists() {
                safetensors_path
            } else {
                return Err(anyhow::anyhow!(
                    "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: åœ¨ç›®å½• {} ä¸­æœªæ‰¾åˆ° webrwkv.safetensors æˆ– rwkvtts-Int8_22.safetensors",
                    model_path
                ));
            }
        } else {
            model_path_ref.to_path_buf()
        };
        if !model_file_path.exists() {
            return Err(anyhow::anyhow!(
                "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {}",
                model_file_path.display()
            ));
        }

        // åŠ è½½æ¨¡å‹æ–‡ä»¶
        let file = std::fs::File::open(&model_file_path)?;
        let _file_size = file.metadata()?.len();
        let data = unsafe { Mmap::map(&file)? };

        // æ¨¡å‹å®Œæ•´æ€§æ ¡éªŒï¼šæ‰“å°å¤§å°ä¸SHA256
        let mut hasher = Sha256::new();
        hasher.update(&data[..]);

        // åˆ›å»º GPU ä¸Šä¸‹æ–‡
        let instance = Instance::default();
        let adapter = instance
            .adapter(wgpu::PowerPreference::HighPerformance)
            .await?;

        // æ£€æµ‹æ¨¡å‹æ ¼å¼
        let load_type = {
            // é¦–å…ˆå°è¯•SafeTensorsæ ¼å¼
            if SafeTensors::deserialize(&data).is_ok() {
                // SafeTensors æ ¼å¼æ¨¡å‹
                LoadType::SafeTensors(data.to_vec())
            } else {
                // å¦‚æœä¸æ˜¯SafeTensorsï¼Œå‡è®¾æ˜¯prefabæ ¼å¼
                // prefab æ ¼å¼æ¨¡å‹
                LoadType::Prefab(data.to_vec())
            }
        };

        // ä¸ºV7æ¨¡å‹åˆ›å»ºé»˜è®¤ä¿¡æ¯ï¼ˆç¨ååœ¨å®é™…åŠ è½½æ—¶ä¼šè¢«éªŒè¯ï¼‰
        let info = ModelInfo {
            version: ModelVersion::V7,
            num_vocab: 65536,           // é»˜è®¤å€¼ï¼Œå®é™…å€¼ä¼šåœ¨æ¨¡å‹åŠ è½½æ—¶ç¡®å®š
            num_layer: 32,              // é»˜è®¤å€¼
            num_emb: 4096,              // é»˜è®¤å€¼
            num_hidden: 4096,           // é»˜è®¤å€¼
            num_head: 32,               // é»˜è®¤å€¼
            custom: Default::default(), // é»˜è®¤å€¼
        };

        // åŸºäºæ¨¡å‹ä¿¡æ¯è‡ªåŠ¨é…ç½® Context çš„ç¡¬ä»¶ limits

        // æ‰“å°é€‚é…å™¨/åç«¯/é©±åŠ¨ä¸ç²¾åº¦

        let context = ContextBuilder::new(adapter)
            .auto_limits(&info)
            .build()
            .await?;

        // æ ¹æ®åŠ è½½ç±»å‹åˆ›å»ºV7æ¨¡å‹
        let model = match load_type {
            LoadType::SafeTensors(data_vec) => {
                // ä»Vec<u8>é‡æ–°åˆ›å»ºSafeTensors
                let safetensors = SafeTensors::deserialize(&data_vec)
                    .map_err(|e| anyhow::anyhow!("Failed to deserialize SafeTensors: {}", e))?;

                // è·å–å¹¶éªŒè¯æ¨¡å‹ä¿¡æ¯
                let actual_info = Loader::info(&safetensors)?;
                if actual_info.version != ModelVersion::V7 {
                    return Err(anyhow::anyhow!(
                        "Only V7 models are supported, got {:?}",
                        actual_info.version
                    ));
                }
                // æ¨¡å‹ä¿¡æ¯éªŒè¯

                let mut builder = ModelBuilder::new(&context, safetensors);
                if let Some(quant) = quant_config {
                    builder = builder.quant(quant);
                }
                builder.build_v7().await?
            }
            LoadType::Prefab(data_vec) => {
                // ä½¿ç”¨cbor4ii Deserializerååºåˆ—åŒ–prefabæ•°æ®
                // å‚è€ƒweb-rwkvçš„serdeç¤ºä¾‹å®ç°
                use cbor4ii::{core::utils::SliceReader, serde::Deserializer};

                // ååºåˆ—åŒ–V7 prefabæ¨¡å‹
                let reader = SliceReader::new(&data_vec);
                let mut deserializer = Deserializer::new(reader);

                let seed = Seed::<Context, v7::Model>::new(&context);
                seed.deserialize(&mut deserializer)
                    .map_err(|e| anyhow::anyhow!("Failed to deserialize v7 model: {}", e))?
            }
        };

        // åˆ›å»ºBundleä¸TokioRuntimeï¼ˆåˆ‡æ¢ä¸º f32 ä»¥å¯ç”¨ FP32 æ¨ç†ï¼‰
        // å¢åŠ batch sizeä»¥æ”¯æŒå¹¶å‘æ¨ç†
        let max_batch = 8;
        let bundle = v7::Bundle::<f32>::new(model, max_batch);
        let runtime: Box<dyn Runtime<Rnn> + Send + Sync> =
            Box::new(TokioRuntime::new(bundle).await);

        // åŠ è½½tokenizer
        let vocab_content = std::fs::read_to_string(vocab_path)?;
        let tokenizer = Tokenizer::new(&vocab_content)?;

        // Initialize inference state manager
        let inference_state_config = InferenceStateConfig {
            max_cache_entries: 200,
            max_entry_age: std::time::Duration::from_secs(600),
            batch_inference_size: 8,
            prediction_window: 16,
            enable_async_pre_inference: true,
            state_similarity_threshold: 0.95,
        };
        let inference_state_manager = Arc::new(InferenceStateManager::new(inference_state_config));

        // Initialize StreamingInference
        let batch_config = BatchConfig {
            max_batch_size: 8,
            batch_timeout: Duration::from_millis(50),
            dynamic_batching: true,
            min_batch_size: 2,
            prefetch_window: 4,
        };
        let streaming_inference = Arc::new(StreamingInference::new(batch_config));

        Ok(Self {
            runtime,
            tokenizer,
            rng: None,
            batch_counter: AtomicUsize::new(0),
            token_chunk_size,
            streaming_inference: Some(streaming_inference),
            inference_state_manager,
        })
    }

    /// è®¾ç½®éšæœºç§å­ï¼ˆå¯ç”¨ç¡®å®šæ€§é‡‡æ ·ï¼‰ã€‚ä¼ Noneåˆ™å…³é—­ç¡®å®šæ€§æ¨¡å¼ã€‚
    pub fn set_seed(&mut self, seed: Option<u64>) {
        self.rng = seed.map(StdRng::seed_from_u64);
    }

    /// ä¸ºç‰¹å®šé˜¶æ®µåˆ›å»ºç‹¬ç«‹çš„RNG
    pub fn create_stage_rng(&self, base_seed: Option<u64>, stage_offset: u64) -> Option<StdRng> {
        base_seed.map(|seed| StdRng::seed_from_u64(seed.wrapping_add(stage_offset)))
    }

    /// åº”ç”¨éŸ³è‰²ä¿çœŸåº¦è°ƒæ•´é‡‡æ ·å‚æ•°
    pub fn apply_voice_fidelity_adjustment(
        &self,
        args: &SamplerArgs,
        stage_randomness: f32,
    ) -> SamplerArgs {
        let mut adjusted_args = args.clone();

        // æ ¹æ®éŸ³è‰²ä¿çœŸåº¦å’Œé˜¶æ®µéšæœºæ€§è°ƒæ•´é‡‡æ ·å‚æ•°
        let fidelity_factor = args.voice_fidelity;
        let randomness_factor = stage_randomness;

        // é«˜ä¿çœŸåº¦ + ä½éšæœºæ€§ = æ›´ä¿å®ˆçš„é‡‡æ ·
        let conservative_factor = fidelity_factor * (1.0 - randomness_factor);

        // è°ƒæ•´æ¸©åº¦ï¼šä¿çœŸåº¦è¶Šé«˜ï¼Œæ¸©åº¦è¶Šä½
        adjusted_args.temperature = args.temperature * (0.5 + 0.5 * (1.0 - conservative_factor));

        // è°ƒæ•´top_pï¼šä¿çœŸåº¦è¶Šé«˜ï¼Œtop_pè¶Šå°ï¼ˆæ›´é›†ä¸­é‡‡æ ·ï¼‰
        adjusted_args.top_p = args.top_p * (0.7 + 0.3 * (1.0 - conservative_factor));

        // è°ƒæ•´top_kï¼šä¿çœŸåº¦è¶Šé«˜ï¼Œtop_kè¶Šå°
        if adjusted_args.top_k > 0 {
            let reduction_factor = 0.5 + 0.5 * (1.0 - conservative_factor);
            adjusted_args.top_k =
                ((adjusted_args.top_k as f32) * reduction_factor).max(1.0) as usize;
        }

        adjusted_args
    }

    /// åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼ˆå¤ç”¨å·²åŠ è½½çš„æ¨¡å‹å’Œtokenizerï¼‰
    /// è¿™æ ·å¯ä»¥é¿å…é‡æ–°åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ç¡®ä¿æ¯ä¸ªä¸Šä¸‹æ–‡æœ‰ç‹¬ç«‹çš„çŠ¶æ€
    /// æ³¨æ„ï¼šç”±äºRuntimeæ˜¯traitå¯¹è±¡ï¼Œæ— æ³•ç›´æ¥cloneï¼Œéœ€è¦é‡æ–°åˆ›å»º
    pub async fn create_independent_context(
        model_path: &str,
        vocab_path: &str,
        quant_config: Option<HashMap<usize, Quant>>,
    ) -> Result<Self> {
        // é‡æ–°åˆ›å»ºä¸€ä¸ªæ–°çš„é‡‡æ ·å™¨å®ä¾‹
        // è™½ç„¶è¿™ä¼šé‡æ–°åŠ è½½æ¨¡å‹ï¼Œä½†ç¡®ä¿äº†å®Œå…¨ç‹¬ç«‹çš„çŠ¶æ€
        Self::new(model_path, vocab_path, quant_config, 512).await
    }

    /// ä¸ºè¯·æ±‚ç”Ÿæˆå”¯ä¸€ID
    fn generate_request_id(&self) -> String {
        let counter = self.batch_counter.load(Ordering::SeqCst);
        let mut id = String::with_capacity(16); // é¢„åˆ†é…è¶³å¤Ÿå®¹é‡
        use std::fmt::Write;
        write!(&mut id, "req_{}", counter).unwrap();
        id
    }

    /// åªè¯»è®¿é—®å†…éƒ¨tokenizerï¼ˆç”¨äºå¤–éƒ¨æŒ‰ç›¸åŒæ–¹å¼ç¼–ç å±æ€§ï¼‰
    pub fn tokenizer(&self) -> &Tokenizer {
        &self.tokenizer
    }

    /// ç”Ÿæˆæ–‡æœ¬ï¼ˆç¤ºä¾‹ï¼‰
    pub async fn generate_text(&mut self, prompt: &str, args: &SamplerArgs) -> Result<String> {
        // è‹¥æä¾›äº†ç§å­ï¼Œè®¾ç½®ç¡®å®šæ€§é‡‡æ ·
        self.set_seed(args.seed);

        // ç¼–ç prompt
        let prompt_tokens: Vec<u32> = self
            .tokenizer
            .encode(prompt.as_bytes())
            .map_err(|e| anyhow::anyhow!(e.to_string()))?;
        let prompt_batch = RnnInputBatch::new(prompt_tokens.clone(), RnnOption::Last);
        let mut inference = RnnInput::new(vec![prompt_batch], self.token_chunk_size);

        // é¢„å¡«å……é˜¶æ®µï¼šå…ˆæŠŠå®Œæ•´ prompt åƒå®Œï¼Œç›´åˆ° runtime å¼€å§‹äº§ç”Ÿè¾“å‡º
        loop {
            let (next_inference, output) = self.runtime.infer(inference).await?;
            inference = next_inference;
            if output[0].0.size() > 0 {
                break;
            }
        }

        // é‡‡æ ·ç”Ÿæˆ
        let mut generated: Vec<u32> = Vec::with_capacity(args.max_tokens);
        for _ in 0..args.max_tokens {
            // æ¯æ­¥ä»…æ¶ˆè€—å½“å‰å‰©ä½™è¾“å…¥ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ï¼Œå¹¶åŸºäºè¾“å‡ºé‡‡æ ·ä¸€ä¸ªæ–° token
            let (next_inference, output) = self.runtime.infer(inference).await?;
            inference = next_inference;

            // è‹¥ä»åœ¨æ¶ˆè€—è¾“å…¥ï¼ˆsize==0ï¼‰ï¼Œç»§ç»­ç›´åˆ°äº§ç”Ÿè¾“å‡º
            if output[0].0.size() == 0 {
                continue;
            }

            let logits = output[0].0.clone().to_vec();
            let next_id = self.sample_logits(&logits, args, None) as u32;

            // å°†æ–° token è¿½åŠ åˆ°åç»­è¾“å…¥ä¸­ï¼Œå®ç°å¢é‡æ¨ç†
            inference.batches[0].push(next_id);
            generated.push(next_id);
        }

        // è§£ç ï¼ˆprompt + ç”Ÿæˆï¼‰
        let mut all = prompt_tokens;
        all.extend_from_slice(&generated);
        let decoded = self
            .tokenizer
            .decode(&all)
            .map_err(|e| anyhow::anyhow!(e.to_string()))?;
        let text = String::from_utf8_lossy(&decoded).to_string();
        Ok(text)
    }

    pub async fn generate_tts_tokens(
        &mut self,
        text: &str,
        property_tokens: &[i32],
        _ref_global_tokens: Option<&[i32]>,
        _ref_semantic_tokens: Option<&[i32]>,
        voice_id: Option<&str>,
        args: &SamplerArgs,
    ) -> Result<(Vec<i32>, Vec<i32>)> {
        // ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
        let _request_id = self.generate_request_id();

        // å¼€å§‹TTSç”Ÿæˆ

        // è‹¥æä¾›äº†ç§å­ï¼Œè®¾ç½®ç¡®å®šæ€§é‡‡æ ·
        self.set_seed(args.seed);

        // å…³é”®ä¿®å¤ï¼šä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºå®Œå…¨ç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
        // è¿™ç¡®ä¿äº†ä¸åŒè¯·æ±‚ä¹‹é—´çš„çŠ¶æ€å®Œå…¨éš”ç¦»
        // åˆ›å»ºç‹¬ç«‹æ¨ç†ä¸Šä¸‹æ–‡

        // ç¼–ç æ–‡æœ¬ï¼šä½¿ç”¨åŸå§‹æ–‡æœ¬tokenï¼ˆä¸åŠ ä»»ä½•åç§»ï¼‰ä»¥åŒ¹é…å‚è€ƒå®ç°
        // ç¼–ç è¾“å…¥æ–‡æœ¬
        let text_tokens_u32: Vec<u32> = self
            .tokenizer
            .encode(text.as_bytes())
            .map_err(|e| anyhow::anyhow!(e.to_string()))?;
        let text_tokens: Vec<i32> = text_tokens_u32.into_iter().map(|t| t as i32).collect();
        // æ–‡æœ¬ç¼–ç å®Œæˆ

        // å‚è€ƒå®ç°åœ¨prefillé˜¶æ®µå–‚å…¥å±æ€§tokensï¼ˆåŸå§‹åŸŸï¼‰ã€æ–‡æœ¬tokensä¸é˜¶æ®µæ ‡ç­¾ã€‚
        let mut input_tokens: Vec<i32> = Vec::new();
        input_tokens.extend_from_slice(property_tokens);
        input_tokens.push(TTS_TAG_2);
        input_tokens.extend_from_slice(&text_tokens);
        input_tokens.push(TTS_TAG_0);
        // æ„å»ºå®Œæ•´è¾“å…¥åºåˆ—

        // === Prefill é˜¶æ®µ ===
        let input_tokens_u32: Vec<u32> = input_tokens.iter().map(|&t| t as u32).collect();

        // Prefillé˜¶æ®µ - åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡

        // å…³é”®ä¿®å¤ï¼šä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºå®Œå…¨ç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
        // ä½¿ç”¨å›ºå®šçš„batchç´¢å¼•0ï¼Œä½†ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„æ¨ç†çŠ¶æ€
        // è¿™é¿å…äº†ä¸åŒè¯·æ±‚ä¹‹é—´çš„çŠ¶æ€æ±¡æŸ“é—®é¢˜
        // åˆ›å»ºç‹¬ç«‹æ¨ç†ä¸Šä¸‹æ–‡
        let batch = RnnInputBatch::new(input_tokens_u32.clone(), RnnOption::Last);
        let mut inference = RnnInput::new(vec![batch], self.token_chunk_size);

        // é‡è¦ï¼šç¡®ä¿æ¨ç†ä¸Šä¸‹æ–‡å®Œå…¨ç‹¬ç«‹ï¼Œä¸å—ä¹‹å‰è¯·æ±‚å½±å“
        // æ¨ç†ä¸Šä¸‹æ–‡å·²éš”ç¦»
        // å…³é”®ä¿®å¤ï¼šç›´æ¥æ‰§è¡ŒPrefillæ¨ç†ï¼Œä¸ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
        // è¿™ç¡®ä¿ç¬¬ä¸€ä¸ªlogitä¸ä¼šè¢«è·³è¿‡æˆ–ä¸¢å¤±
        let mut last_logits: Vec<f32> = loop {
            let (next_inference, output) = self.runtime.infer(inference).await?;
            inference = next_inference;
            if output[0].0.size() > 0 {
                break output[0].0.clone().to_vec();
            }
        };

        // === Global é˜¶æ®µ ===
        let mut global_tokens: Vec<i32> = Vec::new();
        let mut semantic_tokens: Vec<i32> = Vec::new();

        // ä¼˜å…ˆå°è¯•ä»voice_idç¼“å­˜è·å–tokens
        if let Some(voice_id) = voice_id {
            // åˆ›å»ºVoiceFeatureManagerå®ä¾‹ï¼ˆå‡è®¾ä½¿ç”¨é»˜è®¤RAFç›®å½•ï¼‰
            if let Ok(voice_manager) = VoiceFeatureManager::new("./raf") {
                if let Ok((cached_global, cached_semantic)) =
                    voice_manager.get_voice_tokens(voice_id).await
                {
                    return Ok((cached_global, cached_semantic));
                }
            }
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰é¢„æå–çš„éŸ³è‰²ç‰¹å¾
        let has_ref_audio = _ref_global_tokens.is_some() || _ref_semantic_tokens.is_some();

        // å¦‚æœæœ‰é¢„æå–çš„éŸ³è‰²ç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨å®ƒä»¬
        if has_ref_audio {
            if let Some(ref_global) = _ref_global_tokens {
                global_tokens = ref_global.to_vec();
                // ä½¿ç”¨é¢„æå–çš„global tokens
            }
            if let Some(ref_semantic) = _ref_semantic_tokens {
                semantic_tokens = ref_semantic.to_vec();
                // ä½¿ç”¨é¢„æå–çš„semantic tokens
            }

            // å£°éŸ³å…‹éš†æ¨¡å¼ï¼šä½¿ç”¨é¢„æå–ç‰¹å¾

            return Ok((global_tokens, semantic_tokens));
        }

        // å¦‚æœæ²¡æœ‰é¢„æå–ç‰¹å¾ï¼Œåˆ™è¿›è¡Œæ­£å¸¸çš„ç”Ÿæˆæµç¨‹
        // è®¾ç½®åˆ†å±‚é‡‡æ ·å‚æ•°å’Œç‹¬ç«‹RNG
        let mut args_global = if args.layered_randomness.use_independent_seeds {
            self.apply_voice_fidelity_adjustment(args, args.layered_randomness.global_randomness)
        } else {
            args.clone()
        };

        let mut args_sem = if args.layered_randomness.use_independent_seeds {
            self.apply_voice_fidelity_adjustment(args, args.layered_randomness.semantic_randomness)
        } else {
            args.clone()
        };

        // è®¾ç½®é»˜è®¤top_kå€¼
        if args_global.top_k == 0 {
            args_global.top_k = 20;
        }
        if args_sem.top_k == 0 {
            args_sem.top_k = 80;
        }

        // å£°éŸ³å…‹éš†æ—¶ä½¿ç”¨ç¡®å®šæ€§å‚æ•°
        if has_ref_audio {
            // å£°éŸ³å…‹éš†æ¨¡å¼ï¼šä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·å‚æ•°ç¡®ä¿ç»“æœä¸€è‡´æ€§
            // å£°éŸ³å…‹éš†æ—¶ä½¿ç”¨å›ºå®šçš„ç¡®å®šæ€§å‚æ•°
            args_global.temperature = 0.1; // æä½æ¸©åº¦ç¡®ä¿ç¡®å®šæ€§
            args_global.top_p = 0.9;
            args_global.top_k = 1; // åªé€‰æ‹©æœ€å¯èƒ½çš„token

            args_sem.temperature = 0.1; // æä½æ¸©åº¦ç¡®ä¿ç¡®å®šæ€§
            args_sem.top_p = 0.9;
            args_sem.top_k = 1; // åªé€‰æ‹©æœ€å¯èƒ½çš„token
        } else {
            // æ­£å¸¸ç”Ÿæˆæ¨¡å¼ï¼šä½¿ç”¨ä¸Pythonç‰ˆæœ¬ä¸€è‡´çš„å›ºå®šå‚æ•°
            // Pythonç‰ˆæœ¬: Globalé˜¶æ®µ(temperature=1.0, top_p=0.95, top_k=20)
            args_global.temperature = 1.0;
            args_global.top_p = 0.95;
            args_global.top_k = 20;

            // Pythonç‰ˆæœ¬: Semanticé˜¶æ®µ(temperature=1.0, top_p=0.95, top_k=80)
            args_sem.temperature = 1.0;
            args_sem.top_p = 0.95;
            args_sem.top_k = 80;
        }

        // ç®€åŒ–RNGç®¡ç†ï¼Œå‚è€ƒPythonç‰ˆæœ¬ä½¿ç”¨ç»Ÿä¸€çš„éšæœºçŠ¶æ€
        // å£°éŸ³å…‹éš†æ—¶ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·ï¼ˆtemperature=0ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å½“å‰RNGçŠ¶æ€
        let use_deterministic = has_ref_audio;

        // å¦‚æœæ˜¯å£°éŸ³å…‹éš†ï¼Œä¸´æ—¶è°ƒæ•´é‡‡æ ·å‚æ•°ä¸ºç¡®å®šæ€§
        if use_deterministic {
            args_global.temperature = 0.01; // æ¥è¿‘ç¡®å®šæ€§
            args_sem.temperature = 0.01;
        }

        // Pythonå®ç°å›ºå®šç”Ÿæˆ32ä¸ªglobal tokensï¼Œå¹¶ä¸”ä»…åœ¨å‰4096ç»´å†…é‡‡æ ·
        let global_tokens_size: usize = 32;

        // ä½¿ç”¨æ‰¹é‡æ¨ç†ä¼˜åŒ–Globalé˜¶æ®µ

        // æ‰“å°Globalé˜¶æ®µé‡‡æ ·å‚æ•°
        log::info!("ğŸ¯ å¼€å§‹ç”ŸæˆGlobal tokensï¼Œç›®æ ‡æ•°é‡: {}", global_tokens_size);
        log::info!("ğŸ“‹ Globalé˜¶æ®µé‡‡æ ·å‚æ•°:");
        log::info!(
            "   - é»˜è®¤å‚æ•°: temperature={:.3}, top_p={:.3}, top_k={}",
            args.temperature,
            args.top_p,
            args.top_k
        );
        log::info!(
            "   - å®é™…å‚æ•°: temperature={:.3}, top_p={:.3}, top_k={}",
            args_global.temperature,
            args_global.top_p,
            args_global.top_k
        );
        if has_ref_audio {
            log::info!("   - æ¨¡å¼: å£°éŸ³å…‹éš† (ç¡®å®šæ€§é‡‡æ ·)");
        } else {
            log::info!("   - æ¨¡å¼: æ­£å¸¸ç”Ÿæˆ (éšæœºé‡‡æ ·)");
        }
        for (i, _) in (0..global_tokens_size).enumerate() {
            // å…³é”®ä¿®å¤ï¼šç¡®ä¿ç¬¬ä¸€ä¸ªtokenä½¿ç”¨Prefillé˜¶æ®µçš„æ­£ç¡®logits
            let logits: &[f32] = if i == 0 {
                // ç¬¬ä¸€ä¸ªtokenå¿…é¡»ä½¿ç”¨Prefillé˜¶æ®µçš„logits
                &last_logits
            } else {
                // åç»­tokené€šè¿‡æ¨ç†è·å–
                let (next_inference, output) = self.runtime.infer(inference).await?;
                inference = next_inference;
                if output[0].0.size() > 0 {
                    last_logits = output[0].0.clone().to_vec();
                    &last_logits
                } else {
                    // å¦‚æœæ²¡æœ‰è¾“å‡ºï¼Œç»§ç»­ä½¿ç”¨ä¹‹å‰çš„logits
                    &last_logits
                }
            };

            // ä»…åœ¨[0..4096)èŒƒå›´å†…é‡‡æ ·ï¼ˆGlobalç›¸å¯¹åŸŸï¼‰ï¼Œä¸æ¶‰åŠEOSä¸é˜¶æ®µæ ‡ç­¾
            let vocab_global = if logits.len() < 4096 {
                logits.len()
            } else {
                4096
            };
            // Globalé˜¶æ®µé‡‡æ · - ä½¿ç”¨ç®€åŒ–çš„é‡‡æ ·æ–¹æ³•
            let next_id = self.sample_logits(&logits[..vocab_global], &args_global, None);

            // è¿½åŠ åˆ°globalè¾“å‡ºï¼ˆç›¸å¯¹åŸŸ [0..4095]ï¼‰
            global_tokens.push(next_id as i32);
            // åé¦ˆåˆ°æ¨¡å‹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹IDï¼ˆä¸C++ä»£ç ä¸€è‡´ï¼‰
            inference.batches[0].push(next_id as u32);

            // æ‰“å°å½“å‰ç”Ÿæˆè¿›åº¦
            if (i + 1) % 8 == 0 || i == global_tokens_size - 1 {
                println!(
                    "ğŸ“Š Globalé˜¶æ®µ: å·²ç”Ÿæˆ {}/{} tokens",
                    i + 1,
                    global_tokens_size
                );
            }
            // Global tokenç”Ÿæˆ
        }

        // === åˆ‡æ¢åˆ° Semantic é˜¶æ®µ ===
        inference.batches[0].push(TTS_TAG_1 as u32);
        // è®©æ ‡ç­¾ç”Ÿæ•ˆï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºï¼Œå¹¶ä¿ç•™logitsä¾›é¦–æ­¥ä½¿ç”¨
        let mut last_sem_logits: Vec<f32> = loop {
            let (next_inference, output) = self.runtime.infer(inference).await?;
            inference = next_inference;
            if output[0].0.size() > 0 {
                break output[0].0.clone().to_vec();
            }
        };

        // è¯­ä¹‰é˜¶æ®µï¼šé™åˆ¶æœ€å¤§ç”Ÿæˆæ­¥æ•°ä¸º2048
        let semantic_limit: usize = usize::min(args.max_tokens, 2048);

        // ä½¿ç”¨æ‰¹é‡æ¨ç†ä¼˜åŒ–Semanticé˜¶æ®µ
        let semantic_context_id = format!("tts_semantic_{}", self.generate_request_id());
        let mut semantic_logits_cache: Vec<Vec<f32>> = Vec::new();
        let mut semantic_cache_index = 0;

        // æ‰“å°Semanticé˜¶æ®µé‡‡æ ·å‚æ•°
        log::info!("ğŸ¯ å¼€å§‹ç”ŸæˆSemantic tokensï¼Œæœ€å¤§æ•°é‡: {}", semantic_limit);
        log::info!("ğŸ“‹ Semanticé˜¶æ®µé‡‡æ ·å‚æ•°:");
        log::info!(
            "   - é»˜è®¤å‚æ•°: temperature={:.3}, top_p={:.3}, top_k={}",
            args.temperature,
            args.top_p,
            args.top_k
        );
        log::info!(
            "   - å®é™…å‚æ•°: temperature={:.3}, top_p={:.3}, top_k={}",
            args_sem.temperature,
            args_sem.top_p,
            args_sem.top_k
        );
        if has_ref_audio {
            log::info!("   - æ¨¡å¼: å£°éŸ³å…‹éš† (ç¡®å®šæ€§é‡‡æ ·)");
        } else {
            log::info!("   - æ¨¡å¼: æ­£å¸¸ç”Ÿæˆ (éšæœºé‡‡æ ·)");
        }
        for (i, _) in (0..semantic_limit).enumerate() {
            // å–å¾—å½“å‰è¯­ä¹‰é˜¶æ®µçš„logitsï¼šé¦–æ­¥ä½¿ç”¨æ³¨å…¥æ ‡ç­¾åçš„logitsï¼Œå…¶åæ¯æ­¥ä»runtimeè·å–
            let logits: &[f32] = if i == 0 {
                &last_sem_logits
            } else if semantic_cache_index < semantic_logits_cache.len() {
                // ä½¿ç”¨ç¼“å­˜çš„logits
                &semantic_logits_cache[semantic_cache_index]
            } else {
                // éœ€è¦æ‰¹é‡è·å–æ›´å¤šlogits
                let remaining_tokens = semantic_limit - i;
                let batch_size = remaining_tokens.min(16); // æ‰¹é‡æ¨ç†16ä¸ªtoken

                let (next_inference, batch_logits) = self
                    .inference_state_manager
                    .smart_inference(
                        &mut self.runtime,
                        inference,
                        &semantic_context_id,
                        batch_size,
                    )
                    .await?;
                inference = next_inference;

                if !batch_logits.is_empty() {
                    semantic_logits_cache.extend(batch_logits);
                    semantic_cache_index = 0;
                    &semantic_logits_cache[semantic_cache_index]
                } else {
                    // å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
                    loop {
                        let (next_inference, output) = self.runtime.infer(inference).await?;
                        inference = next_inference;
                        if output[0].0.size() > 0 {
                            // é‡ç”¨å˜é‡ï¼Œé¿å…é‡å¤åˆ†é…
                            last_sem_logits = output[0].0.clone().to_vec();
                            break &last_sem_logits;
                        }
                    }
                }
            };

            // è¯­ä¹‰é˜¶æ®µä»…é‡‡æ · [0..8192]ï¼ˆåŒ…å«EOSï¼‰ï¼Œå±è”½TTS_TAG_*ä¸å…¶å®ƒåŸŸ
            // ä½¿ç”¨æ ˆåˆ†é…çš„ç¼“å†²åŒºï¼Œé¿å…å †åˆ†é…
            let mut logits_buf = [f32::NEG_INFINITY; 8192];
            let copy_len = logits.len().min(8192);
            logits_buf[..copy_len].copy_from_slice(&logits[..copy_len]);

            // å±è”½è¶…å‡ºEOSçš„token
            for item in logits_buf
                .iter_mut()
                .take(copy_len)
                .skip(TTS_EOS_TOKEN as usize + 1)
            {
                *item = f32::NEG_INFINITY;
            }

            // å±è”½TTSæ ‡ç­¾
            for tag in [TTS_TAG_0, TTS_TAG_1, TTS_TAG_2] {
                let idx = tag as usize;
                if idx < copy_len {
                    logits_buf[idx] = f32::NEG_INFINITY;
                }
            }

            let next_id = self.sample_logits(&logits_buf[..copy_len], &args_sem, None);

            // è¿½åŠ åˆ°semanticè¾“å‡ºï¼ˆåŸå§‹åŸŸ [0..8191]ï¼‰
            semantic_tokens.push(next_id as i32);
            // è¯­ä¹‰é˜¶æ®µåé¦ˆï¼šç›´æ¥åé¦ˆåŸå§‹idï¼ˆç»éªŒï¼‰
            inference.batches[0].push(next_id as u32);

            // æ‰“å°å½“å‰ç”Ÿæˆè¿›åº¦
            if (i + 1) % 16 == 0 || i == semantic_limit - 1 {
                println!(
                    "ğŸ“Š Semanticé˜¶æ®µ: å·²ç”Ÿæˆ {}/{} tokens",
                    i + 1,
                    semantic_limit
                );
            }

            // è¯­ä¹‰tokenç”Ÿæˆå®Œæˆ
            // Semantic tokenç”Ÿæˆ
        }

        // TTSç”Ÿæˆå®Œæˆ
        println!(
            "âœ… TTSç”Ÿæˆå®Œæˆ - Global tokens: {}, Semantic tokens: {}",
            global_tokens.len(),
            semantic_tokens.len()
        );
        Ok((global_tokens, semantic_tokens))
    }

    /// æ‰¹å¤„ç†ç”ŸæˆTTS tokens - å®Œå…¨ç‹¬ç«‹çš„ä¸²è¡Œå¤„ç†
    /// æ¯ä¸ªè¯·æ±‚éƒ½æœ‰ç‹¬ç«‹çš„æ¨ç†çŠ¶æ€ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
    pub async fn generate_tts_tokens_batch(
        &mut self,
        requests: Vec<TtsBatchRequest>,
    ) -> Result<Vec<(Vec<i32>, Vec<i32>)>> {
        if requests.is_empty() {
            return Ok(vec![]);
        }

        let batch_size = requests.len();
        // æ‰¹å¤„ç†ç”ŸæˆTTS tokens

        // æ‰¹å¤„ç†å¼€å§‹å‰è¿›è¡Œå…¨å±€çŠ¶æ€é‡ç½®
        self.reset();
        // æ‰¹å¤„ç†å‰å·²é‡ç½®å…¨å±€çŠ¶æ€

        // å®Œå…¨ç‹¬ç«‹çš„ä¸²è¡Œå¤„ç†ï¼šæ¯ä¸ªè¯·æ±‚éƒ½æœ‰ç‹¬ç«‹çŠ¶æ€ï¼Œç¡®ä¿æ— æ±¡æŸ“
        let mut results = Vec::with_capacity(batch_size);
        for request in requests.into_iter() {
            // å¤„ç†ç‹¬ç«‹è¯·æ±‚

            // å…³é”®ä¿®å¤ï¼šæ¯ä¸ªè¯·æ±‚å‰è¿›è¡Œå½»åº•çš„çŠ¶æ€é‡ç½®
            self.reset();

            // ç»Ÿä¸€å¤„ç†ç§å­è®¾ç½®ï¼Œä¸åŒºåˆ†å£°éŸ³å…‹éš†åœºæ™¯
            if let Some(seed) = request.args.seed {
                self.set_seed(Some(seed));
                // è®¾ç½®ç¡®å®šæ€§ç§å­
            } else {
                self.set_seed(None); // é‡ç½®ä¸ºéç¡®å®šæ€§æ¨¡å¼
                                     // ä½¿ç”¨éç¡®å®šæ€§é‡‡æ ·
            }

            let result = self
                .generate_tts_tokens(
                    &request.text,
                    &request.property_tokens,
                    request.ref_global_tokens.as_deref(),
                    request.ref_semantic_tokens.as_deref(),
                    request.voice_id.as_deref(),
                    &request.args,
                )
                .await?;
            results.push(result);

            // æ¯ä¸ªè¯·æ±‚å®Œæˆåè¿›è¡Œå½»åº•çš„çŠ¶æ€æ¸…ç†
            self.reset();
            // è¯·æ±‚å®Œæˆï¼ŒçŠ¶æ€å·²æ¸…ç†
        }

        // æ‰¹å¤„ç†å®Œæˆåè¿›è¡Œæœ€ç»ˆçŠ¶æ€é‡ç½®
        self.reset();
        // æ‰¹å¤„ç†å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆç‹¬ç«‹ç»“æœï¼Œæœ€ç»ˆçŠ¶æ€å·²é‡ç½®
        Ok(results)
    }

    /// é‡ç½®é‡‡æ ·å™¨çŠ¶æ€ - å½»åº•æ¸…ç†æ‰€æœ‰çŠ¶æ€
    pub fn reset(&mut self) {
        // é‡ç½®éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
        self.rng = None;

        // é‡ç½®batchè®¡æ•°å™¨ï¼Œé¿å…ç´¢å¼•ç´¯ç§¯
        self.batch_counter.store(0, Ordering::SeqCst);

        // å…³é”®ä¿®å¤ï¼šå°è¯•æ¸…ç†Runtimeçš„å†…éƒ¨çŠ¶æ€
        // è™½ç„¶æˆ‘ä»¬ä¸èƒ½ç›´æ¥é‡ç½®Runtimeï¼Œä½†å¯ä»¥ç¡®ä¿ä¸‹æ¬¡ä½¿ç”¨æ—¶çŠ¶æ€æ˜¯å¹²å‡€çš„
        // é€šè¿‡é‡ç½®batchç´¢å¼•ï¼Œç¡®ä¿ä½¿ç”¨ä¸åŒçš„æ¨ç†ä¸Šä¸‹æ–‡

        // é‡‡æ ·å™¨çŠ¶æ€å·²å½»åº•é‡ç½® (RNG + batchç´¢å¼•)
    }

    /// é‡‡æ ·å‡½æ•° - ä½¿ç”¨ä¸Pythonç‰ˆæœ¬ä¸€è‡´çš„sample_logitsé€»è¾‘
    /// forbid_token: å¯é€‰ç¦æ­¢é‡‡æ ·çš„tokenï¼ˆå¦‚æŸäº›é˜¶æ®µçš„ç‰¹æ®Šç¬¦å·ï¼‰
    pub fn sample_logits(
        &mut self,
        logits: &[f32],
        args: &SamplerArgs,
        forbid_token: Option<usize>,
    ) -> usize {
        // ä½¿ç”¨ä¸Pythonç‰ˆæœ¬ä¸€è‡´çš„sample_logitsé€»è¾‘
        sample_logits_with_top_p_k(
            logits,
            args.temperature,
            args.top_p,
            args.top_k,
            forbid_token,
            &mut self.rng,
        )
    }
}
