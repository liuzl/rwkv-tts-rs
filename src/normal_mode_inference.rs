use anyhow::Result;
use rand::rngs::StdRng;
use rand::SeedableRng;
use tracing::{debug, info, warn};
use web_rwkv::runtime::infer::{RnnInput, RnnInputBatch, RnnOption};

use crate::shared_runtime::TtsInferContext;

/// æ‰§è¡Œæ™®é€šæ¨¡å¼æ¨ç†
pub async fn execute_normal_inference(
    infer_context: &TtsInferContext,
    request: crate::rwkv_sampler::TtsBatchRequest,
    text_tokens: Vec<i32>,
) -> Result<(Vec<i32>, Vec<i32>)> {
    let request_id = &infer_context.request_id;
    info!(
        "ğŸš€ [{}] å¼€å§‹æ™®é€šæ¨¡å¼æ¨ç† - æ–‡æœ¬: '{}'",
        request_id, request.text
    );

    // ä¸ºæœ¬æ¬¡è¯·æ±‚åˆ›å»ºç‹¬ç«‹RNGï¼ˆå¯å¤ç°ä¸”äº’ä¸å¹²æ‰°ï¼‰
    // æ™®é€šæ¨¡å¼ä¸æ˜¯å£°éŸ³å…‹éš†ï¼Œä½¿ç”¨æ­£å¸¸çš„éšæœºæ•°ç”Ÿæˆé€»è¾‘
    let rng: rand::rngs::StdRng = if let Some(seed) = request.args.seed {
        rand::rngs::StdRng::seed_from_u64(seed)
    } else {
        rand::rngs::StdRng::from_rng(rand::thread_rng()).expect("failed to seed StdRng")
    };

    // è·å–tokenizerå’Œruntime
    let runtime = &infer_context.runtime;
    let state = &infer_context.state;

    // æ„å»ºè¾“å…¥åºåˆ—ï¼šå±æ€§tokens + TTS_TAG_2 + æ–‡æœ¬tokens + TTS_TAG_0
    let mut input_tokens: Vec<i32> = Vec::new();
    input_tokens.extend_from_slice(&request.property_tokens);
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_2);
    input_tokens.extend_from_slice(&text_tokens);
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_0);

    debug!(
        "ğŸ” [{}] å®Œæ•´è¾“å…¥åºåˆ—: {:?} (é•¿åº¦: {})",
        request_id,
        input_tokens,
        input_tokens.len()
    );

    // === Prefill é˜¶æ®µ ===
    let input_tokens_u32: Vec<u32> = input_tokens.iter().map(|&t| t as u32).collect();
    let token_chunk_size = request.args.token_chunk_size;

    info!("ğŸ”§ [{}] Prefillé˜¶æ®µ - åˆå§‹åŒ–ç‹¬ç«‹çŠ¶æ€", request_id);

    // åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
    let batch = RnnInputBatch::new(input_tokens_u32.clone(), RnnOption::Last);
    let mut inference = RnnInput::new(vec![batch], token_chunk_size);

    // ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€ï¼Œç¡®ä¿çŠ¶æ€éš”ç¦»
    {
        let state_guard = state.lock().await;
        let initial_state = state_guard.init();
        state_guard.load(initial_state, 0)?;
        info!("ğŸ”§ [{}] å·²ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€", request_id);
    }

    // æ¶ˆåŒ–è¾“å…¥ç›´åˆ°äº§ç”Ÿè¾“å‡º
    let last_logits: Vec<f32> = loop {
        let (remaining_input, output) = runtime.infer(inference.clone()).await?;
        inference = remaining_input;
        if !output.is_empty() && output[0].0.size() > 0 {
            break output[0].0.clone().to_vec();
        }
    };

    // === Global é˜¶æ®µ ===
    let mut global_tokens: Vec<i32> = Vec::new();
    let mut semantic_tokens: Vec<i32> = Vec::new();

    // æ™®é€šæ¨¡å¼è¿›è¡Œæ­£å¸¸çš„ç”Ÿæˆæµç¨‹ï¼ˆä¸ä½¿ç”¨é¢„æå–ç‰¹å¾ï¼‰
    // è®¾ç½®é‡‡æ ·å‚æ•°ï¼Œä¼˜åŒ–EOSç”Ÿæˆæ¦‚ç‡
    let sampler_args = &request.args;

    let mut args_global = sampler_args.clone();
    let mut args_semantic = sampler_args.clone();

    // ä¼˜åŒ–globalé˜¶æ®µå‚æ•°
    if args_global.top_k == 0 {
        args_global.top_k = 20;
    }

    // Semanticé˜¶æ®µä½¿ç”¨å›ºå®šå‚æ•°ï¼Œä¸Pythonä»£ç ä¿æŒä¸¥æ ¼ä¸€è‡´
    args_semantic.temperature = 1.0;
    args_semantic.top_p = 0.95;
    args_semantic.top_k = 80;

    // åˆ›å»ºç‹¬ç«‹çš„RNGç”¨äºä¸åŒé˜¶æ®µ
    let mut global_rng = if sampler_args.layered_randomness.use_independent_seeds {
        if let Some(seed) = sampler_args.seed {
            // ç”¨æˆ·æä¾›äº†seedï¼Œä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
            Some(StdRng::seed_from_u64(seed.wrapping_add(
                sampler_args.layered_randomness.global_seed_offset,
            )))
        } else {
            // æ²¡æœ‰seedï¼Œåˆ›å»ºéšæœºRNG
            Some(StdRng::from_entropy())
        }
    } else {
        Some(rng.clone())
    };

    let mut semantic_rng = if sampler_args.layered_randomness.use_independent_seeds {
        if let Some(seed) = sampler_args.seed {
            // ç”¨æˆ·æä¾›äº†seedï¼Œä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
            Some(StdRng::seed_from_u64(seed.wrapping_add(
                sampler_args.layered_randomness.semantic_seed_offset,
            )))
        } else {
            // æ²¡æœ‰seedï¼Œåˆ›å»ºéšæœºRNG
            Some(StdRng::from_entropy())
        }
    } else {
        Some(rng.clone())
    };

    // æ·»åŠ RNGçŠ¶æ€è°ƒè¯•æ—¥å¿—
    info!(
        "ğŸ” [{}] RNGçŠ¶æ€: seed={:?}, use_independent_seeds={}, global_rng=Some, semantic_rng=Some",
        request_id, sampler_args.seed, sampler_args.layered_randomness.use_independent_seeds
    );

    // åº”ç”¨éŸ³è‰²ä¿çœŸåº¦è°ƒæ•´
    let global_fidelity_factor = sampler_args.voice_fidelity;
    let global_randomness_factor = sampler_args.layered_randomness.global_randomness;
    let global_conservative_factor = global_fidelity_factor * (1.0 - global_randomness_factor);

    // Globalé˜¶æ®µé‡‡ç”¨æ›´ä¿å®ˆçš„å‚æ•°è°ƒæ•´
    args_global.temperature *= (0.3 + 0.7 * (1.0 - global_conservative_factor)).max(0.1);
    args_global.top_p = (args_global.top_p * (0.8 + 0.2 * global_conservative_factor)).max(0.2);
    args_global.top_k =
        ((args_global.top_k as f32) * (0.9 + 0.1 * global_conservative_factor)).max(5.0) as usize;

    // Semanticé˜¶æ®µä½¿ç”¨å›ºå®šå‚æ•°ï¼Œé¿å…é‡å¤å¾ªç¯
    info!(
        "ğŸ” [{}] Semanticé˜¶æ®µé‡‡æ ·å‚æ•°: temperature={:.2}, top_p={:.2}, top_k={} (å›ºå®šå‚æ•°ï¼Œä¸Pythonä¸€è‡´)",
        request_id, args_semantic.temperature, args_semantic.top_p, args_semantic.top_k
    );

    // ç”Ÿæˆ32ä¸ªglobal tokens - å¢å¼ºå‚è€ƒéŸ³é¢‘ç‰¹å¾æƒé‡
    let global_tokens_size: usize = 32;
    info!(
        "ğŸ” [{}] å¼€å§‹ç”Ÿæˆ {} ä¸ªglobal tokens",
        request_id, global_tokens_size
    );
    info!(
        "ğŸ” [{}] Globalé˜¶æ®µé‡‡æ ·å‚æ•°: temperature={:.2}, top_p={:.2}, top_k={}",
        request_id, args_global.temperature, args_global.top_p, args_global.top_k
    );

    for i in 0..global_tokens_size {
        let logits: Vec<f32> = if i == 0 {
            last_logits.clone()
        } else {
            // ç»§ç»­æ¨ç†è·å–logits - ä½¿ç”¨ç°æœ‰inferenceä¸Šä¸‹æ–‡
            loop {
                let (next_inference, output) = runtime.infer(inference.clone()).await?;
                inference = next_inference;
                if output[0].0.size() > 0 {
                    break output[0].0.clone().to_vec();
                }
            }
        };

        // ä»…åœ¨[0..4096)èŒƒå›´å†…é‡‡æ ·
        let vocab_global = if logits.len() < 4096 {
            logits.len()
        } else {
            4096
        };

        // ç›´æ¥ä½¿ç”¨åŸå§‹logitsï¼Œä¸è¿›è¡Œå¢å¼ºå¤„ç†
        let sampling_logits = logits[..vocab_global].to_vec();

        let next_id = crate::rwkv_sampler::sample_logits(
            &sampling_logits,
            &args_global,
            None,
            &mut global_rng,
        );

        // å®‰å…¨è½¬æ¢ï¼šç¡®ä¿tokenåœ¨æœ‰æ•ˆèŒƒå›´å†…
        if next_id > i32::MAX as usize {
            warn!(
                "ğŸš¨ [{}] Global token {} è¶…å‡ºi32èŒƒå›´ï¼Œè·³è¿‡æ­¤token",
                request_id, next_id
            );
            continue;
        }

        // é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿tokenåœ¨globalèŒƒå›´å†… [0..4096)
        if next_id >= 4096 {
            warn!(
                "ğŸš¨ [{}] Global token {} è¶…å‡ºèŒƒå›´[0..4096)ï¼Œè·³è¿‡æ­¤token",
                request_id, next_id
            );
            continue;
        }

        global_tokens.push(next_id as i32);

        // åé¦ˆåˆ°æ¨¡å‹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹IDï¼ˆä¸C++ä»£ç ä¸€è‡´ï¼‰
        inference.batches[0].push(next_id as u32);

        #[cfg(debug_assertions)]
        debug!(
            "ğŸ” [{}] Global token {}: é‡‡æ ·={}, åé¦ˆ={}",
            request_id, i, next_id, next_id
        );
    }

    info!(
        "âœ… [{}] Global tokensç”Ÿæˆå®Œæˆ: {:?} (å…±{}ä¸ª)",
        request_id,
        global_tokens,
        global_tokens.len()
    );

    // === åˆ‡æ¢åˆ° Semantic é˜¶æ®µ ===
    inference.batches[0].push(crate::rwkv_sampler::TTS_TAG_1 as u32);
    info!(
        "ğŸ” [{}] åˆ‡æ¢åˆ°Semanticé˜¶æ®µï¼Œæ¨å…¥TTS_TAG_1={}",
        request_id,
        crate::rwkv_sampler::TTS_TAG_1
    );

    // è®©æ ‡ç­¾ç”Ÿæ•ˆï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºï¼Œå¹¶ä¿ç•™logitsä¾›é¦–æ­¥ä½¿ç”¨
    let last_sem_logits: Vec<f32> = loop {
        let (next_inference, output) = runtime.infer(inference).await?;
        inference = next_inference;
        if output[0].0.size() > 0 {
            break output[0].0.clone().to_vec();
        }
    };

    // è¯­ä¹‰é˜¶æ®µï¼šé™åˆ¶æœ€å¤§ç”Ÿæˆæ­¥æ•°ä¸º2048
    let semantic_limit: usize = usize::min(request.args.max_tokens, 2048);
    info!(
        "ğŸ” [{}] å¼€å§‹ç”Ÿæˆsemantic tokensï¼Œæœ€å¤§é™åˆ¶: {}",
        request_id, semantic_limit
    );

    for i in 0..semantic_limit {
        let logits: Vec<f32> = if i == 0 {
            last_sem_logits.clone()
        } else {
            loop {
                let (next_inference, output) = runtime.infer(inference.clone()).await?;
                inference = next_inference;
                if output[0].0.size() > 0 {
                    break output[0].0.clone().to_vec();
                }
            }
        };

        // è¯­ä¹‰é˜¶æ®µä»…é‡‡æ · [0..8192]ï¼ˆåŒ…å«EOSï¼‰ï¼Œå±è”½TTS_TAG_*ä¸å…¶å®ƒåŸŸ
        let mut logits_masked = logits.clone();
        // ä¿®å¤ï¼šä¸å±è”½EOS tokenï¼Œåªå±è”½å¤§äºEOS tokençš„éƒ¨åˆ†
        for (j, v) in logits_masked.iter_mut().enumerate() {
            if j > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
                *v = f32::NEG_INFINITY;
            }
        }
        // å±è”½TTS_TAG tokensï¼Œä½†ä¿ç•™EOS token
        for tag in [
            crate::rwkv_sampler::TTS_TAG_0,
            crate::rwkv_sampler::TTS_TAG_1,
            crate::rwkv_sampler::TTS_TAG_2,
        ] {
            let idx = tag as usize;
            if idx < logits_masked.len() {
                logits_masked[idx] = f32::NEG_INFINITY;
            }
        }

        // æ³¨æ„ï¼šä¸å±è”½EOS tokenï¼Œè®©å®ƒèƒ½å¤Ÿè¢«æ­£å¸¸é‡‡æ ·ä»¥ç»ˆæ­¢ç”Ÿæˆ

        // æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºEOS tokençš„logitså€¼
        let eos_logit = if (crate::rwkv_sampler::TTS_EOS_TOKEN as usize) < logits_masked.len() {
            logits_masked[crate::rwkv_sampler::TTS_EOS_TOKEN as usize]
        } else {
            f32::NEG_INFINITY
        };


        // ç›´æ¥ä½¿ç”¨å±è”½åçš„logitsè¿›è¡Œé‡‡æ ·
        let next_id = crate::rwkv_sampler::sample_logits(
            &logits_masked,
            &args_semantic,
            None,
            &mut semantic_rng,
        );


        // æ£€æŸ¥æ˜¯å¦é‡åˆ°EOS tokenï¼ˆå¿…é¡»åœ¨èŒƒå›´æ£€æŸ¥ä¹‹å‰ï¼‰
        if next_id == crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
            info!(
                "ğŸ” [{}] æ­£å¸¸æ¨¡å¼é‡åˆ°EOS token({}), åœæ­¢ç”Ÿæˆ",
                request_id, next_id
            );
            break;
        }


        // é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿tokenåœ¨semanticèŒƒå›´å†… [0..8192)ï¼ˆä¿®å¤ï¼šåº”è¯¥æ˜¯>8192è€Œä¸æ˜¯>=8192ï¼‰
        if next_id > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
            warn!(
                "ğŸš¨ [{}] Token {} è¶…å‡ºsemanticèŒƒå›´[0..8192]ï¼Œè·³è¿‡æ­¤token",
                request_id, next_id
            );
            continue;
        }

        semantic_tokens.push(next_id as i32);

        // åé¦ˆåˆ°æ¨¡å‹ï¼šè¯­ä¹‰é˜¶æ®µç›´æ¥ä½¿ç”¨åŸå§‹tokenï¼ˆä¸åŠ åç§»ï¼‰
        inference.batches[0].push(next_id as u32);
    }


    Ok((global_tokens, semantic_tokens))
}
