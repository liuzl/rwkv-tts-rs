use anyhow::Result;
use rand::rngs::StdRng;
use rand::SeedableRng;
use tracing::warn;
use web_rwkv::runtime::infer::{RnnInput, RnnInputBatch, RnnOption};

use crate::shared_runtime::TtsInferContext;

/// æ‰§è¡Œæ™®é€šæ¨¡å¼æ¨ç†
pub async fn execute_normal_inference(
    infer_context: TtsInferContext,
    text_tokens: Vec<i32>,
    property_tokens: Vec<i32>,
    _rng: rand::rngs::StdRng,
    request: &crate::rwkv_sampler::TtsBatchRequest,
) -> Result<(Vec<i32>, Vec<i32>)> {
    let request_id = &infer_context.request_id;
    // å¼€å§‹æ™®é€šæ¨¡å¼æ¨ç†

    // è·å–é‡‡æ ·å‚æ•°
    let _sampler_args = &request.args;

    // Acquire runtime semaphore for the entire inference to ensure isolation
    let _runtime_permit = infer_context
        .runtime_semaphore
        .acquire()
        .await
        .map_err(|e| anyhow::anyhow!("æ— æ³•è·å–è¿è¡Œæ—¶ä¿¡å·é‡: {}", e))?;

    // å·²è·å–ä¿¡å·é‡è®¸å¯ï¼Œå¼€å§‹æ¨ç†

    // è·å–runtime
    let runtime = &infer_context.runtime;
    let state = &infer_context.state;

    // æ„å»ºè¾“å…¥åºåˆ—ï¼šå±æ€§tokens + TTS_TAG_2 + æ–‡æœ¬tokens + TTS_TAG_0
    let mut input_tokens: Vec<i32> = Vec::new();
    input_tokens.extend_from_slice(&property_tokens);
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_2);
    input_tokens.extend_from_slice(&text_tokens);
    input_tokens.push(crate::rwkv_sampler::TTS_TAG_0);

    // è°ƒè¯•ï¼šæ‰“å°è¾“å…¥åºåˆ—æ„å»ºä¿¡æ¯
    log::info!("ğŸ” [{}] è¾“å…¥åºåˆ—æ„å»ºè¯¦æƒ…:", request_id);
    log::info!("   ğŸ“ å±æ€§tokens: {:?}", property_tokens);
    log::info!("   ğŸ“ æ–‡æœ¬tokensé•¿åº¦: {}", text_tokens.len());
    log::info!("   ğŸ“ å®Œæ•´è¾“å…¥åºåˆ—é•¿åº¦: {}", input_tokens.len());
    log::info!(
        "   ğŸ“ è¾“å…¥åºåˆ—å‰10ä¸ªtoken: {:?}",
        &input_tokens[..std::cmp::min(10, input_tokens.len())]
    );

    // æ„å»ºå®Œæ•´è¾“å…¥åºåˆ—

    // === Prefill é˜¶æ®µ ===
    let input_tokens_u32: Vec<u32> = input_tokens.iter().map(|&t| t as u32).collect();
    let token_chunk_size = infer_context.options.token_chunk_size;

    // Prefillé˜¶æ®µ - åˆå§‹åŒ–ç‹¬ç«‹çŠ¶æ€

    // åˆ›å»ºç‹¬ç«‹çš„æ¨ç†ä¸Šä¸‹æ–‡
    let batch = RnnInputBatch::new(input_tokens_u32.clone(), RnnOption::Last);
    let mut inference = RnnInput::new(vec![batch], token_chunk_size);

    // ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€ï¼Œç¡®ä¿çŠ¶æ€éš”ç¦»
    {
        let state_guard = state.lock().await;
        let initial_state = state_guard.init();
        state_guard.load(initial_state, 0)?;
        // å·²ä¸ºæ‰¹å¤„ç†æ§½ä½0åŠ è½½åˆå§‹çŠ¶æ€
    }

    // æ¶ˆåŒ–è¾“å…¥ç›´åˆ°äº§ç”Ÿè¾“å‡º
    let last_logits: Vec<f32> = loop {
        let (remaining_input, output) = runtime.infer(inference.clone()).await?;
        inference = remaining_input;
        if !output.is_empty() && output[0].0.size() > 0 {
            break output[0].0.clone().to_vec();
        }
    };

    // æ–°å¢ï¼šæ ¹æ®logitsé•¿åº¦æ¨æ–­è¯è¡¨å¤§å°ï¼Œå¹¶æ ¡éªŒå±æ€§tokenæ˜¯å¦è¶Šç•Œ
    let vocab_size = last_logits.len();
    if !property_tokens.is_empty() {
        let mut out_of_range = vec![];
        for &t in &property_tokens {
            if (t as usize) >= vocab_size {
                out_of_range.push(t);
            }
        }
        if !out_of_range.is_empty() {
            log::warn!(
                "ğŸš¨ [{}] æ£€æµ‹åˆ°å±æ€§tokensè¶…å‡ºè¯è¡¨èŒƒå›´ï¼Œå¯èƒ½è¢«æ¨¡å‹å¿½ç•¥ï¼šè¶Šç•Œtoken={:?}ï¼Œè¯è¡¨å¤§å°={}ã€‚è¯·æ ¸å¯¹TTS_SPECIAL_TOKEN_OFFSETæ˜¯å¦ä¸æ¨¡å‹/è¯è¡¨åŒ¹é…ã€‚",
                request_id,
                out_of_range,
                vocab_size
            );
        } else {
            log::info!(
                "âœ… [{}] å±æ€§tokensåœ¨è¯è¡¨èŒƒå›´å†…ï¼ˆvocab_size={}ï¼‰ï¼Œå°†å‚ä¸Prefillé˜¶æ®µã€‚",
                request_id,
                vocab_size
            );
        }
    }

    // === Global é˜¶æ®µ ===
    let mut global_tokens: Vec<i32> = Vec::new();
    let mut semantic_tokens: Vec<i32> = Vec::new();

    // æ™®é€šæ¨¡å¼è¿›è¡Œæ­£å¸¸çš„ç”Ÿæˆæµç¨‹ï¼ˆä¸ä½¿ç”¨é¢„æå–ç‰¹å¾ï¼‰
    // Globalé˜¶æ®µä½¿ç”¨å›ºå®šå‚æ•°ï¼ˆä¸Pythonç‰ˆæœ¬ä¸€è‡´ï¼‰
    let args_global = crate::rwkv_sampler::SamplerArgs {
        temperature: 1.0, // Globalé˜¶æ®µä½¿ç”¨å›ºå®šå‚æ•°
        top_k: 20,
        top_p: 0.95,
        seed: infer_context.options.seed,
        max_tokens: 32, // Globalé˜¶æ®µå›ºå®š32ä¸ªtokens
        voice_fidelity: infer_context.options.voice_fidelity,
        layered_randomness: infer_context.options.layered_randomness.clone(),
        token_chunk_size: infer_context.options.token_chunk_size,
    };

    let args_semantic = crate::rwkv_sampler::SamplerArgs {
        temperature: 1.0, // Semanticé˜¶æ®µä½¿ç”¨å›ºå®šå‚æ•°
        top_p: 0.95,
        top_k: 80,
        seed: infer_context.options.seed,
        max_tokens: 2048,
        voice_fidelity: infer_context.options.voice_fidelity,
        layered_randomness: infer_context.options.layered_randomness.clone(),
        token_chunk_size: infer_context.options.token_chunk_size,
    };

    // ç®€åŒ–é‡‡æ ·ï¼Œç§»é™¤ä¼˜åŒ–ç»„ä»¶

    // åˆ›å»ºç‹¬ç«‹çš„RNGç”¨äºä¸åŒé˜¶æ®µ
    let mut global_rng = if args_global.layered_randomness.use_independent_seeds {
        if let Some(seed) = args_global.seed {
            // ç”¨æˆ·æä¾›äº†seedï¼Œä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
            Some(StdRng::seed_from_u64(seed.wrapping_add(
                args_global.layered_randomness.global_seed_offset,
            )))
        } else {
            // æ²¡æœ‰seedï¼Œåˆ›å»ºéšæœºRNG
            Some(StdRng::from_entropy())
        }
    } else {
        // åˆ›å»ºæ–°çš„RNGå®ä¾‹ï¼Œé¿å…å…±äº«çŠ¶æ€å¯¼è‡´çš„ä¸ä¸€è‡´
        Some(if let Some(seed) = args_global.seed {
            StdRng::seed_from_u64(seed.wrapping_add(100))
        } else {
            StdRng::from_entropy()
        })
    };

    let mut semantic_rng = if args_semantic.layered_randomness.use_independent_seeds {
        if let Some(seed) = args_semantic.seed {
            // ç”¨æˆ·æä¾›äº†seedï¼Œä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
            Some(StdRng::seed_from_u64(seed.wrapping_add(
                args_semantic.layered_randomness.semantic_seed_offset,
            )))
        } else {
            // æ²¡æœ‰seedï¼Œåˆ›å»ºéšæœºRNG
            Some(StdRng::from_entropy())
        }
    } else {
        // åˆ›å»ºæ–°çš„RNGå®ä¾‹ï¼Œé¿å…å…±äº«çŠ¶æ€å¯¼è‡´çš„ä¸ä¸€è‡´
        Some(if let Some(seed) = args_semantic.seed {
            StdRng::seed_from_u64(seed.wrapping_add(200))
        } else {
            StdRng::from_entropy()
        })
    };

    // RNGçŠ¶æ€åˆå§‹åŒ–

    // Globalå’ŒSemanticé˜¶æ®µéƒ½ä½¿ç”¨å›ºå®šå‚æ•°ï¼ˆä¸Pythonç‰ˆæœ¬ä¸€è‡´ï¼‰
    // ç§»é™¤å‚æ•°è°ƒæ•´é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨å›ºå®šå€¼

    // å‚æ•°å¯¹æ¯”æ‰“å°ï¼šPython vs Rust
    log::info!("ğŸ” [{}] é‡‡æ ·å‚æ•°å¯¹æ¯” (Python vs Rust):", request_id);
    log::info!("   ğŸ“Š Globalé˜¶æ®µ:");
    log::info!("      Python: temperature=1.0, top_p=0.95, top_k=20");
    log::info!(
        "      Rust:   temperature={:.1}, top_p={:.2}, top_k={}",
        args_global.temperature,
        args_global.top_p,
        args_global.top_k
    );
    log::info!("   ğŸ“Š Semanticé˜¶æ®µ:");
    log::info!("      Python: temperature=1.0, top_p=0.95, top_k=80");
    log::info!(
        "      Rust:   temperature={:.1}, top_p={:.2}, top_k={}",
        args_semantic.temperature,
        args_semantic.top_p,
        args_semantic.top_k
    );

    // éªŒè¯å‚æ•°ä¸€è‡´æ€§
    let global_match = (args_global.temperature - 1.0).abs() < 0.001
        && (args_global.top_p - 0.95).abs() < 0.001
        && args_global.top_k == 20;
    let semantic_match = (args_semantic.temperature - 1.0).abs() < 0.001
        && (args_semantic.top_p - 0.95).abs() < 0.001
        && args_semantic.top_k == 80;

    if global_match && semantic_match {
        log::info!("âœ… [{}] å‚æ•°å®Œå…¨åŒ¹é…Pythonç‰ˆæœ¬ï¼", request_id);
    } else {
        log::warn!(
            "âš ï¸ [{}] å‚æ•°ä¸Pythonç‰ˆæœ¬ä¸åŒ¹é…ï¼GlobalåŒ¹é…: {}, SemanticåŒ¹é…: {}",
            request_id,
            global_match,
            semantic_match
        );
    }

    // ç”Ÿæˆ32ä¸ªglobal tokens
    let global_tokens_size: usize = 32;

    for i in 0..global_tokens_size {
        let logits: Vec<f32> = if i == 0 {
            last_logits.clone()
        } else {
            // ç»§ç»­æ¨ç†è·å–logits - ä½¿ç”¨ç°æœ‰inferenceä¸Šä¸‹æ–‡
            loop {
                let (next_inference, output) = runtime.infer(inference.clone()).await?;
                inference = next_inference;
                if output[0].0.size() > 0 {
                    break output[0].0.clone().to_vec();
                }
            }
        };

        // ä»…åœ¨[0..4096)èŒƒå›´å†…é‡‡æ ·
        let vocab_global = if logits.len() < 4096 {
            logits.len()
        } else {
            4096
        };

        // ç›´æ¥ä½¿ç”¨åŸå§‹logitsï¼Œä¸è¿›è¡Œå¢å¼ºå¤„ç†
        let sampling_logits = logits[..vocab_global].to_vec();

        // ä½¿ç”¨top-p/top-ké‡‡æ ·å™¨é‡‡æ ·
        let next_id = crate::rwkv_sampler::sample_logits_with_top_p_k(
            &sampling_logits,
            args_global.temperature,
            args_global.top_p,
            args_global.top_k,
            None, // forbid_token
            &mut global_rng,
        );

        // å®‰å…¨è½¬æ¢ï¼šç¡®ä¿tokenåœ¨æœ‰æ•ˆèŒƒå›´å†…
        if next_id > i32::MAX as usize {
            warn!(
                "ğŸš¨ [{}] Global token {} è¶…å‡ºi32èŒƒå›´ï¼Œè·³è¿‡æ­¤token",
                request_id, next_id
            );
            continue;
        }

        // é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿tokenåœ¨globalèŒƒå›´å†… [0..4096)
        if next_id >= 4096 {
            warn!(
                "ğŸš¨ [{}] Global token {} è¶…å‡ºèŒƒå›´[0..4096)ï¼Œè·³è¿‡æ­¤token",
                request_id, next_id
            );
            continue;
        }

        global_tokens.push(next_id as i32);

        // å›çŒåˆ°æ¨¡å‹ï¼šåŠ ä¸ŠGLOBAL_TOKEN_OFFSETä»¥è¿›å…¥GlobalåŸŸï¼ˆä¸Python/zero-shotä¸€è‡´ï¼‰
        let with_offset = (next_id as i32 + crate::rwkv_sampler::GLOBAL_TOKEN_OFFSET) as u32;
        inference.batches[0].push(with_offset);
        log::debug!(
            "ğŸ”§ [{}] å›çŒGlobal token: raw={}, with_offset={}",
            request_id,
            next_id,
            with_offset
        );

        // Global tokenç”Ÿæˆ
    }

    // è®°å½•Globalé˜¶æ®µå‰è‹¥å¹²ä¸ªtokenï¼Œä¾¿äºè¯Šæ–­å¼€å¤´æ¼å­—é—®é¢˜
    if !global_tokens.is_empty() {
        let head = std::cmp::min(8, global_tokens.len());
        log::info!(
            "ğŸ¯ [{}] Globalé˜¶æ®µç”Ÿæˆå‰{}ä¸ªtoken: {:?}",
            request_id,
            head,
            &global_tokens[..head]
        );
    }

    // Global tokensç”Ÿæˆå®Œæˆ

    // === åˆ‡æ¢åˆ° Semantic é˜¶æ®µ ===
    inference.batches[0].push(crate::rwkv_sampler::TTS_TAG_1 as u32);
    // åˆ‡æ¢åˆ°Semanticé˜¶æ®µ

    // è®©æ ‡ç­¾ç”Ÿæ•ˆï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºï¼Œå¹¶ä¿ç•™logitsä¾›é¦–æ­¥ä½¿ç”¨
    let last_sem_logits: Vec<f32> = loop {
        let (next_inference, output) = runtime.infer(inference).await?;
        inference = next_inference;
        if output[0].0.size() > 0 {
            break output[0].0.clone().to_vec();
        }
    };

    // è¯­ä¹‰é˜¶æ®µï¼šé™åˆ¶æœ€å¤§ç”Ÿæˆæ­¥æ•°ä¸º2048
    let semantic_limit: usize = usize::min(request.args.max_tokens, 2048);
    // å¼€å§‹ç”Ÿæˆsemantic tokens

    for i in 0..semantic_limit {
        let logits: Vec<f32> = if i == 0 {
            last_sem_logits.clone()
        } else {
            loop {
                let (next_inference, output) = runtime.infer(inference.clone()).await?;
                inference = next_inference;
                if output[0].0.size() > 0 {
                    break output[0].0.clone().to_vec();
                }
            }
        };

        // è¯­ä¹‰é˜¶æ®µä»…é‡‡æ · [0..8192]ï¼ˆåŒ…å«EOSï¼‰ï¼Œå±è”½TTS_TAG_*ä¸å…¶å®ƒåŸŸ
        let mut logits_masked = logits.clone();
        // ä¿®å¤ï¼šä¸å±è”½EOS tokenï¼Œåªå±è”½å¤§äºEOS tokençš„éƒ¨åˆ†
        for (j, v) in logits_masked.iter_mut().enumerate() {
            if j > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
                *v = f32::NEG_INFINITY;
            }
        }
        // å±è”½TTS_TAG tokensï¼Œä½†ä¿ç•™EOS token
        for tag in [
            crate::rwkv_sampler::TTS_TAG_0,
            crate::rwkv_sampler::TTS_TAG_1,
            crate::rwkv_sampler::TTS_TAG_2,
        ] {
            let idx = tag as usize;
            if idx < logits_masked.len() {
                logits_masked[idx] = f32::NEG_INFINITY;
            }
        }

        // æ³¨æ„ï¼šä¸å±è”½EOS tokenï¼Œè®©å®ƒèƒ½å¤Ÿè¢«æ­£å¸¸é‡‡æ ·ä»¥ç»ˆæ­¢ç”Ÿæˆ

        // EOS token logitsæ£€æŸ¥
        let _eos_logit = if (crate::rwkv_sampler::TTS_EOS_TOKEN as usize) < logits_masked.len() {
            logits_masked[crate::rwkv_sampler::TTS_EOS_TOKEN as usize]
        } else {
            f32::NEG_INFINITY
        };

        // ä½¿ç”¨top-p/top-ké‡‡æ ·å™¨é‡‡æ ·
        let next_id = crate::rwkv_sampler::sample_logits_with_top_p_k(
            &logits_masked,
            args_semantic.temperature,
            args_semantic.top_p,
            args_semantic.top_k,
            None, // forbid_token
            &mut semantic_rng,
        );

        // æ£€æŸ¥æ˜¯å¦é‡åˆ°EOS tokenï¼ˆå¿…é¡»åœ¨èŒƒå›´æ£€æŸ¥ä¹‹å‰ï¼‰
        if next_id == crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
            // é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ
            break;
        }

        // é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿tokenåœ¨è¯­ä¹‰èŒƒå›´å†… [0..=8192]
        if next_id > crate::rwkv_sampler::TTS_EOS_TOKEN as usize {
            warn!(
                "ğŸš¨ [{}] Semantic token {} è¶…å‡ºèŒƒå›´[0..=8192]ï¼Œè·³è¿‡æ­¤token",
                request_id, next_id
            );
            continue;
        }

        let next_id_i32 = next_id as i32;
        semantic_tokens.push(next_id_i32);

        // åé¦ˆåˆ°æ¨¡å‹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹IDï¼ˆä¸C++ä»£ç ä¸€è‡´ï¼‰
        inference.batches[0].push(next_id as u32);
    }

    // è®°å½•Semanticé˜¶æ®µå‰è‹¥å¹²ä¸ªtokenï¼Œè¾…åŠ©è¯Šæ–­â€œå¼€å¤´æ¼å­—â€
    if !semantic_tokens.is_empty() {
        let head = std::cmp::min(12, semantic_tokens.len());
        log::info!(
            "ğŸ—£ï¸ [{}] Semanticé˜¶æ®µç”Ÿæˆå‰{}ä¸ªtoken: {:?}",
            request_id,
            head,
            &semantic_tokens[..head]
        );
    } else {
        log::warn!(
            "âš ï¸ [{}] Semanticé˜¶æ®µæœªç”Ÿæˆä»»ä½•tokenï¼ˆå¯èƒ½è¿‡æ—©é‡‡æ ·åˆ°EOSæˆ–è¾“å…¥åºåˆ—æ„å»ºå¼‚å¸¸ï¼‰",
            request_id
        );
    }

    // è¿”å›ç”Ÿæˆç»“æœ
    Ok((global_tokens, semantic_tokens))
}
